{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxEUppTpMEKCbtUKHmiwSb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 파이토치(PyTorch)의 nn.Embedding()\n","파이토치에서는 임베딩 벡터를 사용하는 방법이 크게 두 가지가 있습니다. 바로 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법과 미리 사전에 훈련된 임베딩 벡터(pre-trained word embedding)들을 가져와 사용하는 방법입니다. 이번 챕터에서는 전자에 해당되는 방법에 대해서 배웁니다. 파이토치에서는 이를 nn.Embedding()를 사용하여 구현합니다.\n","\n","이와 대조되는 방법인 사전에 훈련된 임베딩 벡터(pre-trained word embedding)를 사용하는 방법은 다음 챕터에서 다룹니다. 이번 실습은 다음과 같이 torch 패키지를 임포트했다고 가정합니다."],"metadata":{"id":"pKRIO7HD8MSk"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"HNkBnVxO7iFV","executionInfo":{"status":"ok","timestamp":1750064051162,"user_tz":-540,"elapsed":6140,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","source":["### 1. 임베딩 층은 룩업 테이블이다.\n","임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 합니다.\n","\n","어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터\n","\n","임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다. 훈련 과정에서 단어는 모델이 풀고자하는 작업에 맞는 값으로 업데이트 됩니다. 그리고 이 밀집 벡터를 임베딩 벡터라고 부릅니다.\n","\n","정수를 밀집 벡터 또는 임베딩 벡터로 맵핑한다는 것은 어떤 의미일까요? 특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블이라고 볼 수 있습니다. 그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가집니다."],"metadata":{"id":"EnOTUoKH8gxQ"}},{"cell_type":"markdown","source":["룩업 테이블의 개념을 이론적으로 우선 접하고, 처음 파이토치를 배울 때 어떤 분들은 임베딩 층의 입력이 원-핫 벡터가 아니어도 동작한다는 점에 헷갈려 합니다. 파이토치는 단어를 정수 인덱스로 바꾸고 원-핫 벡터로 한번 더 바꾸고나서 임베딩 층의 입력으로 사용하는 것이 아니라, 단어를 정수 인덱스로만 바꾼채로 임베딩 층의 입력으로 사용해도 룩업 테이블 된 결과인 임베딩 벡터를 리턴합니다.\n","\n","룩업 테이블 과정을 nn.Embedding()을 사용하지 않고 구현해보면서 이해해보겠습니다.\n","우선 임의의 문장으로부터 단어 집합을 만들고 각 단어에 정수를 부여합니다."],"metadata":{"id":"eU74R7lO9Djv"}},{"cell_type":"code","source":["train_data = 'you need to know how to code'\n","\n","# 중복을 제거한 단어들의 집합인 단어 집합 생성\n","word_set = set(train_data.split())\n","\n","# 단어 집합의 각 단어에 고유한 정수 맵핑\n","vocab = {word: i+2 for i, word in enumerate(word_set)}\n","vocab['<unk>'] = 0  # unknown word, 즉 훈련에 등장하지 않은 단어를 처리하기 위한 토큰이에요.\n","vocab['<pad>'] = 1  # padding token, 짧은 문장의 길이를 맞출 때 사용하는 토큰이에요.\n","\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wsaYlm8e8f10","executionInfo":{"status":"ok","timestamp":1750064391107,"user_tz":-540,"elapsed":12,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2f1c6e02-a9c3-4d8f-a9bb-25d6e216b990"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{'need': 2, 'you': 3, 'how': 4, 'to': 5, 'code': 6, 'know': 7, '<unk>': 0, '<pad>': 1}\n"]}]},{"cell_type":"markdown","source":["이제 단어 집합의 크기를 행으로 가지는 임베딩 테이블을 구현합니다. 단, 여기서 임베딩 벡터의 차원은 3으로 정했습니다."],"metadata":{"id":"7x01tCPT-QO-"}},{"cell_type":"code","source":["# 단어 집합의 크기 만큼의 행을 가지는 테이블 생성\n","embedding_table = torch.FloatTensor([\n","                               [ 0.0,  0.0,  0.0],\n","                               [ 0.0,  0.0,  0.0],\n","                               [ 0.2,  0.9,  0.3],\n","                               [ 0.1,  0.5,  0.7],\n","                               [ 0.2,  0.1,  0.8],\n","                               [ 0.4,  0.1,  0.1],\n","                               [ 0.1,  0.8,  0.9],\n","                               [ 0.6,  0.1,  0.1]])"],"metadata":{"id":"xY53EuDY9zi4","executionInfo":{"status":"ok","timestamp":1750064562131,"user_tz":-540,"elapsed":49,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["이제 임의의 문장 'you need to run'에 대해서 룩업 테이블을 통해 임베딩 벡터들을 가져와보겠습니다."],"metadata":{"id":"xv0kiXdp-iFH"}},{"cell_type":"code","source":["sample = 'you need to run'.split()\n","idxes = []\n","\n","# 각 단어를 정수로 변환\n","for word in sample:\n","  try:\n","    idxes.append(vocab[word])\n","  except KeyError:\n","    idxes.append(vocab['<unk>'])\n","idxes = torch.LongTensor(idxes) # 정수(int64)형 텐서로 만듬\n","\n","# 각 정수를 인덱스로 임베딩 테이블에서 값을 가져온다.\n","lookup_result = embedding_table[idxes, :]\n","print(lookup_result)\n","print(idxes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oc6URzYD-f5Q","executionInfo":{"status":"ok","timestamp":1750065245853,"user_tz":-540,"elapsed":13,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2560a8ea-aeef-47b1-a1cd-5d03d5763e70"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1000, 0.5000, 0.7000],\n","        [0.2000, 0.9000, 0.3000],\n","        [0.4000, 0.1000, 0.1000],\n","        [0.0000, 0.0000, 0.0000]])\n","tensor([3, 2, 5, 0])\n"]}]},{"cell_type":"markdown","source":["###2. 임베딩 층 사용하기\n","이제 nn.Embedding()으로 사용할 경우를 봅시다. 우선 전처리는 동일한 과정을 거칩니다."],"metadata":{"id":"XLvuXuuUAJ8_"}},{"cell_type":"code","source":["train_data = 'you need to know how to code'\n","\n","# 중복을 제거한 단어들의 집합인 단어 집합 생성.\n","word_set = set(train_data.split())\n","\n","# 단어 집합의 각 단어에 고유한 정수 맵핑.\n","vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}\n","vocab['<unk>'] = 0\n","vocab['<pad>'] = 1\n"],"metadata":{"id":"69Z0in2K_M8E","executionInfo":{"status":"ok","timestamp":1750065950838,"user_tz":-540,"elapsed":18,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["이제 nn.Embedding()을 사용하여 학습가능한 임베딩 테이블을 만듭니다."],"metadata":{"id":"Ycv6aHzLDyEV"}},{"cell_type":"code","source":["embedding_layer = nn.Embedding(num_embeddings=len(vocab),\n","                               embedding_dim=3,\n","                               padding_idx=1)"],"metadata":{"id":"R4vMVpaCDzCI","executionInfo":{"status":"ok","timestamp":1750066003540,"user_tz":-540,"elapsed":34,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["nn.Embedding은 크게 두 가지 인자를 받는데 각각 num_embeddings과 embedding_dim입니다.\n","\n","* num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기입니다.\n","* embedding_dim : 임베딩 할 벡터의 차원입니다. 사용자가 정해주는 하이퍼파라미터입니다.\n","* padding_idx : 선택적으로 사용하는 인자입니다. 패딩을 위한 토큰의 인덱스를 알려줍니다."],"metadata":{"id":"5FQWjedSEBIg"}},{"cell_type":"code","source":["print(embedding_layer.weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5RKx_6KD_zU","executionInfo":{"status":"ok","timestamp":1750066085632,"user_tz":-540,"elapsed":17,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"fa120644-d5aa-411c-ab1f-486f44d21b4d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-0.5101,  1.3317,  0.7063],\n","        [ 0.0000,  0.0000,  0.0000],\n","        [ 0.3319,  0.3071, -0.1595],\n","        [ 0.4999, -0.3982,  0.1985],\n","        [-2.5035, -1.7929,  1.1883],\n","        [ 0.4554,  1.5597, -0.7227],\n","        [ 2.4552,  0.9548,  0.7709],\n","        [ 1.7258,  0.4103, -0.3662]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["앞선 예제와 마찬가지로 단어 집합의 크기의 행을 가지는 임베딩 테이블이 생성되었습니다."],"metadata":{"id":"QTl05pb0EYbE"}},{"cell_type":"code","source":[],"metadata":{"id":"qjyy3yakET2Q"},"execution_count":null,"outputs":[]}]}