{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOsNQ5z6ODC6Mw9WjtFUGjv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Seq2Seq를 이용한 번역기 구현하기\n","seq2seq를 이용해서 기계 번역기를 만들어보겠습니다. 실제 서비스에 사용되는 번역기는 뒤의 챕터에서 배우게 될 어텐션 메커니즘을 사용해야 하고, 최소 수백만 개의 데이터가 필요합니다. 하지만 그럼에도 번역기를 만드는 간단한 토이 프로젝트를 사용해서 seq2seq 구조와 인코더와 디코더의 역할을 이해할 수 있습니다."],"metadata":{"id":"oan0neZc8wRI"}},{"cell_type":"markdown","source":["### 1. 데이터 로드 및 전처리\n","\n","실제 성능이 좋은 기계 번역기를 구현하려면 방대한 데이터가 필요하므로 여기서는 seq2seq를 간단히 실습해보는 수준의 간단한 기계 번역기를 구현해보겠습니다. 기계 번역기를 훈련시키기 위해서는 훈련 데이터로 병렬 코퍼스(parallel corpus)가 필요합니다. 병렬 코퍼스란, 두 개 이상의 언어가 병렬적으로 구성된 코퍼스를 의미합니다.\n","\n","링크 : http://www.manythings.org/anki\n","\n","이번 실습에서는 프랑스어-영어 병렬 코퍼스인 fra-eng.zip 파일을 사용합니다. 위의 링크에서 해당 파일을 다운받은 후 압축을 풀면 fra.txt라는 파일을 얻을 수 있는데 해당 파일을 이 실습에서 사용합니다.\n","\n","병렬 코퍼스 데이터에 대해서 이해해봅시다. 병렬 데이터라고 하면 앞서 수행한 태깅 작업 챕터의 개체명 인식과 같은 데이터를 생각할 수 있지만, 앞서 수행한 태깅 작업의 병렬 데이터와 seq2seq가 사용하는 병렬 데이터는 성격이 다릅니다. 태깅 작업의 병렬 데이터는 쌍이 되는 데이터와 레이블이 길이가 동일하였으나 여기서는 쌍이 된다고 해서 반드시 길이가 같지는 않습니다.\n","\n","실제 번역기를 생각해보면 구글 번역기에 '나는 학생이다.'라는 토큰의 개수가 2인 문장을 넣었을 때 'I am a student.'라는 토큰의 개수가 4인 문장이 나오는 것과 같은 이치입니다. seq2seq는 기본적으로 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있다고 가정합니다. 지금 구현 예제는 기계 번역기이지만 seq2seq로 구현할 수 있는 또 다른 예제인 챗봇을 만든다고 가정해보면, 대답의 길이가 질문의 길이와 항상 똑같아야 한다고하면 그 또한 이상합니다. 여기서 사용할 fra.txt 데이터는 아래와 같이 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 형식이 하나의 샘플입니다.\n","\n","- Watch me.           Regardez-moi !\n"],"metadata":{"id":"3pCzJztz88B6"}},{"cell_type":"markdown","source":["데이터는 위와 동일한 형식의 약 19만개의 병렬 문장 샘플을 포함하고 있습니다. 데이터를 읽고 전처리를 진행해보겠습니다. 앞으로의 코드에서 src는 source의 줄임말로 입력 문장을 나타내며, tar는 target의 줄임말로 번역하고자 하는 문장을 나타냅니다."],"metadata":{"id":"9hjpvY_49njy"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"pEBGu4AZ8um8","executionInfo":{"status":"ok","timestamp":1750668733748,"user_tz":-540,"elapsed":4413,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"outputs":[],"source":["import re\n","import os\n","import unicodedata\n","import urllib3\n","import zipfile\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import torch\n","from collections import Counter\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","source":["이번 실습에서는 약 19만개의 데이터 중 33,000개의 샘플만을 사용할 예정입니다."],"metadata":{"id":"NW0q8Km6-IV5"}},{"cell_type":"code","source":["num_samples =33000"],"metadata":{"id":"fc_iEYRu-F6v","executionInfo":{"status":"ok","timestamp":1750668733756,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["fra-eng.zip 파일을 다운로드하고 압축을 풀겠습니다."],"metadata":{"id":"PPU4HLfR-ULk"}},{"cell_type":"code","source":["!wget -c http://www.manythings.org/anki/fra-eng.zip && unzip -o fra-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zbE9VS--SY2","executionInfo":{"status":"ok","timestamp":1750668734662,"user_tz":-540,"elapsed":900,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"f80c75be-8057-4e16-d159-1e601144fced"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-06-23 08:52:12--  http://www.manythings.org/anki/fra-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n","Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8143096 (7.8M) [application/zip]\n","Saving to: ‘fra-eng.zip’\n","\n","fra-eng.zip         100%[===================>]   7.77M  23.9MB/s    in 0.3s    \n","\n","2025-06-23 08:52:13 (23.9 MB/s) - ‘fra-eng.zip’ saved [8143096/8143096]\n","\n","Archive:  fra-eng.zip\n","  inflating: _about.txt              \n","  inflating: fra.txt                 \n"]}]},{"cell_type":"markdown","source":["전처리 함수들을 구현합니다. 구두점 등을 제거하거나 단어와 구분해주기 위한 전처리입니다."],"metadata":{"id":"jF8eNLlO-Zq9"}},{"cell_type":"code","source":["def unicode_to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"],"metadata":{"id":"LXFYSmGA-Wlr","executionInfo":{"status":"ok","timestamp":1750668734674,"user_tz":-540,"elapsed":8,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def preprocess_sentence(sent):\n","  # 악센트 삭제 함수 호출\n","  sent = unicode_to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백을 만듭니다.\n","  # Ex) \"he is a boy.\" => \"he is a boy .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"],"metadata":{"id":"Uc_oApV9-2G_","executionInfo":{"status":"ok","timestamp":1750668734710,"user_tz":-540,"elapsed":29,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], []\n","\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines):\n","      # source 데이터와 target 데이터 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","\n","  return encoder_input, decoder_input, decoder_target\n"],"metadata":{"id":"ix9x3tvN_wbr","executionInfo":{"status":"ok","timestamp":1750668734720,"user_tz":-540,"elapsed":6,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["구현한 전처리 함수들을 임의의 문장을 입력으로 테스트해봅시다."],"metadata":{"id":"GgR5oueGC9XT"}},{"cell_type":"code","source":["# 전처리 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print('전처리 전 영어 문장 :', en_sent)\n","print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n","print('전처리 전 프랑스어 문장 :', fr_sent)\n","print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XPuFc1DBWcw","executionInfo":{"status":"ok","timestamp":1750668734747,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8c4f6ac6-0962-442e-9515-05745cad9e66"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["전처리 전 영어 문장 : Have you had dinner?\n","전처리 후 영어 문장 : have you had dinner ?\n","전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n","전처리 후 프랑스어 문장 : avez vous deja dine ?\n"]}]},{"cell_type":"markdown","source":["전체 데이터에서 33,000개의 샘플에 대해서 전처리를 수행합니다. 또한 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정이므로, 훈련 시 사용할 디코더의 입력 시퀀스와 실제값. 즉, 레이블에 해당되는 출력 시퀀스를 따로 분리하여 저장합니다. 입력 시퀀스에는 시작을 의미하는 토큰인 를 추가하고, 출력 시퀀스에는 종료를 의미하는 토큰인 를 추가합니다. 이렇게 얻은 3개의 데이터셋 인코더의 입력, 디코더의 입력, 디코더의 레이블을 상위 5개 샘플만 출력해봅시다."],"metadata":{"id":"VUl2UsIcDXo8"}},{"cell_type":"code","source":["sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n","print('인코더의 입력 :',sents_en_in[:5])\n","print('디코더의 입력 :',sents_fra_in[:5])\n","print('디코더의 레이블 :',sents_fra_out[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77S1daZ8DFia","executionInfo":{"status":"ok","timestamp":1750668735664,"user_tz":-540,"elapsed":911,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2f8ebc23-d745-45d9-bdc0-29eab2263af7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n","디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n","디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"]}]},{"cell_type":"markdown","source":["모델을 설계하기 전 의아한 점이 있을 수 있습니다. 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다고 설명하였는데 디코더의 입력에 해당하는 데이터인 sents_fra_in이 왜 필요할까요?\n","\n","훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다."],"metadata":{"id":"KMRM9g2AK2Lp"}},{"cell_type":"markdown","source":["단어로부터 정수를 얻는 딕셔너리. 즉, 단어 집합(Vocabulary)을 만들어봅시다. 이를 위한 함수로 build_vocab()을 구현합니다. build_vocab은 입력된 데이터로부터 단어의 등장 빈도순으로 정렬 후에 등장 빈도가 높은 순서일 수록 낮은 정수를 부여합니다. 이때, 패딩 토큰을 위한 <PAD> 토큰은 0번, OOV에 대응하기 위한 <UNK> 토큰은 1번에 할당합니다. 이렇게 되면 빈도수가 가장 높은 단어는 정수가 2번, 빈도수가 두번 째로 많은 단어는 정수 3번이 할당됩니다."],"metadata":{"id":"IdF84OnGLT4c"}},{"cell_type":"code","source":["def build_vocab(sents):\n","  word_list = []\n","\n","  for sent in sents:\n","      for word in sent:\n","        word_list.append(word)\n","\n","  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n","  word_counts = Counter(word_list)\n","  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","\n","  word_to_index = {}\n","  word_to_index['<PAD>'] = 0\n","  word_to_index['<UNK>'] = 1\n","\n","  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n","  for index, word in enumerate(vocab) :\n","    word_to_index[word] = index + 2\n","\n","  return word_to_index"],"metadata":{"id":"3Cgvuwb0D89z","executionInfo":{"status":"ok","timestamp":1750668735723,"user_tz":-540,"elapsed":55,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["영어를 위한 단어 집합 src_vocab과 프랑스어를 이용한 단어 집합 tar_vocab를 만들어봅시다. 구현 방식에 따라서는 하나의 단어 집합으로 만들어도 상관없으며 이는 선택의 차이입니다."],"metadata":{"id":"DeknNdpcMjNf"}},{"cell_type":"code","source":["src_vocab = build_vocab(sents_en_in)\n","tar_vocab = build_vocab(sents_fra_in + sents_fra_out)\n","\n","src_vocab_size = len(src_vocab)\n","tar_vocab_size = len(tar_vocab)\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgkAq7kaN5oJ","executionInfo":{"status":"ok","timestamp":1750668735787,"user_tz":-540,"elapsed":59,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8c210698-d4a5-4f5a-ed87-b38d6d7c48a2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 4498, 프랑스어 단어 집합의 크기 : 7895\n"]}]},{"cell_type":"markdown","source":["정수로부터 단어를 얻는 딕셔너리를 각각 만들어줍니다. 이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용됩니다."],"metadata":{"id":"mMirGmr8Ocfq"}},{"cell_type":"code","source":["index_to_src = {v: k for k, v in src_vocab.items()}\n","index_to_tar = {v: k for k, v in tar_vocab.items()}\n","\n","def texts_to_sequences(sents, word_to_index):\n","  encoded_X_data = []\n","  for sent in tqdm(sents):\n","    index_sequences = []\n","    for word in sent:\n","      try:\n","          index_sequences.append(word_to_index[word])\n","      except KeyError:\n","          index_sequences.append(word_to_index['<UNK>'])\n","    encoded_X_data.append(index_sequences)\n","  return encoded_X_data"],"metadata":{"id":"YvT5_ZnYMc6h","executionInfo":{"status":"ok","timestamp":1750668735837,"user_tz":-540,"elapsed":24,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n","decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n","decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VydYdcKrNutW","executionInfo":{"status":"ok","timestamp":1750668736066,"user_tz":-540,"elapsed":222,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2fc120f4-fed0-4d42-ce16-f6b92dff59c1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 33000/33000 [00:00<00:00, 1019009.29it/s]\n","100%|██████████| 33000/33000 [00:00<00:00, 786615.25it/s]\n","100%|██████████| 33000/33000 [00:00<00:00, 176228.79it/s]\n"]}]},{"cell_type":"code","source":["# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n","# 인코더 입력이므로 <sos>나 <eos>가 없음\n","for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n","    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nlz2qGvPIux","executionInfo":{"status":"ok","timestamp":1750668736138,"user_tz":-540,"elapsed":68,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2852eed0-8df6-443a-f984-4ef0bf502a0f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [740, 2]\n"]}]},{"cell_type":"code","source":["def pad_sequences(sentences, max_len=None):\n","    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n","    if max_len is None:\n","        max_len = max([len(sentence) for sentence in sentences])\n","\n","    features = np.zeros((len(sentences), max_len), dtype=int)\n","    for index, sentence in enumerate(sentences):\n","        if len(sentence) != 0:\n","            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n","    return features"],"metadata":{"id":"03eJ4PUxPhPk","executionInfo":{"status":"ok","timestamp":1750668736149,"user_tz":-540,"elapsed":7,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["encoder_input = pad_sequences(encoder_input)\n","decoder_input = pad_sequences(decoder_input)\n","decoder_target = pad_sequences(decoder_target)"],"metadata":{"id":"6K6IXJZSQrvQ","executionInfo":{"status":"ok","timestamp":1750668736232,"user_tz":-540,"elapsed":74,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOuQEILpQvsP","executionInfo":{"status":"ok","timestamp":1750668736275,"user_tz":-540,"elapsed":39,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2c16edb7-a16c-4e0e-97c2-7c23d8332828"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (33000, 7)\n","디코더의 입력의 크기(shape) : (33000, 16)\n","디코더의 레이블의 크기(shape) : (33000, 16)\n"]}]},{"cell_type":"markdown","source":["테스트 데이터를 분리하기 전 데이터를 섞어줍니다. 이를 위해서 순서가 섞인 정수 시퀀스 리스트를 만듭니다."],"metadata":{"id":"uB74-VEFQ4V-"}},{"cell_type":"code","source":["indices = np.arange(encoder_input.shape[0]) # 행의 순서를 섞음\n","np.random.shuffle(indices)\n","print(indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNT3VRHdRBUc","executionInfo":{"status":"ok","timestamp":1750668736294,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"07b78e53-cec8-43c6-bb58-bba9f07275bf"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[10581   471  4113 ...  4850 31988 21789]\n"]}]},{"cell_type":"markdown","source":["이를 데이터셋의 순서로 지정해주면 샘플들이 기존 순서와 다른 순서로 섞이게 됩니다."],"metadata":{"id":"HFIxzciAQ4AL"}},{"cell_type":"code","source":["encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"BAxJm5w0Q14f","executionInfo":{"status":"ok","timestamp":1750668736351,"user_tz":-540,"elapsed":55,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["임의로 30997번째 샘플을 출력해봅시다. 이때 decoder_input과 decoder_target은 데이터의 구조상으로 앞에 붙은 sos 토큰과 뒤에 붙은 eos을 제외하면 동일한 시퀀스를 가져야 합니다."],"metadata":{"id":"FCurFjchRZXx"}},{"cell_type":"code","source":["print([index_to_src[word] for word in encoder_input[30997]])\n","print([index_to_tar[word] for word in decoder_input[30997]])\n","print([index_to_tar[word] for word in decoder_target[30997]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQBW6ua9RYAW","executionInfo":{"status":"ok","timestamp":1750668736376,"user_tz":-540,"elapsed":28,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"737fea6d-3c3b-4cb5-fb6a-8bb4e7a35044"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['she', 'can', 'jump', 'high', '.', '<PAD>', '<PAD>']\n","['<sos>', 'elle', 'peut', 'sauter', 'haut', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","['elle', 'peut', 'sauter', 'haut', '.', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"]}]},{"cell_type":"markdown","source":["33,000개의 10%에 해당되는 3,300개의 데이터를 테스터 데이터로 사용합니다."],"metadata":{"id":"AoUsnCebSPed"}},{"cell_type":"code","source":["n_of_val = int(33000*0.1)\n","print('검증 데이터의 개수: ', n_of_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbEfLCuCSI0n","executionInfo":{"status":"ok","timestamp":1750668736395,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"1b98f914-0ac0-4798-e26f-85eea7130580"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["검증 데이터의 개수:  3300\n"]}]},{"cell_type":"code","source":["encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]"],"metadata":{"id":"SYssD03rShO_","executionInfo":{"status":"ok","timestamp":1750668736406,"user_tz":-540,"elapsed":8,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터와 테스트 데이터의 크기를 출력해봅시다."],"metadata":{"id":"6rp7_Ep6TdmZ"}},{"cell_type":"code","source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xcq2QwSCTRm2","executionInfo":{"status":"ok","timestamp":1750668736425,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"e44c5223-2d7f-42de-d0a5-a3e4be05af1f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (29700, 7)\n","훈련 target 데이터의 크기 : (29700, 16)\n","훈련 target 레이블의 크기 : (29700, 16)\n","테스트 source 데이터의 크기 : (3300, 7)\n","테스트 target 데이터의 크기 : (3300, 16)\n","테스트 target 레이블의 크기 : (3300, 16)\n"]}]},{"cell_type":"markdown","source":["###2. 기계 번역기 만들기"],"metadata":{"id":"v0guA-HqU7dc"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","embedding_dim = 256\n","hidden_units = 256\n","\n","class Encoder(nn.Module):\n","  def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n","    super(Encoder, self).__init__()\n","    self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0)\n","    self.lstm = nn.LSTM(embedding_dim, hidden_units,  batch_first=True)\n","\n","  def forward(self, x):\n","    x = self.embedding(x)\n","    _, (hidden, cell) = self.lstm(x)\n","    return hidden, cell\n","\n","class Decoder(nn.Module):\n","  def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n","    super(Decoder, self).__init__()\n","    self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n","    self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n","    self.fc = nn.Linear(hidden_units, tar_vocab_size)\n","\n","  def forward(self, x, hidden, cell):\n","    x = self.embedding(x)\n","    output, (hidden, cell) = self.lstm(x, (hidden, cell))\n","    output = self.fc(output)\n","    return output, hidden, cell\n","\n","class Seq2Seq(nn.Module):\n","  def __init__(self, encoder, decoder):\n","    super(Seq2Seq, self).__init__()\n","    self.encoder = encoder\n","    self.decoder = decoder\n","\n","  def forward(self, src, trg):\n","    hidden, cell = self.encoder(src)\n","    output, _,_ = self.decoder(trg, hidden, cell)\n","    return output\n","\n","encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n","decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n","model = Seq2Seq(encoder, decoder)\n","\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters())"],"metadata":{"id":"lDMcEFo7TdAg","executionInfo":{"status":"ok","timestamp":1750668740881,"user_tz":-540,"elapsed":4453,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VibxlUXOY5y3","executionInfo":{"status":"ok","timestamp":1750668740907,"user_tz":-540,"elapsed":17,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"67d33233-706e-4e08-9f0a-930a2e0052f6"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(4498, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(7895, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","    (fc): Linear(in_features=256, out_features=7895, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["Encoder 클래스는 입력 시퀀스를 받아 해당 시퀀스의 정보를 압축하여 context vector로 변환하는 역할을 합니다. Encoder는 임베딩 레이어와 LSTM 레이어로 구성되어 있습니다. 임베딩 레이어는 입력 시퀀스의 각 토큰을 고정 크기의 벡터로 변환하고, LSTM 레이어는 시퀀스의 순서 정보를 고려하여 해당 시퀀스를 요약합니다. Encoder의 forward 메서드는 입력 시퀀스를 받아 LSTM의 hidden state와 cell state를 반환합니다.\n","\n","Decoder 클래스는 Encoder에서 생성된 context vector(인코더의 마지막 은닉 상태)를 기반으로 출력 시퀀스를 생성하는 역할을 합니다. Decoder 또한 임베딩 레이어와 LSTM 레이어로 구성되어 있습니다. Decoder의 LSTM은 Encoder에서 전달받은 hidden state와 cell state를 초기 상태로 사용하여 출력 시퀀스를 생성합니다. 생성된 출력 시퀀스는 fully connected 레이어를 통과하여 각 시점의 출력 토큰에 대한 확률 분포를 얻습니다. Decoder의 forward 메서드는 입력 시퀀스, hidden state, cell state를 받아 출력 시퀀스, 업데이트된 hidden state와 cell state를 반환합니다."],"metadata":{"id":"OP9r8imUwn_0"}},{"cell_type":"markdown","source":["Seq2Seq 클래스는 Encoder와 Decoder를 결합하여 전체 모델을 구성합니다. Seq2Seq 모델의 forward 메서드는 입력 시퀀스(src)와 출력 시퀀스(trg)를 받아 Encoder에서 생성된 은닉 상태(hidden state)와 셀 상태(cell state)를 Decoder로 전달하고, Decoder에서 생성된 출력 시퀀스를 반환합니다.\n","\n","Seq2Seq의 디코더는 기본적으로 각 시점마다 다중 클래스 분류 문제를 풀고있습니다. 매 시점마다 프랑스어 단어 집합의 크기(tar_vocab_size)의 선택지에서 단어를 1개 선택하여 이를 이번 시점에서 예측한 단어로 택합니다. 다중 클래스 분류 문제이므로 모델 학습을 위해 CrossEntropyLoss 함수를 사용하여 손실을 계산하고, Adam 옵티마이저를 사용하여 모델의 파라미터를 최적화합니다. CrossEntropyLoss의 ignore_index 파라미터는 패딩 토큰에 해당하는 인덱스를 무시하도록 설정합니다.\n","\n","이 코드에서는 임베딩 차원(embedding_dim)과 LSTM의 은닉 상태 크기(hidden_units)를 256으로 설정하였습니다. Encoder와 Decoder의 인스턴스를 생성한 후, 이를 Seq2Seq 모델로 결합하여 전체 모델을 구성합니다. 이렇게 구현된 Seq2Seq 모델은 기계 번역이나 챗봇과 같은 시퀀스-투-시퀀스 문제를 해결하는 데 사용될 수 있습니다. 입력 시퀀스가 Encoder를 통과하여 context vector로 변환되고, 이를 기반으로 Decoder에서 출력 시퀀스를 생성합니다. 모델의 학습은 입력 시퀀스와 해당하는 출력 시퀀스의 쌍을 사용하여 이루어집니다."],"metadata":{"id":"EcKQFrcgw-Ep"}},{"cell_type":"code","source":["def evaluation(model, dataloader, loss_function, device):\n","  model.eval()\n","  total_loss = 0.0\n","  total_correct = 0\n","  total_count = 0\n","\n","  with torch.no_grad():\n","    for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n","      encoder_inputs = encoder_inputs.to(device)\n","      decoder_inputs = decoder_inputs.to(device)\n","      decoder_targets = decoder_targets.to(device)\n","\n","      # 손실 계산\n","      outputs = model(encoder_inputs, decoder_inputs)\n","      loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n","      total_loss += loss.item()\n","\n","      # 정확도 계산(패딩 토큰 제외)\n","      mask = decoder_targets !=0\n","      total_correct +=((outputs.argmax(dim=-1) ==  decoder_targets) * mask).sum().item()\n","      total_count +=mask.sum().item()\n","\n","  return total_loss / len(dataloader), total_correct / total_count"],"metadata":{"id":"RX5Zx5jpZCFn","executionInfo":{"status":"ok","timestamp":1750668740924,"user_tz":-540,"elapsed":6,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["평가 함수의 입력으로는 평가할 모델(model), 데이터로더(dataloader), 손실 함수(loss_function), 그리고 모델을 실행할 디바이스(device)가 주어집니다. 먼저, 모델을 평가 모드로 설정합니다. 이는 model.eval()을 호출하여 이루어지며, 드롭아웃(dropout)이나 배치 정규화(batch normalization)와 같은 층의 동작을 조정합니다."],"metadata":{"id":"bU5JYCePy_wo"}},{"cell_type":"markdown","source":["다음으로, 총 손실(total_loss), 총 정확도(total_correct), 그리고 총 토큰 수(total_count)를 초기화합니다. 이 변수들은 전체 데이터셋에 대한 평가 결과를 누적하는 데 사용됩니다.\n","\n","그 후, torch.no_grad() 컨텍스트 매니저 내에서 데이터로더를 순회합니다. 이는 기울기(gradient) 계산을 비활성화하여 메모리 사용량을 줄이고 평가 속도를 향상시킵니다.\n","\n","각 배치(batch)에 대해, 인코더 입력(encoder_inputs), 디코더 입력(decoder_inputs), 그리고 디코더 타겟(decoder_targets)을 디바이스로 이동시킵니다. 그런 다음, 모델에 인코더 입력과 디코더 입력을 전달하여 순방향 전파(forward pass)를 수행합니다. 이를 통해 모델의 출력(outputs)을 얻습니다. 그 후, 출력과 디코더 타겟을 사용하여 손실을 계산합니다. 이때, 출력과 타겟의 차원을 조정하기 위해 view() 함수를 사용합니다. 계산된 손실을 총 손실에 누적합니다."],"metadata":{"id":"eO8JKrRnzJZF"}},{"cell_type":"markdown","source":["정확도를 계산하기 위해, 패딩 토큰(padding token)을 제외한 실제 토큰들에 대해서만 고려합니다. 이를 위해 디코더 타겟이 0이 아닌 위치에 대한 마스크(mask)를 생성합니다. 출력의 argmax를 취하여 예측된 토큰을 얻고, 이를 디코더 타겟과 비교하여 정확한 예측 수를 계산합니다. 정확한 예측 수와 전체 토큰 수를 누적합니다.\n","\n","마지막으로, 평균 손실(average loss)과 정확도(accuracy)를 계산하여 반환합니다. 평균 손실은 총 손실을 데이터로더의 배치 수로 나누어 계산하고, 정확도는 총 정확도를 총 토큰 수로 나누어 계산합니다. 이 평가 함수를 사용하여 모델의 성능을 측정할 수 있습니다. 평균 손실이 낮을수록, 그리고 정확도가 높을수록 모델의 성능이 좋다는 것을 나타냅니다. 이를 통해 모델의 학습 진행 상황을 모니터링하고, 최적의 모델을 선택할 수 있습니다."],"metadata":{"id":"htZUQ4D7zUIX"}},{"cell_type":"code","source":["encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n","decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n","decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n","\n","encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n","decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n","decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n","\n","batch_size = 128\n","\n","train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"G4Kkp7zny9H-","executionInfo":{"status":"ok","timestamp":1750668740938,"user_tz":-540,"elapsed":11,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# 학습 설정\n","num_epochs =30\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v29cB33R1YTr","executionInfo":{"status":"ok","timestamp":1750668741196,"user_tz":-540,"elapsed":208,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"c4af2416-df4a-4dd3-dcac-4869eb80120a"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(4498, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(7895, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","    (fc): Linear(in_features=256, out_features=7895, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["학습 설정에서는 학습 에포크 수를 30으로 설정하고, 학습에 사용할 디바이스를 설정합니다. GPU가 사용 가능한 경우 \"cuda\"로 설정하고, 그렇지 않은 경우 \"cpu\"로 설정합니다. 그 후, model.to(device)를 사용하여 모델을 설정한 디바이스로 이동시킵니다. 이를 통해 모델의 계산을 해당 디바이스에서 수행할 수 있습니다.\n","\n","이제 모델을 훈련합니다. 128개의 배치 크기(128개씩 데이터를 병렬로 학습)로 총 30 에포크 학습합니다. 검증 데이터로 훈련이 제대로 되고있는지 모니터링하겠습니다."],"metadata":{"id":"Wv6kGaT-1wpa"}},{"cell_type":"code","source":["torch.batch_norm_gather_stats_with_counts\n","\n","best_val_loss = float('inf') # Initialize best_val_loss with infinity\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","\n","  for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n","    encoder_inputs = encoder_inputs.to(device)\n","    decoder_inputs = decoder_inputs.to(device)\n","    decoder_targets = decoder_targets.to(device)\n","\n","    # 기울기 초기화\n","    optimizer.zero_grad()\n","\n","    # 순방향 전파\n","    outputs = model(encoder_inputs, decoder_inputs)\n","\n","    # 손실 계산 및 역방향 전파\n","    loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n","    loss.backward()\n","\n","    # 가중치 업데이트\n","    optimizer.step()\n","\n","  train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n","  valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n","\n","  print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n","\n","  # 검증 손실이 최소일 때 체크포인트 저장\n","  if valid_loss < best_val_loss:\n","      print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n","      best_val_loss = valid_loss\n","      torch.save(model.state_dict(), 'best_model_checkpoint.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FZcc6Fpr1rT9","executionInfo":{"status":"ok","timestamp":1750668803961,"user_tz":-540,"elapsed":62760,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"582145cd-bc67-478c-8dbc-9f21a7304813"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/30 | Train Loss: 1.1967 | Train Acc: 0.5089 | Valid Loss: 1.2267 | Valid Acc: 0.5093\n","Validation loss improved from inf to 1.2267. 체크포인트를 저장합니다.\n","Epoch: 2/30 | Train Loss: 0.9338 | Train Acc: 0.5793 | Valid Loss: 1.0044 | Valid Acc: 0.5719\n","Validation loss improved from 1.2267 to 1.0044. 체크포인트를 저장합니다.\n","Epoch: 3/30 | Train Loss: 0.7715 | Train Acc: 0.6261 | Valid Loss: 0.8836 | Valid Acc: 0.6077\n","Validation loss improved from 1.0044 to 0.8836. 체크포인트를 저장합니다.\n","Epoch: 4/30 | Train Loss: 0.6540 | Train Acc: 0.6599 | Valid Loss: 0.8063 | Valid Acc: 0.6298\n","Validation loss improved from 0.8836 to 0.8063. 체크포인트를 저장합니다.\n","Epoch: 5/30 | Train Loss: 0.5594 | Train Acc: 0.6893 | Valid Loss: 0.7478 | Valid Acc: 0.6485\n","Validation loss improved from 0.8063 to 0.7478. 체크포인트를 저장합니다.\n","Epoch: 6/30 | Train Loss: 0.4735 | Train Acc: 0.7306 | Valid Loss: 0.6970 | Valid Acc: 0.6688\n","Validation loss improved from 0.7478 to 0.6970. 체크포인트를 저장합니다.\n","Epoch: 7/30 | Train Loss: 0.4043 | Train Acc: 0.7629 | Valid Loss: 0.6606 | Valid Acc: 0.6815\n","Validation loss improved from 0.6970 to 0.6606. 체크포인트를 저장합니다.\n","Epoch: 8/30 | Train Loss: 0.3426 | Train Acc: 0.7940 | Valid Loss: 0.6306 | Valid Acc: 0.6923\n","Validation loss improved from 0.6606 to 0.6306. 체크포인트를 저장합니다.\n","Epoch: 9/30 | Train Loss: 0.2918 | Train Acc: 0.8236 | Valid Loss: 0.6086 | Valid Acc: 0.7010\n","Validation loss improved from 0.6306 to 0.6086. 체크포인트를 저장합니다.\n","Epoch: 10/30 | Train Loss: 0.2468 | Train Acc: 0.8488 | Valid Loss: 0.5903 | Valid Acc: 0.7105\n","Validation loss improved from 0.6086 to 0.5903. 체크포인트를 저장합니다.\n","Epoch: 11/30 | Train Loss: 0.2115 | Train Acc: 0.8653 | Valid Loss: 0.5795 | Valid Acc: 0.7118\n","Validation loss improved from 0.5903 to 0.5795. 체크포인트를 저장합니다.\n","Epoch: 12/30 | Train Loss: 0.1824 | Train Acc: 0.8802 | Valid Loss: 0.5726 | Valid Acc: 0.7192\n","Validation loss improved from 0.5795 to 0.5726. 체크포인트를 저장합니다.\n","Epoch: 13/30 | Train Loss: 0.1606 | Train Acc: 0.8905 | Valid Loss: 0.5712 | Valid Acc: 0.7198\n","Validation loss improved from 0.5726 to 0.5712. 체크포인트를 저장합니다.\n","Epoch: 14/30 | Train Loss: 0.1396 | Train Acc: 0.9026 | Valid Loss: 0.5624 | Valid Acc: 0.7243\n","Validation loss improved from 0.5712 to 0.5624. 체크포인트를 저장합니다.\n","Epoch: 15/30 | Train Loss: 0.1248 | Train Acc: 0.9089 | Valid Loss: 0.5607 | Valid Acc: 0.7254\n","Validation loss improved from 0.5624 to 0.5607. 체크포인트를 저장합니다.\n","Epoch: 16/30 | Train Loss: 0.1136 | Train Acc: 0.9134 | Valid Loss: 0.5650 | Valid Acc: 0.7244\n","Epoch: 17/30 | Train Loss: 0.1018 | Train Acc: 0.9189 | Valid Loss: 0.5643 | Valid Acc: 0.7284\n","Epoch: 18/30 | Train Loss: 0.0953 | Train Acc: 0.9211 | Valid Loss: 0.5737 | Valid Acc: 0.7241\n","Epoch: 19/30 | Train Loss: 0.0896 | Train Acc: 0.9234 | Valid Loss: 0.5752 | Valid Acc: 0.7246\n","Epoch: 20/30 | Train Loss: 0.0832 | Train Acc: 0.9255 | Valid Loss: 0.5779 | Valid Acc: 0.7272\n","Epoch: 21/30 | Train Loss: 0.0797 | Train Acc: 0.9266 | Valid Loss: 0.5860 | Valid Acc: 0.7233\n","Epoch: 22/30 | Train Loss: 0.0763 | Train Acc: 0.9280 | Valid Loss: 0.5890 | Valid Acc: 0.7236\n","Epoch: 23/30 | Train Loss: 0.0723 | Train Acc: 0.9290 | Valid Loss: 0.5900 | Valid Acc: 0.7243\n","Epoch: 24/30 | Train Loss: 0.0710 | Train Acc: 0.9291 | Valid Loss: 0.5936 | Valid Acc: 0.7228\n","Epoch: 25/30 | Train Loss: 0.0689 | Train Acc: 0.9301 | Valid Loss: 0.6017 | Valid Acc: 0.7267\n","Epoch: 26/30 | Train Loss: 0.0669 | Train Acc: 0.9309 | Valid Loss: 0.6036 | Valid Acc: 0.7246\n","Epoch: 27/30 | Train Loss: 0.0653 | Train Acc: 0.9307 | Valid Loss: 0.6072 | Valid Acc: 0.7237\n","Epoch: 28/30 | Train Loss: 0.0643 | Train Acc: 0.9313 | Valid Loss: 0.6125 | Valid Acc: 0.7248\n","Epoch: 29/30 | Train Loss: 0.0623 | Train Acc: 0.9318 | Valid Loss: 0.6188 | Valid Acc: 0.7248\n","Epoch: 30/30 | Train Loss: 0.0616 | Train Acc: 0.9316 | Valid Loss: 0.6201 | Valid Acc: 0.7249\n"]}]},{"cell_type":"markdown","source":["검증 데이터 손실이 가장 최소일 때의 모델을 로드하고 다시 재평가해봅시다."],"metadata":{"id":"IPR_cCmd45-A"}},{"cell_type":"code","source":["# 모델 로드\n","model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n","\n","# 모델을 device에 올립니다.\n","model.to(device)\n","\n","# 검증 데이터에 대한 정확도와 손실 계산\n","val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n","\n","print(f'Best model validation loss: {val_loss:.4f}')\n","print(f'Best model validation accuracy: {val_accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-b6sngiQ3Wue","executionInfo":{"status":"ok","timestamp":1750668804083,"user_tz":-540,"elapsed":83,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"9db73e6c-ba1e-4c93-dc49-85c40734b98f"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Best model validation loss: 0.5607\n","Best model validation accuracy: 0.7254\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-29-ccccf4da9b07>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n"]}]},{"cell_type":"markdown","source":["로드 후 재평가를 진행하였더니, 저장할 당시와 검증 데이터의 손실과 정확도가 동일하므로 저장 및 로드가 원활히 되었습니다. sos와 eos 토큰의 정수는 각각 3과 4입니다."],"metadata":{"id":"hX9vhhDG6EqN"}},{"cell_type":"code","source":["print(tar_vocab['<sos>'])\n","print(tar_vocab['<eos>'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9G8LYZf45kwV","executionInfo":{"status":"ok","timestamp":1750668804121,"user_tz":-540,"elapsed":35,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"64470b28-f811-437c-f19b-e31ede3737aa"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","4\n"]}]},{"cell_type":"markdown","source":["## 3. seq2seq 기계 번역기 동작시키기\n","seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다릅니다. 그래서 테스트 과정을 위해 모델을 다시 설계해주어야 합니다. 특히 디코더를 수정해야 합니다. 이번에는 번역 단계를 위해 모델을 수정하고 동작시켜보겠습니다.\n","\n","전체적인 번역 단계를 정리하면 아래와 같습니다."],"metadata":{"id":"Pvqxo0P26Oum"}},{"cell_type":"markdown","source":["1) 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻습니다.\n","\n","2) 인코더의 은닉 상태와 셀 상태, 그리고 토큰 sos를 디코더로 보냅니다.\n","\n","3) 디코더가 토큰 eos가 나올 때까지 다음 단어를 예측하는 행동을 반복합니다."],"metadata":{"id":"K-lR1C8X6bVN"}},{"cell_type":"code","source":["index_to_src = {v: k for k, v in src_vocab.items()}\n","index_to_tar = {v: k for k, v in tar_vocab.items()}\n","\n","# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word !=0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence"],"metadata":{"id":"slpDYeqC6LsX","executionInfo":{"status":"ok","timestamp":1750668804134,"user_tz":-540,"elapsed":8,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["print(encoder_input_test[25])\n","print(decoder_input_test[25])\n","print(decoder_target_test[25])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8v6cJMt7lsa","executionInfo":{"status":"ok","timestamp":1750668804174,"user_tz":-540,"elapsed":34,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"ad4e9e60-c476-4763-8474-74919c22d240"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["[  3  91 115   2   0   0   0]\n","[  3   5  31 102 209   2   0   0   0   0   0   0   0   0   0   0]\n","[  5  31 102 209   2   4   0   0   0   0   0   0   0   0   0   0]\n"]}]},{"cell_type":"markdown","source":["decode_sequence() 함수를 봅시다. 테스트 단계에서는 디코더를 매 시점 별로 컨트롤 하게 됩니다. 각 시점을 for문을 통해서 컨트롤하게 되며, 현재 시점의 예측은 다음 시점의 입력으로 사용됩니다. 여기서 사용될 변수는 decoder_input입니다."],"metadata":{"id":"Ap_KBkha76Gf"}},{"cell_type":"code","source":["def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n","  encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n","\n","  #인코더의 초기 상태 설정\n","  hidden, cell = model.encoder(encoder_inputs)\n","\n","  # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n","  # unsqueeze(0)은 배치 차원을 추가하기 위함\n","  decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device)\n","\n","  decoded_tokens = []\n","\n","  # 디코더의 각 시점\n","  for _ in range(max_output_len):\n","    output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n","\n","    # softmax회귀를 수행. 예측 단어의 인덱스\n","    output_token = output.argmax(dim=-1).item()\n","\n","    # 종료 토큰 <eos>\n","    if output_token == 4:\n","      break\n","\n","    # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴합니다.\n","    decoded_tokens.append(output_token)\n","\n","    # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n","    decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n","\n","  return ' '.join(int_to_tar_token[token] for token in decoded_tokens)"],"metadata":{"id":"-lgGjFFU71uZ","executionInfo":{"status":"ok","timestamp":1750668804184,"user_tz":-540,"elapsed":7,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["결과 확인을 위한 함수를 만듭니다. seq_to_src 함수는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환합니다. seq_to_tar은 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환합니다. 훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."],"metadata":{"id":"MXPTBoU5-MFn"}},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 400, 1001]:\n","  input_seq = encoder_input_train[seq_index]\n","  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",translated_text)\n","  print(\"-\"*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"epKyJrYU93DX","executionInfo":{"status":"ok","timestamp":1750668804214,"user_tz":-540,"elapsed":22,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"4ef57d03-356d-4006-869d-217aca94b48d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : are you homeless ? \n","정답문장 : etes vous sans abri ? \n","번역문장 : es tu sans abri ?\n","--------------------------------------------------\n","입력문장 : it s dead . \n","정답문장 : c est mort . \n","번역문장 : c est mort .\n","--------------------------------------------------\n","입력문장 : this is unsafe . \n","정답문장 : ce n est pas securise . \n","번역문장 : ce n est pas securise .\n","--------------------------------------------------\n","입력문장 : you re safe now . \n","정답문장 : tu es desormais en securite . \n","번역문장 : vous etes desormais en securite .\n","--------------------------------------------------\n","입력문장 : don t push . \n","정답문장 : ne poussez pas . \n","번역문장 : ne poussez pas .\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["테스트 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."],"metadata":{"id":"lXq4bcvt-161"}},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index]\n","  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",translated_text)\n","  print(\"-\"*50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JF40aRMG-zSC","executionInfo":{"status":"ok","timestamp":1750668804308,"user_tz":-540,"elapsed":90,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d8772c50-f0a3-46b9-b2a3-92729214a85d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : i ate apples . \n","정답문장 : j ai mange des pommes . \n","번역문장 : j ai mange des frites .\n","--------------------------------------------------\n","입력문장 : i saw a fight . \n","정답문장 : j ai vu une bagarre . \n","번역문장 : j ai vu une etoile .\n","--------------------------------------------------\n","입력문장 : tom is knocking . \n","정답문장 : tom frappe . \n","번역문장 : tom a ete frappe .\n","--------------------------------------------------\n","입력문장 : i ve had enough . \n","정답문장 : j en ai assez gobe . \n","번역문장 : j en ai assez avale .\n","--------------------------------------------------\n","입력문장 : we got separated . \n","정답문장 : nous avons ete separes . \n","번역문장 : nous nous sommes separees .\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fQwXRLbH-3iD","executionInfo":{"status":"ok","timestamp":1750668804317,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":35,"outputs":[]}]}