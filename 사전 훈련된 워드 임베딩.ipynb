{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEPlh7wj+Ro5BM2ycfPpJX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)\n","임베딩 벡터를 얻기 위해서 파이토치의 nn.Embedding()을 사용하기도 하지만, 때로는 이미 훈련되어져 있는 워드 임베딩을 불러서 이를 임베딩 벡터로 사용하기도 합니다. 훈련 데이터가 부족한 상황이라면 모델에 파이토치의 nn.Embedding()을 사용하는 것보다 다른 텍스트 데이터로 사전 훈련되어 있는 임베딩 벡터를 불러오는 것이 나은 선택일 수 있습니다.\n","\n","훈련 데이터가 적다면 파이토치의 nn.Embedding()으로 해당 문제에 충분히 특화된 임베딩 벡터를 만들어내는 것이 쉽지 않습니다. 이 경우, 해당 문제에 특화된 것은 아니지만 보다 일반적이고 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용하는 것이 성능의 개선을 가져올 수 있습니다."],"metadata":{"id":"r9S1UOabHP5v"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vONbrc6jHPN-","executionInfo":{"status":"ok","timestamp":1750066988175,"user_tz":-540,"elapsed":6872,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"f653a642-b1f9-4f3c-c1a1-7cc400d29acb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"]}],"source":["!pip install gensim"]},{"cell_type":"markdown","source":["### 1. 사전 훈련된 임베딩을 사용하지 않는 경우"],"metadata":{"id":"uBLkyV9cHkdy"}},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","import gensim"],"metadata":{"id":"lTeN3UuzHinu","executionInfo":{"status":"ok","timestamp":1750067035644,"user_tz":-540,"elapsed":1106,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["문장의 긍, 부정을 판단하는 감성 분류 모델을 만들어봅시다. 문장과 레이블 데이터를 만들었습니다. 긍정인 문장은 레이블 1, 부정인 문장은 레이블이 0입니다."],"metadata":{"id":"WgxziSRUH8Xq"}},{"cell_type":"code","source":["sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n","y_train = [1, 0, 0, 1, 1, 0, 1]"],"metadata":{"id":"nSwnvdh_H7h3","executionInfo":{"status":"ok","timestamp":1750067058746,"user_tz":-540,"elapsed":40,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["각 샘플에 대해서 단어 토큰화를 수행합니다."],"metadata":{"id":"EyWHdZFmIEeN"}},{"cell_type":"code","source":["tokenized_sentences = [sent.split( ) for sent in sentences]\n","print('단어 토큰화된 결과: ', tokenized_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCppbRzrIBch","executionInfo":{"status":"ok","timestamp":1750067145800,"user_tz":-540,"elapsed":14,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"746db7fd-fc33-48d7-b449-b67918f83565"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 토큰화된 결과:  [['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'work'], ['supreme', 'quality'], ['bad'], ['highly', 'respectable']]\n"]}]},{"cell_type":"markdown","source":["토큰화 된 결과를 바탕으로 단어 집합을 만들어봅시다. 우선 Counter() 모듈을 이용하여 각 단어의 등장 빈도수를 기록합니다."],"metadata":{"id":"JPQ_pg5qIcLx"}},{"cell_type":"code","source":["word_list = []\n","for sent in tokenized_sentences:\n","  for word in sent:\n","    word_list.append(word)\n","\n","word_counts = Counter(word_list)\n","print('총 단어의수 :', len(word_counts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqD-zuL8IWsK","executionInfo":{"status":"ok","timestamp":1750067254728,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"db9d9ee7-fa21-49d2-db90-99bb7c1835d1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["총 단어의수 : 15\n"]}]},{"cell_type":"markdown","source":["현재 존재하는 총 단어의 수는 15개입니다. 이 단어들을 등장 빈도가 높은 순서부터 정렬합니다."],"metadata":{"id":"HQez4A5EI0Mb"}},{"cell_type":"code","source":["# 등장 빈도순 정렬\n","vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvCFP9RGIxSJ","executionInfo":{"status":"ok","timestamp":1750067352357,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"e566d814-de43-4632-992c-50d215a88906"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['nice', 'great', 'best', 'amazing', 'stop', 'lies', 'pitiful', 'nerd', 'excellent', 'work', 'supreme', 'quality', 'bad', 'highly', 'respectable']\n"]}]},{"cell_type":"markdown","source":["nice가 등장 빈도수로 가장 높은 단어이고, 그 다음은 great, 그 다음은 best로 등장 빈도가 높은 순서대로 단어가 정렬된 상태입니다. 이제 이로부터 단어 집합을 완성해봅시다. 0번은 패딩 토큰을 위한 용도로 사용하고, 1번은 단어 집합에 없는 단어가 등장하는 OOV(Out-Of-Vocabulary) 문제가 발생하면 사용하는 용도로 각각 할당합니다."],"metadata":{"id":"zw9PDfhjJSfd"}},{"cell_type":"code","source":["word_to_index = {}\n","word_to_index['<PAD>'] = 0\n","word_to_index['<UNK>'] = 1\n","\n","for index, word in enumerate(vocab):\n","  word_to_index[word] = index +2\n","\n","vocab_size = len(word_to_index)\n","print('패딩 토큰, UNK 토큰을 고려한단어 집합의 크기: ', vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRSFXe-mJJHh","executionInfo":{"status":"ok","timestamp":1750067556840,"user_tz":-540,"elapsed":24,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"de773fa7-b5f0-42ed-bdae-313cca2bd50d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩 토큰, UNK 토큰을 고려한단어 집합의 크기:  17\n"]}]},{"cell_type":"code","source":["print(word_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZuMmATqJ7CV","executionInfo":{"status":"ok","timestamp":1750067565950,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"77d6f460-5ab7-431b-cfa0-68d9b41ab730"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"]}]},{"cell_type":"markdown","source":["단어 집합을 이용하여 정수 인코딩을 진행합니다. 단어 집합에 없는 단어가 등장할 경우에는 정수 1이 할당되지만 이번 실습에서는 학습 데이터에 단어 집합에 없는 단어가 존재하지 않으므로 해당되지 않습니다."],"metadata":{"id":"7vzNVPMeKEk_"}},{"cell_type":"code","source":["def texts_to_sequences(tokenized_X_data, word_to_index):\n","  encoded_X_data = []\n","  for sent in tokenized_X_data:\n","    index_sequences = []\n","    for word in sent:\n","      try:\n","        index_sequences.append(word_to_index[word])\n","      except:\n","        index_sequences.append(word_to_index['<UNK>'])\n","    encoded_X_data.append(index_sequences)\n","  return encoded_X_data\n","\n","X_encoded= texts_to_sequences(tokenized_sentences, word_to_index)\n","print(X_encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyV9LZZVJ9Rq","executionInfo":{"status":"ok","timestamp":1750068006801,"user_tz":-540,"elapsed":11,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"237ea3d6-e297-4927-86cc-10c8cadc369a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]\n"]}]},{"cell_type":"markdown","source":["현재 데이터의 최대 길이를 측정하고, 해당 길이로 패딩을 진행합니다."],"metadata":{"id":"MXy5PJtYLsab"}},{"cell_type":"code","source":["max_len = max(len(l) for l in X_encoded)\n","print('최대 길이: ', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5qEJhJrLmLa","executionInfo":{"status":"ok","timestamp":1750068061483,"user_tz":-540,"elapsed":15,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"c2d89b77-9768-4e99-9fb8-b021f873354b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["최대 길이:  4\n"]}]},{"cell_type":"code","source":["def pad_sequences(sentences,max_len):\n","  features = np.zeros((len(sentences), max_len), dtype=int)\n","  for index, sentence in enumerate(sentences):\n","    if len(sentence) != 0:\n","      features[index,:len(sentence)] = np.array(sentence)[:max_len]\n","  return features\n","\n","X_train = pad_sequences(X_encoded, max_len)\n","print('패딩 결과 :')\n","print(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xI_3Xr6DL2PX","executionInfo":{"status":"ok","timestamp":1750068341321,"user_tz":-540,"elapsed":37,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"470a8bf7-b217-4ed3-9414-faff23cd3207"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩 결과 :\n","[[ 2  3  4  5]\n"," [ 6  7  0  0]\n"," [ 8  9  0  0]\n"," [10 11  0  0]\n"," [12 13  0  0]\n"," [14  0  0  0]\n"," [15 16  0  0]]\n"]}]},{"cell_type":"markdown","source":["모든 데이터의 길이가 4로 변환된 것을 확인하였습니다. 이제 nn.Embedding()를 이용하여 모델을 설계합니다."],"metadata":{"id":"Wcpx-fVIM_id"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader, TensorDataset"],"metadata":{"id":"4H09bksxM6ji","executionInfo":{"status":"ok","timestamp":1750068411813,"user_tz":-540,"elapsed":38,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class SimpleModel(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(SimpleModel, self).__init__()\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    self.flatten = nn.Flatten() # 평평하게 1차원으로 변경\n","    self.fc = nn.Linear(embedding_dim*max_len, 1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","     # embedded.shape == (배치 크기, 문장의 길이, 임베딩 벡터의 차원)\n","     embedded = self.embedding(x)\n","\n","     # flattend.shape == (배치 크기, 문장의 길이 × 임베딩 벡터의 차원)\n","     flattened = self.flatten(embedded)\n","\n","     # output.shape == (배치 크기, 1)\n","     output = self.fc(flattened)\n","     return self.sigmoid(output)"],"metadata":{"id":"_y1p4yAsNLwx","executionInfo":{"status":"ok","timestamp":1750068915240,"user_tz":-540,"elapsed":6,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["모델 객체를 선언합니다. 임베딩 벡터의 크기는 100으로 정했습니다."],"metadata":{"id":"ZBsmmGZ2Omr9"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","embedding_dim = 100\n","simple_model= SimpleModel(vocab_size, embedding_dim).to(device)"],"metadata":{"id":"Xdi6krEjOl3z","executionInfo":{"status":"ok","timestamp":1750068916771,"user_tz":-540,"elapsed":51,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["출력층에 로지스틱 회귀를 이용한 이진 분류 문제를 푸는 모델이므로 손실 함수로는 바이너리 크로스엔트로피 함수에 해당하는 nn.BCELoss()를 사용합니다."],"metadata":{"id":"xvi3qAFsPL3p"}},{"cell_type":"code","source":["criterion = nn.BCELoss()\n","optimizer = Adam(simple_model.parameters())"],"metadata":{"id":"clmNBDzbO6Wi","executionInfo":{"status":"ok","timestamp":1750069013353,"user_tz":-540,"elapsed":6825,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["데이터를 배치 크기 2로 설정한 데이터로더로 변환합니다."],"metadata":{"id":"dJNpBNtdPe6S"}},{"cell_type":"code","source":["train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long),\n","                              torch.tensor(y_train, dtype=torch.float32))\n","\n","train_dataloader = DataLoader(train_dataset, batch_size = 2)"],"metadata":{"id":"5oNkvPFVPc92","executionInfo":{"status":"ok","timestamp":1750069106394,"user_tz":-540,"elapsed":22,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["데이터가 7개였으므로 배치 크기 2로 묶으면 총 묶음은 4개(2개, 2개, 2개, 1개)가 됩니다."],"metadata":{"id":"6wkGiNVIP3ZB"}},{"cell_type":"code","source":["print(len(train_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cALWSaxyP1V3","executionInfo":{"status":"ok","timestamp":1750069150357,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d70d4398-f07f-45ec-acc5-7545239ffe12"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"markdown","source":["총 10번 학습합니다."],"metadata":{"id":"EK_Hz2xAQCn6"}},{"cell_type":"code","source":["for epoch in range(10):\n","  for inputs, targets in train_dataloader:\n","    # inputs.shape == (배치크기, 문장 길이)\n","    # targets.shape == (배치크기)\n","    inputs, targets = inputs.to(device), targets.to(device)\n","\n","    optimizer.zero_grad()\n","\n","    #outputs.shape == (배치 크기)\n","    outputs = simple_model(inputs).view(-1)\n","\n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","\n","  print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mc4tyjtVQAFA","executionInfo":{"status":"ok","timestamp":1750069377736,"user_tz":-540,"elapsed":272,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d3f27f7c-4623-4850-81c5-9cd56fa2d10b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.3473931550979614\n","Epoch 2, Loss: 1.0007615089416504\n","Epoch 3, Loss: 0.71290522813797\n","Epoch 4, Loss: 0.5088858604431152\n","Epoch 5, Loss: 0.3772549033164978\n","Epoch 6, Loss: 0.29641881585121155\n","Epoch 7, Loss: 0.24746374785900116\n","Epoch 8, Loss: 0.21727779507637024\n","Epoch 9, Loss: 0.19745640456676483\n","Epoch 10, Loss: 0.18272405862808228\n"]}]},{"cell_type":"markdown","source":["###2. 사전 훈련된 임베딩을 사용하는 경우\n","구글에서 사전 학습시킨 Word2Vec 모델을 사용하여 문제를 풀어봅시다. 우선 구글에서 사전 학습시킨 Word2Vec 모델을 다운로드 합니다."],"metadata":{"id":"F7BmyUlLRF_F"}},{"cell_type":"code","source":["!pip install gdown\n","!gdown https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wt8aooPkQ3hx","executionInfo":{"status":"ok","timestamp":1750069511714,"user_tz":-540,"elapsed":42061,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"dcd56934-6d4d-48c8-a241-a28e3ca5ba37"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j\n","From (redirected): https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j&confirm=t&uuid=61ff900d-a99c-4bd9-9126-c07e5ae2cbef\n","To: /content/GoogleNews-vectors-negative300.bin.gz\n","100% 1.65G/1.65G [00:30<00:00, 53.6MB/s]\n"]}]},{"cell_type":"code","source":["# 구글에 사전 훈련되 Word2vec 모델을 로드 합니다.\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"],"metadata":{"id":"Iivs62M1RODJ","executionInfo":{"status":"ok","timestamp":1750070300351,"user_tz":-540,"elapsed":77076,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["위 모델은 각 벡터가 300차원으로 구성되어져 있습니다. 풀고자 하는 문제의 단어 집합 크기의 행과 300개의 열을 가지는 행렬 생성합니다. 이 행렬의 값은 전부 0으로 채웁니다. 이 행렬에 사전 훈련된 임베딩 값을 넣어줄 것입니다."],"metadata":{"id":"GTjUHBA9UIoK"}},{"cell_type":"code","source":["embedding_matrix = np.zeros((vocab_size, 300))\n","print('임베딩 행렬의 크기: ', embedding_matrix.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3S1qSfMUGA9","executionInfo":{"status":"ok","timestamp":1750070314409,"user_tz":-540,"elapsed":28,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"017a292a-9c3a-45e8-e11e-c236c589c541"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["임베딩 행렬의 크기:  (17, 300)\n"]}]},{"cell_type":"markdown","source":["word2vec_model에서 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴받을텐데, 만약 word2vec_model에 특정 단어의 임베딩 벡터가 없다면 None을 리턴하도록 하는 함수 get_vector()를 구현합니다."],"metadata":{"id":"wFNbROPpUlXj"}},{"cell_type":"code","source":["def get_vector(word):\n","  if word in word2vec_model:\n","    return word2vec_model[word]\n","\n","  else:\n","    return None"],"metadata":{"id":"BoqMPhAWUZpe","executionInfo":{"status":"ok","timestamp":1750070431249,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["단어 집합으로부터 단어를 1개씩 호출하여 word2vec_model에 해당 단어의 임베딩 벡터값이 존재하는지 확인합니다. 만약 None이 아니라면 존재한다는 의미이므로 임베딩 행렬에 해당 단어의 인덱스 위치의 행에 임베딩 벡터의 값을 저장합니다."],"metadata":{"id":"ubXET-WrVGGg"}},{"cell_type":"code","source":["# <PAD>를 위한 0번과 <UNK>를 위한 1번은 실제 단어가 아니므로 맵핑에서 제외\n","for word, i in word_to_index.items():\n","  if i > 2:\n","    temp = get_vector(word)\n","    if temp is not None:\n","      embedding_matrix[i] = temp"],"metadata":{"id":"vM-geGHvU4yd","executionInfo":{"status":"ok","timestamp":1750070635843,"user_tz":-540,"elapsed":9,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["현재 풀고자하는 문제의 17개의 단어와 맵핑되는 임베딩 행렬이 완성됩니다. 0번 단어는 패딩을 위한 용도이므로 사전 훈련된 임베딩 벡터값이 불필요합니다. 이에 따라 초기값인 0벡터로 초기화가 되어져 있습니다. embedding_matrix의 0번 위치의 벡터를 출력해봅시다."],"metadata":{"id":"a3NS0ugzV1nJ"}},{"cell_type":"code","source":["# <PAD>나 <UNK>의 경우는 사전 훈련된 임베딩이 들어가지 않아서 0벡터임\n","print(embedding_matrix[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4wZv5aohVqvS","executionInfo":{"status":"ok","timestamp":1750070688298,"user_tz":-540,"elapsed":15,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"98c9e871-1307-4738-999c-38a06d48d433"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}]},{"cell_type":"markdown","source":["0이 300개 채워진 벡터임을 확인하였습니다. 이제 다른 단어들도 제대로 맵핑이 됐는지 확인해볼까요? 기존의 단어 집합에서 단어 'great'가 정수로 몇 번인지 확인합니다."],"metadata":{"id":"e0v5P2zcV6mw"}},{"cell_type":"code","source":["word_to_index['great']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tum-xr1oV3j-","executionInfo":{"status":"ok","timestamp":1750070712397,"user_tz":-540,"elapsed":31,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"80ad7562-e2a1-49de-9377-b14c4caa10e6"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["3번 임을 확인했습니다. 이에 따라서 사전 훈련된 word2vec_model에서의 'great' 벡터와 현재 사전 훈련된 임베딩 벡터가 맵핑된 embedding_matrix의 3번 벡터가 동일한지 확인합니다."],"metadata":{"id":"5XiGd-tOWE8E"}},{"cell_type":"code","source":["# word2vec_model에서 'great'의 임베딩 벡터\n","# embedding_matrix[3]이 일치하는지 체크\n","np.all(word2vec_model['great'] == embedding_matrix[3])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFFdeUIqV9by","executionInfo":{"status":"ok","timestamp":1750070756501,"user_tz":-540,"elapsed":54,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"9e7136ff-172d-423d-ff92-2d56ddb6f25e"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["동일한 것을 확인하였습니다. 이는 현재 3번 위치에 단어 'great' 벡터가 정상적으로 할당되었음을 의미합니다. 이제 사전 훈련된 임베딩을 이용한 모델을 구현합니다."],"metadata":{"id":"8ERjiiRgWQkO"}},{"cell_type":"code","source":["class PretrainedEmbeddingModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(PretrainedEmbeddingModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Linear(embedding_dim * max_len, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        flattened = self.flatten(embedded)\n","        output = self.fc(flattened)\n","        return self.sigmoid(output)\n"],"metadata":{"id":"agxISHqWWIM3","executionInfo":{"status":"ok","timestamp":1750071832023,"user_tz":-540,"elapsed":39,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["모델 객체를 선언합니다. 이때 임베딩 벡터의 크기는 embedding_matrix에서 이미 정해진 임베딩 벡터의 차원인 300으로 해야만 합니다."],"metadata":{"id":"HdzTQSy1Xh9M"}},{"cell_type":"code","source":["pretraiend_embedding_model = PretrainedEmbeddingModel(vocab_size, 300).to(device)"],"metadata":{"id":"6Ebow-TtXfMd","executionInfo":{"status":"ok","timestamp":1750071832434,"user_tz":-540,"elapsed":32,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["출력층에 로지스틱 회귀를 이용한 이진 분류 문제를 푸는 모델이므로 손실 함수로는 바이너리 크로스엔트로피 함수에 해당하는 nn.BCELoss()를 사용합니다."],"metadata":{"id":"K47RtHdkYL-3"}},{"cell_type":"code","source":["criterion = nn.BCELoss()\n","optimizer = Adam(pretraiend_embedding_model.parameters())"],"metadata":{"id":"Ujl_FhG2XsGo","executionInfo":{"status":"ok","timestamp":1750071832754,"user_tz":-540,"elapsed":7,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["데이터를 배치크기 2로 설정한 데이터로더 변환합니다."],"metadata":{"id":"fGLT-VC5YSMO"}},{"cell_type":"code","source":["train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long),\n","                              torch.tensor(y_train, dtype=torch.float32))\n","train_dataloader = DataLoader(train_dataset, batch_size=2)"],"metadata":{"id":"r-jboSNJYRS6","executionInfo":{"status":"ok","timestamp":1750071833352,"user_tz":-540,"elapsed":12,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["print(len(train_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCXREp67Yv9F","executionInfo":{"status":"ok","timestamp":1750071833376,"user_tz":-540,"elapsed":15,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d1f7239a-0e7b-4137-b03c-a2949c705afe"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"markdown","source":["총 10번에 학습 수행"],"metadata":{"id":"cO4uKIPOZWjy"}},{"cell_type":"code","source":["for epoch in range(10):\n","    for inputs, targets in train_dataloader:\n","        # inputs.shape == (배치 크기, 문장 길이)\n","        # targets.shape == (배치 크기)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # outputs.shape == (배치 크기)\n","        outputs = pretraiend_embedding_model(inputs).view(-1)\n","\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4EuOwBh-Zr-f","executionInfo":{"status":"ok","timestamp":1750071834166,"user_tz":-540,"elapsed":81,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"fa1c188a-3d1a-4699-b9dd-7f8763f7dc6a"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.7093459963798523\n","Epoch 2, Loss: 0.64645916223526\n","Epoch 3, Loss: 0.5838637948036194\n","Epoch 4, Loss: 0.5248922109603882\n","Epoch 5, Loss: 0.47041550278663635\n","Epoch 6, Loss: 0.42063507437705994\n","Epoch 7, Loss: 0.37548696994781494\n","Epoch 8, Loss: 0.3347833454608917\n","Epoch 9, Loss: 0.298271119594574\n","Epoch 10, Loss: 0.26566213369369507\n"]}]},{"cell_type":"markdown","source":["사전 훈련된 임베딩을 이용하여 기존 모델 대비 더 높은 성능을 얻는 예시는 '사전 훈련된 임베딩을 이용한 성능 상승 시키기(https://wikidocs.net/217099)' 실습을 참고하시기 바랍니다."],"metadata":{"id":"2uVz8MLwaTjE"}},{"cell_type":"code","source":[],"metadata":{"id":"bTemYL1-Z20B"},"execution_count":null,"outputs":[]}]}