{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwYMjjYS2HU1XWmCjI/ahU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 로라 병합 및 허깅페이스 업로드하기\n","이제 학습이 끝났으니 해당 모델을 허깅페이스에 업로드 해보도록 하겠습니다.\n"],"metadata":{"id":"6UDQRSIso3Hu"}},{"cell_type":"markdown","source":["###주의 사항\n","이번 실습을 진행하기 위해서는 학습한 모델이 현재 경로에 존재해야 하고, 또한 기존에 실습했던 주피터\n","노트북을 종료하고 새로운 주피터 노트북을 생성해야 합니다.(런팟을 종료하는 것이 아닙니다.) 아래의\n","주의 사항을 따르면 됩니다.\n","1. 해당 실습은 이전 실습과 이어서 진행한다고 가정합니다. 따라서 현재 실습 환경에는 학습이 현재\n","경로의 qwen2-7b-rag-ko 내부에 checkpoint-285 디렉토리가 생성된 상태여야 합니다.\n","2. 이전에 학습을 진행했던 주피터 노트북 상단의 Kernel 버튼을 클릭하고 이어서 ShudDown\n","Kernel 버튼을 클릭한 후에 이번 실습을 진행해야 합니다. 해당 주피터 노트북을 종료하는 과\n","정으로 커널을 종료한다고 표현합니다. 저자의 경우 모델을 학습했던 주피터 노트북 파일이 ‘1.\n","tuning_example.ipynb‘ 이므로 해당 주피터 노트북의 커널을 종료하였습니다.\n","3. 이제 주피터 노트북을 새로 생성하세요. 저자의 경우 ‘2. merge and upload.ipynb‘라는 이름의 주\n","피터 노트북을 생성하였습니다. 그리고 이제 이 노트북에서 실습을 진행하겠습니다.\n","4. 참고로 런팟에서 주피터 노트북으로 실습할 때 에러가 나는 경우는 대부분 해당 주피터 노트북의\n","Kernel 버튼을 클릭하고 이어서 Restart Kernel을 하고 나서 pip 부분은 건너뛰고 다시\n","실습하면 해결되는 경우가 많으므로 참고하세요.\n"],"metadata":{"id":"_xGlXs85pFfi"}},{"cell_type":"markdown","source":["### 1. LoRA병합\n","학습이 완료된 후에는 LoRA 어댑터를 베이스 모델과 병합하여 독립적인 모델로 만들 수 있습니다. 먼저 필요한 라이브러리들을 임포트하고 경로를 설정합니다."],"metadata":{"id":"Nk6sD4RfpKWu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAZv1bUsoyTd"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","\n","# 경로 설정\n","base_model_path = \"Qwen/Qwen2-7B-Instruct\"\n","adapter_path = \"./qwen2-7b-rag-ko/checkpoint-285\"\n","merged_model_path = \"./output_dir\"\n","\n","# 디바이스 설정\n","device_arg = {\"device_map\": \"auto\"}"]},{"cell_type":"markdown","source":["- adapter_path는 학습된 LoRA어댑처가 저장된 경로입니다. 여기서 qwen2-7b-rag-ko\n","안의 checkpoint-285를 사용합니다.\n","- base_model_id는 원본 베이스 모델입니다. LoRA 학습 시 사용했던 것과 동일한 모델이어야 합니다\n","- merged_path는 병합된 모델이 저장될 경로입니다. 여기서는 “merged” 라는 폴더에 저장됩니다."],"metadata":{"id":"9aoLryYLqIds"}},{"cell_type":"code","source":["# 베이스 모델 로드\n","print(f\"Loading base model from: {base_model_path}\")\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_path,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    **device_arg\n",")\n"],"metadata":{"id":"MEcfpW7bqbBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["베이스 모델을 업로드 합니다. return_dict=True는 모델의 출력을 딕셔너리 형태로 반환하도록 합니다.\n"],"metadata":{"id":"J_gQ7-IAqs-B"}},{"cell_type":"code","source":["# LoRA 어댑터 로드 및 병합\n","print(f\"Loading and merging PEFT from: {adapter_path}\")\n","peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n","merged_model = peft_model.merge_and_unload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"8ZJPZbdpq1Ii","executionInfo":{"status":"error","timestamp":1758280321575,"user_tz":-540,"elapsed":77,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"47ab5ce5-0b4e-40b4-fd84-872074626095"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'adapter_path' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2674612977.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LoRA 어댑터 로드 및 병합\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading and merging PEFT from: {adapter_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpeft_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmerged_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_and_unload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'adapter_path' is not defined"]}]},{"cell_type":"markdown","source":["PeftModel.from_pretrained() 함수는 베이스 모델 위에 학습된 LoRA 어댑터를 로드합니다. 이함수는 베이스 모델과 어댑터를 함께 관리하는 모델 객체를 생성합니다. merge_and_unload() 함수\n","는 LoRA 어댑터의 가중치를 베이스 모델에 병합합니다."],"metadata":{"id":"ok68KGzOrKeF"}},{"cell_type":"code","source":["# 토크나이저 로드\n","tokenizer = AutoTokenizer.from_pretrained(base_model_path)"],"metadata":{"id":"ObfUJhoirHZs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["베이스 모델에서 토크나이저를 로드합니다. 토크나이저는 텍스트를 토큰으로 변환하고 다시 텍스트로  변환하는 역할을 합니다."],"metadata":{"id":"AqrXh5v2rTsq"}},{"cell_type":"code","source":["# 저장\n","print(f\"Saving merged model to: {merged_model_path}\")\n","model.save_pretrained(merged_model_path)\n","tokenizer.save_pretrained(merged_model_path)\n","print(\"모델과 토크나이저 저장 완료\")"],"metadata":{"id":"M_L2DDXlrcab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["병합된 모델, 즉 학습된 최종 모델을 저장합니다. 마지막으로 모델과 같은 경로에 토크나이저를 저장합니다. 일반적으로 모델과 토크나이저는 함께저장합니다. 또한 허깅페이스에도 모델과 토크나이저를 같이 업로드할 것입니다. 이는 사용자들에게 해당 모델을 사용할때는 해당 토크나이저를 사용해야 한다는 것을 알려주는 역할을 합니다."],"metadata":{"id":"qMlGNDTZrnPv"}},{"cell_type":"markdown","source":["###2. 허깅페이스 업로드\n","병합된 모델을 허깅페이스 허브에 업로드 하여 다른 사람과 공유하거나 나중에 쉽게 사용할 수 있도록 할 수 있습니다."],"metadata":{"id":"2nsUOtdDr7kc"}},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","api = HfApi()\n","\n","username = \"jin63\""],"metadata":{"id":"UAl9wwImr5ra"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","먼저 허깅페이스 허브와 상호작용하기 위한 API 클라이언트를 초기화합니다. HfApi()는 허깅페이스 허브의 모든 기능에 접근할 수 있는 고수준 인터페이스를 제공합니다. 이 API 를 통해 저장소 생성, 파일 업로드, 모델 정보 조회 등의 작업을 수행할 수 있습니다. username 변수에는 자신의 허깅페이스 사용자명 (허깅페이스 로그인 후 보이는 닉네임) 을 설정합니다. 저자의 경우 “iamjoon” 이라는 사용자명을\n","사용하고 있습니다"],"metadata":{"id":"Wa1HXxGAtZ01"}},{"cell_type":"code","source":["MODEL_NAME =  'qwen2-7b-rag-ko-checkpoint-285'"],"metadata":{"id":"npT2a7a6te_J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","업로드할 모델의 이름을 정의합니다. 모델명은 가능한 한 구체적이고 설명적으로 작성하는 것이 좋습니다. 여기서는 qwen2-7b-rag-ko-checkpoint-285라는 이름으로 업로드합니다"],"metadata":{"id":"ll7AIXOYtkF0"}},{"cell_type":"code","source":["api.create_repo(\n","token=\"hf로 시작하는 여러분의 키 값\",\n","repo_id=f\"{username}/{MODEL_NAME}\",\n","repo_type=\"model\"\n",")"],"metadata":{"id":"-NG_HDDktpu7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["허깅페이스 허브에 새로운 저장소를 생성합니다. token 매개변수에는 허깅페이스 액세스 토큰을 입력 합니다. 이 토큰은 허깅페이스 웹사이트의 로그인 후에 발급받는 키 값입니다. Settings > Access\n","Tokens에서 생성할 수 있으며, Write 권한이 있는 토큰이어야 합니다. repo_id는 저장소의 고유 식별자로, 사용자명/모델명 형태로 구성됩니다. repo_type=\"model\"은 이 저장소가 모델을 저장하는 용도임을 명시합니다.\n"],"metadata":{"id":"oV2bpo10t4Or"}},{"cell_type":"code","source":["api.upload_folder(\n","    token=\"hf로 시작하는 여러분의 키값\",\n","    repo_id=f\"{username}/{MODEL_NAME}\",\n","    folder_path=\"merged\",\n",")\n"],"metadata":{"id":"connAdeTuK_Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["로컬에 저장된 모델 폴더를 허깅페이스 허브에 업로드합니다. folder_path=\"merged\"는 앞서 병\n","합된 모델이 저장된 로컬 폴더 경로를 지정합니다. 이 폴더에는 모델 가중치 파일, 설정 파일, 프로세서\n","(VLM 의 토크나이저) 파일 등 모델 실행에 필요한 모든 파일들이 포함되어 있습니다. 업로드 과정에서는\n","폴더 내의 모든 파일이 허깅페이스 허브로 전송되며, 파일 크기에 따라 몇 분에서 몇십 분이 소요될 수 있\n","습니다.\n","업로드가 완료되면 다른 사용자들이 이 모델을 다운로드하고 사용할 수 있게 됩니다. 업로드된 모\n","델은 https://huggingface.co/{username}/{MODEL_NAME} 주소에서 확인할 수 있습니\n","다. 저자가 업로드 한 모델의 경우 https://huggingface.co/iamjoon/qwen2-7b-rag-kocheckpoint-285 주소에서 확인할 수 있습니다."],"metadata":{"id":"UxelxT5TuSbS"}}]}