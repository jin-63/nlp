{"cells":[{"cell_type":"markdown","metadata":{"id":"aeDJMz9PNIsT"},"source":["## 라마3 한국어 뉴스 파인튜닝\n","금융 뉴스로 부터 종목에 영향을 주는 뉴스인지 판별하는 모델을 파인튜닝하는 과정을 진행해보겠습니다. 실습은 Runpod클라우드에서 A100 SXM GPU를 사용하여 진행하였다고 가정합니다."]},{"cell_type":"markdown","metadata":{"id":"uq22svdANgul"},"source":["###1. 패키지 설치 및 데이터로드\n","**1) 패키지 설치 및 임포트**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eW3ojMFWNHp1","outputId":"64846311-46a8-4db4-def9-651c753606ae","executionInfo":{"status":"ok","timestamp":1755515915301,"user_tz":-540,"elapsed":348430,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.4.0\n","  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n","Collecting transformers==4.45.1\n","  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets===3.0.1\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Collecting accelerate==0.34.2\n","  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n","Collecting trl==0.11.1\n","  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n","Collecting peft==0.13.0\n","  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.14.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n","  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.0.0 (from torch==2.4.0)\n","  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (0.34.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (0.6.2)\n","Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n","  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.1) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets===3.0.1) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets===3.0.1) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets===3.0.1) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets===3.0.1) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets===3.0.1) (0.70.16)\n","Collecting fsspec (from torch==2.4.0)\n","  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets===3.0.1) (3.12.15)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (5.9.5)\n","Collecting tyro>=0.5.11 (from trl==0.11.1)\n","  Downloading tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.5.82)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets===3.0.1) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets===3.0.1) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets===3.0.1) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets===3.0.1) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets===3.0.1) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets===3.0.1) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets===3.0.1) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (1.1.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.45.1) (2025.8.3)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.1) (0.17.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.1) (13.9.4)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n","  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.11.1) (4.4.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets===3.0.1) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets===3.0.1) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets===3.0.1) (2025.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets===3.0.1) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (0.1.2)\n","Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.28-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n","Installing collected packages: triton, shtab, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusolver-cu12, nvidia-cudnn-cu12, tyro, torch, tokenizers, transformers, datasets, accelerate, trl, peft\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.4\n","    Uninstalling tokenizers-0.21.4:\n","      Successfully uninstalled tokenizers-0.21.4\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.55.1\n","    Uninstalling transformers-4.55.1:\n","      Successfully uninstalled transformers-4.55.1\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 4.0.0\n","    Uninstalling datasets-4.0.0:\n","      Successfully uninstalled datasets-4.0.0\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 1.10.0\n","    Uninstalling accelerate-1.10.0:\n","      Successfully uninstalled accelerate-1.10.0\n","  Attempting uninstall: peft\n","    Found existing installation: peft 0.17.0\n","    Uninstalling peft-0.17.0:\n","      Successfully uninstalled peft-0.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.0 which is incompatible.\n","torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.34.2 datasets-3.0.1 fsspec-2024.6.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 peft-0.13.0 shtab-1.7.2 tokenizers-0.20.3 torch-2.4.0 transformers-4.45.1 triton-3.0.0 trl-0.11.1 tyro-0.9.28\n"]}],"source":["!pip install torch==2.4.0 transformers==4.45.1 datasets===3.0.1 accelerate==0.34.2 trl==0.11.1 peft==0.13.0"]},{"cell_type":"markdown","metadata":{"id":"kswGA1PsOAKA"},"source":["이제 실습에 사용할 각종 라이브러리의 도구들을 임포트 합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxX_5o9hN8pe"},"outputs":[],"source":["from datasets import load_dataset, Dataset\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import LoraConfig\n","from trl import SFTConfig, SFTTrainer"]},{"cell_type":"markdown","metadata":{"id":"BNtx0u99OssY"},"source":["각 도구들의 쓰임새는 다음과 같습니다.\n","• load_dataset, Dataset: 다양한 데이터셋을 쉽게 로드하고 처리할 수 있습니다. load_dataset 은 허깅페이스에 업로드 된 데이터셋을 불러오거나 로컬 파일을 로드하는 데 사용되며, Dataset 은 개별 데이터셋 객체를 다룰 때 사용됩니다.\n","\n","- AutoModelForCausalLM: 허깅페이스 Transformers 라이브러리에서 제공하는 모델 다운로드를 위한 도구입니다. 언어 모델을 로드하는 데 사용됩니다.\n","- AutoTokenizer: 특정 모델에 맞는 토크나이저를 자동으로 불러오는 도구입니다. 토크나이저는 텍\n","스트를 언어 모델이 처리할 수 있는 정수 시퀀스로 변환하거나 정수 시퀀스를 다시 텍스트 문자열로 복원하는 역할을 합니다.\n","- LoraConfig: 이번 실습에서 사용할 학습 방법인 LoRA(Low‑Rank Adaptation) 튜닝을 사용할 때 필\n","요한 설정값을 정의합니다. 학습 시에 거대 언어 모델 전체를 업데이트하는 것이 아니라, 거대 언어\n","모델의 특정 부분만 업데이트하여 보다 효율적으로 학습하는 방식입니다. 여기서는 로라 튜닝을 할 때 로라 튜닝과 연관된 각종 설정값들을 결정합니다.\n","- SFTConfig: 모델을 학습할 때 필요한 다양한 설정값을 정의하는 도구입니다. 학습 과정에서 모델이 어떻게 업데이트될지를 조정하며 여기에는 학습률, 배치 크기, 옵티마이저 등의 설정을 포함합니다. 모델 전체를 학습할 때도 쓰일 수 있지만, 현재 실습과 같이 로라 튜닝에서처럼 모델의 특정부분만 학습하는 방식에서도 사용합니다. SFTConfig 에서 설정하는 이 설정값들은 모델의 학습 성능과 안정성에 큰 영향을 줍니다.\n","- SFTTrainer: 실제 학습을 수행하는 클래스입니다. 주어진 데이터셋을 이용해 파인 튜닝 과정을 자동으로 수행하며, 특정 부분만 업데이트하는 LoRA 같은 학습 기법도 적용 가능합니다. 모델, 데이터셋, 학습 설정을 한 번에 입력하여 효율적인 학습을 진행할 수 있도록 돕습니다."]},{"cell_type":"markdown","source":["**2) 데이터 전처리**\n","인터넷을 통해 사용할 데이터를 다운로드 하고 특정 형식으로 전처리를 진행해 보겠습니다."],"metadata":{"id":"sea7suEcQXKG"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270,"referenced_widgets":["067381e6e1e746859d32d244aaf76ac3","a36f1c4894714cd68c79c338f613035f","fc905cc9d4304b03befca94fb9fd4d4a","973a19e297864e44b990ac84982f09c3","4e6350c122bd4ea3a66f2bab416c82d1","bfbecc04b30a45d08f6c6274f1501b2b","a2d14ade52af44029bbfe87a47842969","544f176a482a4dc99cbec65234a1dfac","8b199b82d1514a018a487514152c4c3e","29ab92ac32034afa8298aa6f66b376de","fcf3dd0e1fe8413895a0cd9d9f02a567","0052ace845d74edd9254c6fd9e63f8e8","a003f836fb13433b95cadea2eb61ce6f","c6435a7ac5204835adfc47bccd1a8e8d","9b12a33a20b04300a02a5f4c876dfc67","e1e38b70b0cf41d885dafad8db87d203","d983c14eb7a143dc8f0c36da88a95554","423e4910deae487c82fc9abd33f25b2a","eeace58a9616405ab004eccc3b3d1e25","b9041a75d9bf415c992afead6d61d272","c9a4a99dd09f423f86af47520bcd66fc","e4c983b0ff24434d8584a7ea571dc37b","624acb8dfdc54aa6a02a662ce813b4ee","58c0f089847e42f9a65bfe8f92f58e9c","2e6ed55a0b60476eb26c485fa69e4f45","fa456217341b4cccb69752122c74f9c5","40e69c697d904f4db1b33846ba89345f","444231a58ad3454383d55e25e6fa048e","b474e78324aa41ae9fa4afba6d6527c5","513eace20de448ada204ca685f07b609","4b9461dc681e4af99b074f07938f02ae","ddc9940c23f64fc6bb7a4112f6f9e371","50886c8de04b4a7794563f7ed4e66ea3"]},"id":"YuDvL5gAPJ27","executionInfo":{"status":"ok","timestamp":1755515983076,"user_tz":-540,"elapsed":3384,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"3171b4da-3f1d-448f-f14b-fb0d694fd8e3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/781 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067381e6e1e746859d32d244aaf76ac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["data/train-00000-of-00001.parquet:   0%|          | 0.00/1.70M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0052ace845d74edd9254c6fd9e63f8e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/991 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"624acb8dfdc54aa6a02a662ce813b4ee"}},"metadata":{}}],"source":["#1. 허깅 페이스 허브에서 데이터셋 로드\n","dataset = load_dataset(\"iamjoon/finance_news_summarizer\", split=\"train\")"]},{"cell_type":"markdown","source":["허깅페이스 허브에서 앞서 설명한 금융 뉴스 데이터셋을 불러옵니다. load_dataset() 함수를 사용하는데, \"iamjoonfinance_news_summarizer\"이라는 데이터셋을 불러옵니다. 이 데이터셋은\n","우리가 사용할 전체 데이터를 담고 있습니다. 이제 데이터셋의 총 샘플 수를 계산하여 출력합니다."],"metadata":{"id":"K18pcFekQxFS"}},{"cell_type":"code","source":["# 2. 데이터 전체 크기만 출력\n","print(\"전체 데이터 크기: \", len(dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3opiutZ3Qtnp","executionInfo":{"status":"ok","timestamp":1755515983114,"user_tz":-540,"elapsed":35,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"813910a5-1609-46b6-98ef-7d2995da6795"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["전체 데이터 크기:  991\n"]}]},{"cell_type":"markdown","source":["전체 데이터의 개수는 991 개입니다. 이제 전체 데이터를 학습 데이터와 테스트 데이터로 분할해야 합니다. test_ratio 변수에 0.5 를 할당하여 전체 데이터의 50% 를 테스트 데이터로, 나머지를 학습 데이터로 사용하도록 비율을 지정합니다."],"metadata":{"id":"F6v-IQ7YRGn1"}},{"cell_type":"code","source":["# 3. train/test 분할 비율설정 (0.5)\n","test_ratio = 0.5\n","train_data = []\n","test_data = []"],"metadata":{"id":"IYF1Yvc1RBLw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["일반적으로는 머신 러닝에서는 학습 데이터의 양이 더 많고 성능을 평가하기 위한 테스트 데이터의 양을 더 적게 할당합니다. 하지만 현 실습에서는 현재 5:5 분할을 선택하였는데 이는 유료 클라우드인 런팟을 사용하므로 실습 시 과도한 학습 비용을 방지하기 위해서 학습 데이터를 적게 설정하였습니다. 비용 문제에 부담이 없다면 더 많은 학습 데이터를 사용하시기 바랍니다. 그리고 분할된 데이터의 인덱스를 저장할 train_data와 test_data라는 빈 리스트를 생성합니다.\n"],"metadata":{"id":"D297BDB2RZyM"}},{"cell_type":"code","source":["# 4. 전체 데이터의 인덱스를 train/test로 분할\n","data_indices = list(range(len(dataset)))\n","test_size = int(len(data_indices)* test_ratio)\n","\n","test_data = data_indices[:test_size]\n","train_data = data_indices[test_size:]"],"metadata":{"id":"zvm4AGT6RT26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["data_indices = list(range(len(dataset))) 코드로 0 부터 데이터셋 길이‑1 까지의 인덱\n","스 리스트를 생성합니다. test_size = int(len(data_indices)* test_ratio)로 테스트\n","데이터셋의 샘플 수를 계산합니다. test_data = data_indices[:test_size]로 인덱스 리스\n","트의 앞부분을 테스트 데이터로 할당합니다. train_data = data_indices[test_size:]로\n","인덱스 리스트의 뒷부분을 학습 데이터로 할당합니다. 이제 OpenAI 형식으로 데이터를 변환하는\n","format_data() 함수를 정의합니다.\n"],"metadata":{"id":"InayEwVpSALf"}},{"cell_type":"code","source":["# 5. OpenAI format으로 데이터 변환을 위한 함수\n","def format_data(sample):\n","  return {\n","    \"messages\": [\n","      {\"role\": \"system\", \"content\": sample[\"system_prompt\"],},\n","      {\"role\": \"user\", \"content\": sample[\"user_prompt\"],},\n","      {\"role\": \"assistant\", \"content\": str(sample[\"assistant\"])},\n","    ],\n","  }"],"metadata":{"id":"HfkaHBOVR18u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["penAI 형식이란 ChatGPT 를 개발한 OpenAI 에서 만든 데이터의 표준 형식을 말하는데, messages라\n","는 리스트 안에 각각의 대화를 역할 (role) 과 내용 (content) 으로 구분하여 담는 구조입니다. 이는 앞에서 GPT‑4 API 실습에서 보았던 형식을 의미합니다. system은 AI 의 행동 지침을, user는 사용자의 질문을, assistant는 AI 의 답변을 나타냅니다. 이제 해당 함수를 실제로 적용해봅시다. 앞서 분할한 train_data와 test_data의 각 샘플에 format_data() 함수를 적용하여 최종 데이터셋을 생성합니다.\n"],"metadata":{"id":"a06KLyVfTLsJ"}},{"cell_type":"code","source":["# 6. 분할된 데이터를 OpenAI format으로 변환\n","train_dataset = [format_data(dataset[i]) for i in train_data]\n","test_dataset = [format_data(dataset[i]) for i in test_data]"],"metadata":{"id":"V2dZWyD5Shy5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["각 인덱스에 해당하는 데이터를 format_data() 함수를 이용해 OpenAI형식으로 변환하여\n","train_dataset과 test_dataset에 저장합니다. 이제 최종적으로 만들어진 train_dataset과\n","test_dataset의 크기를 출력합니다"],"metadata":{"id":"K_qZJhNbTkxH"}},{"cell_type":"code","source":["# 7. 최종 데이터셋 크기 출력\n","print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuHrhvtLTd2-","executionInfo":{"status":"ok","timestamp":1755515983633,"user_tz":-540,"elapsed":21,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"0b7f27e9-cb57-455c-fe28-a123ef41cf0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","전체 데이터 분할 결과: Train 496개, Test 495개\n"]}]},{"cell_type":"markdown","source":["앞서 학습 데이터와 테스트 데이터를 5:5 분할하여 개수가 거의 유사하게 학습 데이터가 496 개, 테스트 데이터가 495 개입니다. 이제 OpenAI 형식으로 전처리가 완료된 데이터를 하나 임의로 출력하여 형식을 이해해보겠습니다. 여기서는 학습 데이터 중 임의로 345 번 샘플을 출력해보겠습니다. messages라는 리스트 안에 데이터가 저장되어져 있으므로 이를 출력합니다."],"metadata":{"id":"AGCE4ts2UM-Q"}},{"cell_type":"code","source":["train_dataset[345][\"messages\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qb-rwJ_HUATj","executionInfo":{"status":"ok","timestamp":1755515983682,"user_tz":-540,"elapsed":33,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2c271ab4-8773-43e9-af8e-175fca9070da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'role': 'system',\n","  'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}'},\n"," {'role': 'user',\n","  'content': '중소기업 하반기 경기전망 작년보다 악화…원자잿값 상승 우려\\n서울 연합뉴스 신선미 기자 중소기업의 올해 하반기 경기전망 지수가 지난해 같은 기간보다 하락한 것으로 나타났다. 5일 중소기업중앙회에 따르면 지난달 15∼24일 중소기업 500곳을 대상으로 실시한 중소기업 경영애로 및 2022년 하반기 경기전망조사 결과 하반기 경기전망지수 SBHI 는 87.6으로 지난해 하반기 91.6 보다 4.0포인트 p 하락했다. 이 지수가 100 이상이면 경기가 개선될 것으로 보는 응답자가 더 많고 100 미만이면 그 반대라는 의미다. 중기중앙회 중기중앙회 제공 하반기 SBHI를 업종별로 보면 제조업의 경우 펄프·종이 및 종이제품업 54.2 섬유제품업 54.2 전기장비업 68.2 은 경기가 악화될 것으로 내다봤고 기타 운송장비업 127.3 가죽·가방 및 신발업 104.6 은 경기가 호전될 것으로 전망했다. 서비스업에서는 부동산업 및 임대업 60.0 도매 및 소매업 84.0 은 경기가 악화될 것으로 봤지만 예술·스포츠 및 여가 관련 서비스업 112.0 은 업황 개선을 전망했다. 하반기 예상되는 애로 요인 복수응답 은 원자재 가격 상승 58.8% 내수 부진 31.2% 인력 수급난 29.8% 금리상승 28.4% 최저임금 상승 19.4% 등의 순이었다. 또 상반기 겪은 애로 요인으로는 원자재가격 상승 62.6% 내수부진 35.2% 인력 수급난 29.8% 금리상승 25.2% 최저임금 상승 22.8% 등의 순으로 응답률이 높았다. 소상공인·중소기업의 경기 개선을 위해 필요한 정부 정책 복수응답 으로는 세금 및 각종 부담금 인하 61.4% 금융지원 45.0% 인력난 해소 34.6% 원자재 수급 안정화 28.6% 근로시간 유연화 20.0% 순으로 꼽혔다. 코로나19 이전 수준의 경영실적 회복 예상 시기에 대해서는 응답자의 27.0%가 2024년 이후 라고 답했고 이어 2023년 상반기 와 2023년 하반기 각 23.0% 2022년 하반기 14.8% 등이었다.'},\n"," {'role': 'assistant',\n","  'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'negative_keywords': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'positive_impact_stocks': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'positive_keywords': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'reason_for_negative_impact': '펄프·종이 및 종이제품업, 섬유제품업, 전기장비업, 부동산업 및 임대업, 도매 및 소매업은 하반기 경기 전망에서 경기가 악화될 것으로 예측되고 있다.', 'reason_for_positive_impact': '기타 운송장비업, 가죽·가방 및 신발업, 예술·스포츠 및 여가 관련 서비스업은 하반기 경기 전망에서 경기가 호전될 것으로 예측되고 있다.', 'summary': '중소기업중앙회 조사 결과, 올해 하반기 중소기업 경기 전망 지수가 작년보다 하락했다. 제조업에서는 펄프, 종이, 섬유, 전기장비업이 경기 악화를 예상했고, 운송장비와 가죽, 신발업이 호전을 전망했다. 원자재 가격 상승과 내수 부진 등이 주요 애로 요인으로, 경기 개선을 위해 세금 및 부담금 인하와 금융지원 등이 필요하다고 응답됐다.'}\"}]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["role이 system인 경우, content에는 위에서 작성한 시스템 프롬프트가 저장되어져 있습니다.\n","role이user인 경우, content에는 현 샘플에서의 사용자의 입력이 저장되어져 있습니다. role이\n","assistant인 경우, content에는 거대 언어 모델이 답변해야 할 내용이 작성되어져 있습니다. 위와\n","같이 system, user, assistant와 같은 role 과 각각 해당하는 content로 구성된 형태를 OpenAI 형\n","식이라고 합니다. 이 형식은 학습을 하기 위한 최종 형식이 아니며 전처리를 위한 중간 단계입니다. 다만,\n","전처리를 위한 중간 단계의 형태로 OpenAI 에서 제안한 형식을 주로 사용하는 것입니다. 학습을 위한 형\n","식은 뒤의 내용에서 거대 언어 모델의 토크나이저를 통해 한 번 더 전처리를 진행하여 챗 템플릿이 적용되\n","어야 합니다.\n","현재 train_dataset과 test_dataset은 데이터 타입이 리스트입니다. 학습을 위해서 타입을\n","Dataset으로 변경해야 합니다."],"metadata":{"id":"Dxm9BdUWga6D"}},{"cell_type":"code","source":["# 리스트 형태에서 다시 Dataset 객체로 변경\n","print(type(train_dataset))\n","print(type(test_dataset))\n","train_dataset = Dataset.from_list(train_dataset)\n","test_dataset = Dataset.from_list(test_dataset)\n","print(type(train_dataset))\n","print(type(test_dataset))"],"metadata":{"id":"InQbLj-tUVVv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755515984036,"user_tz":-540,"elapsed":347,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"b9071c09-838c-4644-c832-d64d9436abdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","<class 'list'>\n","<class 'datasets.arrow_dataset.Dataset'>\n","<class 'datasets.arrow_dataset.Dataset'>\n"]}]},{"cell_type":"markdown","source":["###2. 모델 로드 및 템플릿 적용\n","이제 학습을 진행할 모델과 토크나이저를 로드합니다. 거대 언어 모델을 학습 시에는 반드시 로드해야 할\n","것이 크게 두 가지가 있습니다. 바로 학습할 모델과 이 모델에 입력할 데이터를 전처리하기 위한 도구인\n","토크나이저입니다."],"metadata":{"id":"rD6cS2MtgxTG"}},{"cell_type":"code","source":["# 허깅 페이스 모델 ID\n","model_id = \"NCSOFT/Llama-varco-8B-Instruct\"\n","\n","# Load model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519,"referenced_widgets":["0e84cc62ef0f4df7b9429b1267d8d09a","31925282af894b4598c1e0b5a2963c3e","7d8db26d14ff4a03abed9fc2260cec6f","42d8306f40fd4393bb8b3d0dfbcb943c","97691aeee22a41eca50211c8f9f30e7a","4e8245b8a8c04341bb09dce4b2bd00f8","1a95031e561d4712b7983fef15d1c31b","251eab513b1c4ce6b808b9cc16ca21ef","8ad257a411d44dfb8acb74b866a0c31a","89e2555c27e749be8194b5627621bc9e","1138b5d39c7346199f7406d9ca4fa5da","9d7a5ee09d25481e8c0f98099152b04c","24b3107431fa468a81e5485e94573afe","af2e7adec9934721af8189787fa46acf","5cef37a51a6b4684b50ce6b44fd980a3","2e791b9765b14e5f9a5dcdfea2fd6f29","e22beddc911e4cd4981897c6ce3c455c","83f90e593f7e475185073f1dd716873e","ed5c9febffee4524a9795be63aa47529","8a8832e681684e0ea10200fc86641bac","8831d1d24fb64a9ca0a2973dc72bc9fc","770cdc1201ad4b90bd2ec39bd9fa4474","2e38838e9d6343dfbbc3d6a9138a464e","dca95ac78d574a3a8a4effdea3d50278","c43339f7bba749079fa60a9a2d7fc979","c8d1aafe4891404b8d05705d9ef92b6e","d9c9850ddfa24df6b0dbd730a332bebd","a3f87422cb88460d8612cfdbedd715d7","7ee8765f526b48e4bdeb330d921dcb21","40d9b992cb354dc68d3bab5745717eb6","0c883d3003a64d81b6f3ef4e7d20983b","dbc58fa2b60041d292d9e96656e9d7f2","12648b520560484580c0d4a7c0c03ce8","70b2f63fd6d74642a13209a1767934b8","fee7f511dcac471ebb74013cf7451606","909aa29c1eb6459e8d5be9eec4e4ddf0","0e1e05a278e94a1282f520481bfb909a","cf93934250dc4312880a0be40da5f372","1f4f5c4138cd4bc8a2f762e5454e0b1c","7b3f198f23544241afba94091e427361","e13537b5a7fd4c0dba768934c1c659ad","09d6d9fcd40e4552ae9f036f63963b11","7d68d88c37e94003a5a6e0f7c6dbe749","530b76003ff84677b31667fde1367039","ac5cb4e92d2f42848697d66ee9923200","a5d31b8381854e248ee5472c4e78272e","6b6221e3b1444002832b14b2d10256e3","43ce795974414799ae00d0723719eec3","ea20f57d1eb34ca5aedbd414c2550940","4c68b050570247549e2df37657ba75e1","a9fd8a070eee4c64b18a009dbc76a141","7bf3abdc5f3b447189933e70a8e20210","3753517e9130497f804016e3e82c9566","3efe0bc7eb93448a8ec70752de24035e","1e8f6bcaff0d45958cb510dad0d05b73","eacc14318ccd4cc4ac577ab1886d17e9","a17d4867fd054f7a8a95608a0eb864a1","de4bf52de0d0493aa455ab087d6dff13","56a1ea97cecb4425a70afa0634b032ae","4ec89108edf54193952d12bb0487ed75","0581f003932049378fef038784a95bfc","6d98015a0dd8448bb564971a076f52d0","6106a5ba52f24495a86d5453cee47a78","8618624614834d6b9a1ce39cc132e67f","82ac1dec63a94f9c8bf096fe3e92af57","7d5e2dc7e293483498bdde648a9e1b8b","1c8b9b48b6a540e8a69202cbb4677bf0","d74c0f6d03064b30a42dec657be16147","6686d61aebbb476eb3a2a8441830a885","4c3a35155307498ba6b0d5b0ebf9b4b1","8a7ccc55a8564168b8dfa714cfafeb73","831d61cf8f8647d6b512355e08d9f7a0","f6c32b57424045cb955b98bab1dd7603","a5177f24262e462b8e1eb084009536f1","71399d7634c54f65b97e1e18f8d264ac","0e7f9e2f184f455f955956f80b0818d7","319ad8dc21ad4070955dbfbba77a1dca","e3eac6e98f634d1a88117ff53583781b","a0e7f59b315c47758cc289f40aa616ba","e29bcfd1911244b8828628479c778f0f","7dcf2dbfa3bb414a90ce20d69ac31936","35fd944a56db4d27a5c87b645c30edfa","2af4eeef2a9c4ee7b79d92407337804c","24568b23824e476fa8180083fe4fc32a","90d2bee43e6f4eea8983ed98bd5d3ab2","b054db9dde804095b39eb0eb355dec2a","661462ba03d841d7b2d2eba26ffa05dd","5796f2926a06423f82f51ba868ba9809","beec095dd73147aab7562bd121c7a1bf","aa00af306678462e9caeb067d3359435","74973a0a67d849f4983fdbea76d5ae94","0af1da908413495fbc9021732cf8aa47","46ee9e5c224d41bca98fab794e7fec41","86a04700627f415ca2e02a642d5dc7c5","549a4ba41df940139a2d30885b798dca","617fa085f75b4bd38c4ee511e9a47fbd","fcd80c3ef5df4dd99b046c2fd7a517f5","fd521c6fd31d4b4ba728589b83d831be","6ea49000f7f446148c389070e0daf12d","dbd5e155cb924f78ab6639b42030463a","7c3d12a4758541ac9da9a8665a11a6b5","1466467fe10c42c3bedf7a07858e54a0","c8686c9089f44a64868eae14de52becb","88e3f7a43a984cd99467be9c44732379","5d0dbae1c7624ff28830ab2b251cf11e","ca11c605680b4e21a786b260e7d39045","4f1e48bb2d3b4189bb966f1d7bdc0841","f0c9414f5202435e90b32201345aaaf7","c21d73c52273499d8ab17151d73a6b75","cdf0176695c84da7a1ae7cdbd0454749","d81d042fc1a445379f3715202a991ebf","c9a7e28f278e42e8a80a7541ea0e6a09","6303d18b132e469e88fe93ecddc19c3c","dbd158d36d4c4c1995b59bb1e28eff1d","9fd23e9141e841faabbdb39b4e089bc7","08632377a63141e1a661b152e80ae8dd","28e25eacff744109b9aafd974e775b96","8a9a9215a9a14c988ff9855d64ac3390","bee122f197204e49aa366562a8388765","a775d6d98b774490bb4b3ce545541ce0","ab280b53cb7045a697e207f33d91f24c"]},"id":"yCJXWabsg_fd","executionInfo":{"status":"ok","timestamp":1755516517041,"user_tz":-540,"elapsed":533014,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"42abee81-9290-4b2f-9c92-0a0447b61642"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e84cc62ef0f4df7b9429b1267d8d09a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d7a5ee09d25481e8c0f98099152b04c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e38838e9d6343dfbbc3d6a9138a464e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b2f63fd6d74642a13209a1767934b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac5cb4e92d2f42848697d66ee9923200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eacc14318ccd4cc4ac577ab1886d17e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c8b9b48b6a540e8a69202cbb4677bf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3eac6e98f634d1a88117ff53583781b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beec095dd73147aab7562bd121c7a1bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbd5e155cb924f78ab6639b42030463a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81d042fc1a445379f3715202a991ebf"}},"metadata":{}}]},{"cell_type":"markdown","source":["여기서는 “NCSOFT/Llama‑VARCO‑8B‑Instruct” 라는 모델을 사용할 것입니다. 따라서 모델과 토크나이\n","저 모두 해당 모델의 이름을 인자로 사용한 것을 알 수 있습니다. 해당 모델은 NCSOFT 에서 공개한 한글\n","성능이 뛰어난 모델입니다.\n","\n","토크나이저를 로드하였다면 이제 LLaMA 를 위한 챗 템플릿 (Chat Template) 을 적용해야 합니다. 챗 템\n","플릿이란 거대 언어 모델이 학습할 때 사용하는 특정한 대화 형식입니다. 우리가 사용하는 거대 언어 모\n","델들은 이미 학습된 모델이고, 우리는 이를 로드하여 우리의 데이터로 추가적으로 파인 튜닝하고는 합니\n","다. 이러한 거대 언어 모델은 만들어졌을 당시에 특정 형식에 맞추어서 학습이 된 상태이므로 파인 튜닝\n","시에도 이 형식을 지켜주어야 합니다\n","\n","LLaMA 3 의 챗 템플릿은 다음과 같습니다."],"metadata":{"id":"eabXg75eiC6g"}},{"cell_type":"code","source":["\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","시 스 템 프 롬 프 트 <|eot_id|><|start_header_id|>user<|end_header_id|>\n","유 저 프 롬 프 트 <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","거 대 언 어 모 델 이 해 야 하 는 답 변 <|eot_id|>\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"0CeSJixehjnh","executionInfo":{"status":"ok","timestamp":1755516517077,"user_tz":-540,"elapsed":40,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"f6f887b9-9c79-4907-ef4b-05162aaf8f65"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n시 스 템 프 롬 프 트 <|eot_id|><|start_header_id|>user<|end_header_id|>\\n유 저 프 롬 프 트 <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n거 대 언 어 모 델 이 해 야 하 는 답 변 <|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["위 템플릿에서 주목할 점은 시스템 프롬프트는 <|start_header_id|>system<|\n","end_header_id|>와 <|eot_id|> 사이에 작성되고, 유저 프롬프트는 <|start_header_id|>\n","user<|end_header_id|>와 <|eot_id|> 사이에 작성되어져 있고, 어시스턴트의 응답 (실제 LLM\n","의 응답이 되어야 하는 부분) 은 <|start_header_id|>assistant<|end_header_id|>와\n","<|eot_id|> 사이에 작성이 되어져 있다는 점입니다. LLaMA 챗 템플릿으로 데이터를 가공하는 방법은\n","토크나이저의 apply_chat_template()에 OpenAI 형식으로 가공된 데이터를 넣으면 됩니다. 다음\n","은 학습 데이터 중 0 번 샘플을 챗 템플릿으로 가공한 후 출력하는 코드를 보여줍니다."],"metadata":{"id":"viGWkEvxienZ"}},{"cell_type":"code","source":["# 템플릿 적용\n","text = tokenizer.apply_chat_template(\n","    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",")\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bW02jzduiPXm","executionInfo":{"status":"ok","timestamp":1755516517145,"user_tz":-540,"elapsed":58,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8fb2b3cf-5a5a-4fbb-d68f-39b9bb26ee88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n","두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n","큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n","아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n","해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n","\n","만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n","\n","답변:\n","{\"is_stock_related\": False,\n","\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n","\n","만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n","\n","답변:\n","{\"is_stock_related\": True,\n","\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n","\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n","\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n","\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n","\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n","\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n","\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n","금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n"]}]},{"cell_type":"markdown","source":["### 3. 로라 학습을 위한 설정값\n","이제 학습을 위해 각종 설정값들을 지정해야 합니다. 이번 실습에서는 LoRA(Low‑Rank Adaptation) 학습\n","방식을 사용하여 파인 튜닝을 해볼 것입니다. LoRA 는 거대한 언어 모델을 학습할 때 풀 파인튜닝 (Full\n","Finetuning) 보다 연산량과 GPU 메모리 사용량을 줄여 학습할 수 있도록 도와주는 방법입니다."],"metadata":{"id":"Gy1HzL8CjCm1"}},{"cell_type":"code","source":["peft_config = LoraConfig(\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    r=8,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    task_type = \"CAUSAL_LM\",\n",")"],"metadata":{"id":"n_lQcIVTjAaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["각 설정값에 대한 설명은 다음과 같습니다.\n","- lora_alpha: LoRA(Low‑Rank Adaptation) 에서 사용하는 스케일링 계수를 설정합니다. LoRA 의\n","가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련\n","이 있습니다.\n","\n","- lora_dropout: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합 (overfitting) 을 방\n","지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. 0.1 로 설정하면 학습 중 10%\n","의 뉴런이 비활성화됩니다.\n","\n","- r: LoRA 의 랭크 (rank) 를 설정합니다. 이는 LoRA 가 학습할 저차원 공간의 크기를 결정합니다. 작\n","은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다\n","\n","- bias: LoRA 적용 시 편향 (bias) 처리 방식을 지정합니다. “none” 으로 설정하면 편향이 LoRA 에 의\n","해 조정되지 않습니다. “all” 또는 “lora_only” 와 같은 값으로 변경하여 편향을 조정할 수도 있습니\n","다.\n","\n","- target_modules: LoRA 를 적용할 특정 모듈 (레이어) 의 이름을 리스트로 지정합니다. 예제에서\n","는 “q_proj” 와 “v_proj” 를 지정하여, 주로 Self‑Attention 메커니즘의 쿼리와 값 프로젝션 부분에\n","LoRA 를 적용합니다.\n","\n","- task_type: LoRA 가 적용되는 작업 유형을 지정합니다. “CAUSAL_LM” 은 Causal Language\n","Modeling, 즉 시퀀스 생성 작업에 해당합니다.\n","다음으로 SFTConfig 를 설정합니다"],"metadata":{"id":"3YSvHV75j4_K"}},{"cell_type":"markdown","source":["###4. 학습을 위한 설정 값"],"metadata":{"id":"XIoR3ePFkbOh"}},{"cell_type":"code","source":["args = SFTConfig(\n","  output_dir=\"llama3-8b-summarizer-ko\",     # 저장될 디렉토리와 저장소 ID\n","  num_train_epochs=3,                       # 학습할 총 에포크 수\n","  per_device_train_batch_size=2,            # GPU당 배치 크기\n","  gradient_accumulation_steps=2,            # 그래디언트 누적 스텝 수\n","  gradient_checkpointing=True,              # 메모리 절약을 위한 체크 포인팅\n","  optim=\"adamw_torch_fused\",                # 최적화기\n","  logging_steps=10,                         # 로그 기록 주기\n","  save_strategy=\"steps\",                    # 저장 전략\n","  save_steps=50,                            # 저장 주기\n","  bf16=True,                                # bfloat16 사용\n","  learning_rate=1e-4,                       # 학습률\n","  max_grad_norm=0.3,                        # 그래디언트 클리핑\n","  warmup_ratio=0.03,                        # 워밍업 비율\n","  lr_scheduler_type=\"constant\",             # 고 정 학 습 률\n","  push_to_hub=False,                        # 허브 업로드 안함\n","  remove_unused_columns=False,\n","  dataset_kwargs={\"skip_prepare_dataset\": True},\n","  report_to=None\n",")\n"],"metadata":{"id":"8fLvrUj-jd7B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["각 설정값에 대한 상세 설명은 다음과 같습니다.\n","- output_dir: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에\n","학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n","\n","- num_train_epochs: 모델을 학습시키는 총 에포크 (epoch) 수를 지정합니다. 에포크는 학습 데이\n","터 전체를 한 번 순회한 주기를 의미합니다.\n","\n","- per_device_train_batch_size: GPU 한 대당 사용되는 배치 (batch) 의 크기를 설정합니다. 배치크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다.\n","\n","- gradient_accumulation_steps: 그래디언트를 누적할 스텝 (step) 수를 지정합니다. 이 값이 2 로\n","설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다.\n","\n","- gradient_checkpointing: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다.\n","\n","- optim: 학습 시 사용할 최적화 알고리즘을 설정합니다. adamw_torch_fused 는 PyTorch 의 효율\n","적인 AdamW 최적화기를 사용합니다.\n","\n","- logging_steps: 로그를 기록하는 주기를 스텝 단위로 지정합니다.\n","\n","- save_strategy 와 save_steps: 모델을 저장하는 전략과 주기를 설정합니다.\n","\n","- bf16: bfloat16 정밀도를 사용하도록 설정합니다.\n","\n","- learning_rate: 학습률을 지정합니다.\n","\n","- max_grad_norm: 그래디언트 클리핑의 임계값을 설정합니다.\n","\n","- warmup_ratio: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다.\n","\n","- lr_scheduler_type: 학습률 스케줄러의 유형을 설정합니다. “constant” 는 학습률을 일정하게 유\n","지합니다.\n"],"metadata":{"id":"hGtWQ7BVlY9Z"}},{"cell_type":"markdown","source":["### 5. 정수 인코딩\n","거대 언어 모델에 학습 데이터를 전달하기 전에 학습 데이터는 수치화 작업을 거칩니다. 더 정확히는 텍\n","스트 데이터는 전부 거대 언어 모델이 이해할 수 있는 정수 데이터로 변환되어야 합니다. 이 과정을 일반\n","적으로 인코딩 (encoding) 이라 부르며 앞서 로드한 토크나이저를 통해 가능합니다. 예를 통해 인코딩 과\n","정을 이해해보겠습니다. 거대 언어 모델을 학습할 때에는 입력과 출력 두 가지가 존재해야 합니다. 여기\n","서는 입력과 출력의 각 변수명을 input_ids와 labels라고 해봅시다. 또한, 모델이 학습해야 하는 데\n","이터는 다음과 같다고 가정하겠습니다."],"metadata":{"id":"j-b4GrLwmEPu"}},{"cell_type":"code","source":["\"\"\"-시스템 프롬프트: \"당신은 친절한 AI 어시스턴트입니다.\"\n","- 유저 프롬프트: \"안녕하세요, 오늘 날씨는 어떤가요?\"\n","- 거대 언어 모델의 응답: \"안녕하세요! 오늘 날씨는 맑고 화창합니다.\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"h-Fg8XQJlTMw","executionInfo":{"status":"ok","timestamp":1755516517264,"user_tz":-540,"elapsed":68,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"fe368076-e495-48e0-8e2d-8312b5672084"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'-시스템 프롬프트: \"당신은 친절한 AI 어시스턴트입니다.\"\\n- 유저 프롬프트: \"안녕하세요, 오늘 날씨는 어떤가요?\"\\n- 거대 언어 모델의 응답: \"안녕하세요! 오늘 날씨는 맑고 화창합니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["이 데이터를 학습하기 위해서는 앞서 진행된 바와 같이 채팅 템플릿을 적용해야 합니다."],"metadata":{"id":"iFlBXBtmm6z2"}},{"cell_type":"code","source":["\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","당신은 친절한 AI 어시스턴트입니다..<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","안녕하세요, 오늘 날씨는 어떤가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","안녕하세요! 오늘 날씨는 맑고 화창합니다.<|eot_id|>\"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"RpK66qnZm1Xx","executionInfo":{"status":"ok","timestamp":1755516517270,"user_tz":-540,"elapsed":28,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"e86b7d14-7821-46ee-a7df-482d034cd75a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n당신은 친절한 AI 어시스턴트입니다..<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n안녕하세요, 오늘 날씨는 어떤가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n안녕하세요! 오늘 날씨는 맑고 화창합니다.<|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["input_ids = [\n","    128000, # <|begin_of_text|>\n","    128006, 9125, 128007, 198, # <|start_header_id|>system<|end_header_id|> (줄 바꿈)\n","    22173, 13, 126808, 49816, 33302, 23239, 18966, 13, # 당 신 은 친 절 한 AI 어 시 스 턴 트 입 니 다.\n","    128009, # <|eot_id|>\n","    128006, 882, 128007, 198, # <|start_header_id|>user<|end_header_id|> (줄 바 꿈)\n","    118145, 11, 24482, 1174, 107485, 102823, 64337, 30, # 안 녕 하 세 요 , 오 늘 날 씨 는어 떤 가 요?\n","    128009, # <|eot_id|>\n","    128006, 78191, 128007, 198, # <|start_header_id|>assistant<|end_header_id|> (줄 바 꿈)\n","    118145, 0, 24482, 1174, 107485, 102823, 64337, 107823, 108562, 13, # 안 녕 하 세요! 오 늘 날 씨 는 맑 고 화 창 합 니 다.\n","    128009 # <|eot_id|>\n","]\n"],"metadata":{"id":"hVRHmREOASpC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = [\n","    -100, # <|begin_of_text|>\n","    -100, -100, -100, -100, # <|start_header_id|>system<|end_header_id|> (줄 바 꿈)\n","    -100, -100, -100, -100, -100, -100, -100, -100, # 당 신 은 친 절 한 AI 어 시 스 턴 트입 니 다.\n","    -100, # <|eot_id|>\n","    -100, -100, -100, -100, # <|start_header_id|>user<|end_header_id|> (줄 바 꿈)\n","    -100, -100, -100, -100, -100, -100, -100, -100, # 안 녕 하 세 요 , 오 늘 날 씨 는 어떤 가 요?\n","    -100, # <|eot_id|>\n","    -100, -100, -100, -100, # <|start_header_id|>assistant<|end_header_id|> (줄 바꿈)\n","    118145, 0, 24482, 1174, 107485, 102823, 64337, 107823, 108562, 13, # 안 녕 하 세요! 오 늘 날 씨 는 맑 고 화 창 합 니 다.\n","    128009 # <|eot_id|>\n","]\n"],"metadata":{"id":"HEP2Ez4zAjK5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 정수 인코딩 적용하기\n","거대 언어 모델에 학습 데이터를 전달하기 전에 학습 데이터는 수치화 작업을 거칩니다. 텍스트 데이터는\n","전부 거대 언어 모델이 이해할 수 있는 정수 데이터로 변환되어야 합니다. 이 과정을 일반적으로 인코딩\n","(encoding) 이라 부르며 앞서 로드한 토크나이저를 통해 가능합니다."],"metadata":{"id":"fEYKeEEpnp-6"}},{"cell_type":"code","source":["def collate_fn(batch):\n","  new_batch = {\n","      \"input_ids\": [],\n","      \"attention_mask\": [],\n","      \"labels\": []\n","  }\n","\n","  for example in batch:\n","    messages = example[\"messages\"]\n","\n","    # LLaMa 3 채팅 템플릿 적용 (시작 토큰 포함)\n","    prompt = \"<|begin_of_text|>\"\n","    for msg in messages:\n","      role = msg[\"role\"]\n","      content = msg[\"content\"].strip()\n","      prompt += f\"<|start_header_id|> {role} <|end_header_id|>\\n{content}<|eot_id|>\"\n","\n","    # 마지막 assistant 메시지는 응답으로 간주하고 레이블에 포함\n","    text = prompt.strip()\n","\n","    # 토큰화\n","    tokenized = tokenizer(\n","        text,\n","        truncation=True,\n","        max_length=max_seq_length,\n","        padding=False,\n","        return_tensors=None,\n","    )\n","\n","    input_ids = tokenized[\"input_ids\"]\n","    attention_mask = tokenized[\"attention_mask\"]\n","    labels = [-100] * len(input_ids)\n","\n","    # assistant 응답의 시작 위치 찾기\n","    assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n","    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n","    eot_token = \"<|eot_id|>\"\n","    eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n","\n","    # 레이블 범위 지정\n","    i = 0\n","    while i <= len(input_ids) - len(assistant_tokens):\n","      if input_ids[i:i + len(assistant_tokens)] == assistant_tokens:\n","        start = i + len(assistant_tokens)\n","        end = start\n","        while end <= len(input_ids) - len(eot_tokens):\n","          if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n","            break\n","          end += 1\n","        for j in range(start, end):\n","          labels[j] = input_ids[j]\n","        for j in range(end, end + len(eot_tokens)):\n","          labels[j] = input_ids[j] # <|eot_id|> 토 큰 도 포 함\n","        break\n","      i += 1\n","    new_batch[\"input_ids\"].append(input_ids)\n","    new_batch[\"attention_mask\"].append(attention_mask)\n","    new_batch[\"labels\"].append(labels)\n","\n","\n","  # 패딩 처리\n","  max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n","  for i in range(len(new_batch[\"input_ids\"])):\n","    pad_len = max_length - len(new_batch[\"input_ids\"][i])\n","    new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)\n","    new_batch[\"attention_mask\"][i].extend([0] * pad_len)\n","    new_batch[\"labels\"][i].extend([-100] * pad_len)\n","\n","  for k in new_batch:\n","    new_batch[k] = torch.tensor(new_batch[k])\n","\n","  return new_batch"],"metadata":{"id":"rt_E1rnhnU-j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["collate_fn(batch) 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니\n","다. 해당 함수 내에서 input_ids는 전체 대화를 챗 템플릿 이후에 정수 인코딩을 적용한 바꾼 결과물입\n","니다. 토크나이저는 \"<|start_header_id|>\"와 \"<|end_header_id|>\", \"<|eot_id|>\"\n","같은 특수 토큰을 포함한 모든 텍스트를 숫자로 변환합니다. 이렇게 변환된 숫자들이 모델에 입력됩니다.\n","코드에서는 assistant_tokens와 eot_tokens를 먼저 인코딩해서 이 토큰들의 숫자값을 미리 준\n","비해둡니다.\n","labels는 모델이 실제로 생성해내야 할 목표값입니다. 이 코드의 핵심은 assistant가 말한 부분만\n","골라서 학습시키는 것입니다. assistant가 말한 부분은 input_ids의 값을 그대로 labels에 복사\n","하고, 나머지는 전부 ‑100 으로 채웁니다. ‑100 은 PyTorch 가 학습에서 무시하는 특별한 값입니다.\n","코 드 는 while 문 을 사 용 해 서 input_ids 안 에 서 \"<|start_header_id|>assistant<|\n","end_header_id|>\\n\"로 시작하는 부분을 찾습니다. 이 부분부터 \"<|eot_id|>\"가 나올 때까지\n","가 assistant의 응답입니다. 이 구간의 토큰들만 labels에 복사하고 나머지는 전부 ‑100 을 넣습니\n","다. 이렇게 하면 모델은 assistant의 응답만 학습하게 됩니다.\n","임의의 샘플에 대해서 실제 전처리가 잘 진행되는지 확인해봅시다. 학습 데이터 중 0 번 인덱스를 가진 첫\n","번째 샘플에 대해서 collate_fn에 통과시켜서 결과를 확인합니다."],"metadata":{"id":"5YWBd-Zp3rF3"}},{"cell_type":"code","source":["# 데이터의 최대 길이 제한\n","max_seq_length=8192\n","\n","# collate_fn 테스트 (배치크기 1. 즉, 데이터 1개에 대히서 전처리 진행해보기)\n","example = train_dataset[0]\n","batch = collate_fn([example])\n","print(\"\\n처리된 배치 데이터:\")\n","print(\"입력 ID 형태: \",batch[\"input_ids\"].shape)\n","print(\"어텐션 마스크 형태: \", batch[\"attention_mask\"].shape)\n","print(\"레이블 형태: \", batch[\"labels\"].shape)"],"metadata":{"id":"Gl5C5W5asDZf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755517840193,"user_tz":-540,"elapsed":36,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"0d39b3d0-32af-4044-fee3-4fe4b509fe8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","처리된 배치 데이터:\n","입력 ID 형태:  torch.Size([1, 2887])\n","어텐션 마스크 형태:  torch.Size([1, 2887])\n","레이블 형태:  torch.Size([1, 2887])\n"]}]},{"cell_type":"markdown","source":["먼저 데이터의 최대 길이를 8192 로 제한합니다. 이는 이보다 긴 데이터들은 임의로 길이를 자르는 것을\n","의미합니다. 그 후 학습 데이터의 첫번째 샘플을 collate_fn에 전달하여 함수의 전처리 결과를 얻어\n","batch라는 변수에 저장합니다. 이 전처리 결과에는 총 3 개의 키가 존재합니다. 바로 input_ids와\n","labels와 attention_mask입니다. 먼저 첫번째로 input_ids를 출력해보겠습니다."],"metadata":{"id":"Unz2_D3U4coJ"}},{"cell_type":"code","source":["print('입력에 대한 정수 인코딩 결과:')\n","print(batch[\"input_ids\"][0].tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DBkT_fyT4dPQ","executionInfo":{"status":"ok","timestamp":1755517841827,"user_tz":-540,"elapsed":46,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"0c9a6f4e-b03c-4546-fb41-2d82c24c36a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력에 대한 정수 인코딩 결과:\n","[128000, 128006, 1887, 220, 128007, 198, 65895, 83628, 34804, 56773, 125441, 111068, 25941, 123103, 99458, 88708, 19954, 126652, 56773, 16969, 111068, 25941, 117469, 106478, 102517, 44005, 104193, 123061, 111068, 25941, 106478, 102517, 21121, 80052, 627, 103097, 109521, 111964, 127491, 122625, 119097, 108859, 101480, 93917, 101868, 56069, 13094, 168, 235, 105, 21028, 11240, 106612, 77437, 43139, 114839, 16582, 119978, 627, 65993, 108, 103386, 36092, 112, 102260, 109055, 19954, 105642, 103386, 36092, 112, 102260, 105880, 103607, 34609, 113348, 45618, 49085, 88525, 96677, 119978, 13, 127063, 11240, 56069, 113890, 18359, 62085, 99742, 102893, 105365, 102467, 112215, 124005, 13, 106725, 27796, 56773, 21028, 16582, 119978, 627, 54059, 54542, 11240, 57575, 106603, 907, 16969, 67890, 30426, 115790, 19954, 95713, 61938, 13, 67890, 56154, 115790, 18359, 106725, 103607, 22035, 100711, 119978, 13, 95713, 67890, 30426, 115790, 19954, 106725, 103607, 104834, 24486, 907, 18918, 104965, 103430, 76242, 96, 34609, 119978, 627, 34983, 65895, 115790, 13094, 47782, 115300, 122292, 81021, 55055, 108520, 122292, 84734, 17835, 103607, 32179, 90759, 109670, 13, 105813, 21028, 17835, 364, 123467, 6, 120908, 103607, 108503, 16969, 96270, 124005, 382, 73653, 103168, 95713, 111068, 122625, 103966, 30381, 99458, 88708, 7, 127702, 125543, 105797, 102662, 119222, 51796, 109745, 11, 103966, 30381, 99458, 88708, 7, 127702, 8, 81673, 111304, 103045, 78453, 101106, 13094, 108838, 111068, 25941, 33177, 50152, 102772, 116100, 81673, 109583, 114839, 61938, 382, 109659, 104449, 512, 5018, 285, 31641, 54356, 794, 3641, 345, 1, 1743, 794, 330, 58126, 21121, 102772, 95713, 111068, 120155, 87097, 103168, 97237, 87097, 103168, 123926, 114839, 16582, 119978, 64259, 73653, 103168, 95713, 111068, 122625, 103966, 30381, 99458, 88708, 7, 127702, 8, 115467, 78453, 101106, 109791, 109745, 11, 103966, 30381, 99458, 88708, 7, 127702, 8, 54780, 111304, 103045, 78453, 101106, 13094, 108838, 111068, 25941, 33177, 50152, 102772, 116100, 81673, 109583, 114839, 61938, 382, 109659, 104449, 512, 5018, 285, 31641, 54356, 794, 3082, 345, 1, 31587, 37888, 533, 1284, 26246, 794, 4482, 101508, 13094, 168, 235, 105, 81021, 55055, 84734, 21028, 106612, 87472, 17835, 23955, 111068, 122625, 41871, 235, 30381, 103684, 126652, 109720, 104349, 43139, 58935, 30381, 107205, 99458, 88708, 106001, 87134, 18359, 114839, 16582, 119978, 13, 106943, 26799, 17835, 103607, 109745, 110192, 80732, 43139, 103607, 22035, 100711, 119978, 13, 99458, 88708, 80732, 43139, 58935, 30381, 107205, 62398, 84391, 126546, 103607, 34609, 119978, 13, 111068, 25941, 123103, 58935, 30381, 48936, 29833, 65621, 127923, 24486, 115382, 101886, 94801, 43139, 103607, 34609, 119978, 13, 63207, 103168, 11, 119097, 88525, 110661, 115300, 122292, 84734, 17835, 114839, 16582, 119978, 1210, 1282, 1, 20489, 5595, 55260, 37888, 533, 794, 330, 82001, 21028, 99458, 88708, 102823, 95713, 111068, 25941, 123103, 41871, 235, 30381, 103684, 126652, 84696, 18359, 111590, 58935, 30381, 24486, 111436, 18918, 84618, 109509, 113631, 114839, 16582, 119978, 761, 1, 31587, 52454, 794, 4482, 18202, 235, 30381, 103684, 126652, 109720, 111590, 58935, 30381, 107205, 99458, 88708, 102823, 119097, 101528, 33390, 84618, 109509, 41871, 235, 30381, 103684, 126652, 56773, 103170, 106589, 93292, 20565, 120789, 101954, 120138, 24486, 104167, 56154, 108652, 103430, 30446, 105880, 56069, 13094, 168, 235, 105, 81021, 55055, 84734, 106612, 87472, 17835, 114839, 16582, 119978, 13, 113094, 80732, 11, 127798, 80732, 120908, 109580, 110097, 61938, 13, 107067, 100660, 104167, 56154, 112887, 108785, 27797, 61938, 13, 47782, 115300, 122292, 84734, 17835, 114839, 100660, 115106, 1210, 1282, 1, 43324, 37888, 533, 1284, 26246, 794, 4482, 101508, 13094, 168, 235, 105, 81021, 55055, 84734, 21028, 106612, 87472, 17835, 23955, 111068, 122625, 41871, 235, 30381, 103684, 126652, 109720, 104349, 43139, 58935, 30381, 107205, 99458, 88708, 105880, 114839, 16582, 119978, 13, 106943, 26799, 17835, 103607, 109745, 110192, 80732, 43139, 103607, 22035, 100711, 119978, 13, 99458, 88708, 80732, 43139, 58935, 30381, 107205, 62398, 84391, 126546, 103607, 34609, 119978, 13, 111068, 25941, 123103, 58935, 30381, 48936, 29833, 65621, 127923, 24486, 115382, 101886, 94801, 43139, 103607, 34609, 119978, 13, 63207, 103168, 11, 119097, 88525, 110661, 115300, 122292, 84734, 17835, 114839, 16582, 119978, 1210, 1282, 1, 20489, 5595, 54965, 37888, 533, 794, 330, 82001, 21028, 99458, 88708, 102823, 95713, 111068, 25941, 123103, 41871, 235, 30381, 103684, 126652, 84696, 18359, 111590, 58935, 30381, 24486, 111436, 18918, 84618, 109509, 113631, 114839, 16582, 119978, 761, 1, 43324, 52454, 794, 4482, 64189, 30381, 103684, 126652, 109720, 111590, 58935, 30381, 107205, 99458, 88708, 102823, 119097, 101528, 33390, 84618, 109509, 86503, 30381, 103684, 126652, 56773, 103170, 106589, 93292, 20565, 120789, 101954, 120138, 24486, 104167, 56154, 108652, 103430, 30446, 105880, 56069, 13094, 168, 235, 105, 81021, 55055, 84734, 106612, 87472, 17835, 114839, 16582, 119978, 13, 113094, 80732, 11, 127798, 80732, 120908, 109580, 110097, 61938, 13, 107067, 100660, 104167, 56154, 112887, 108785, 27797, 61938, 13, 47782, 115300, 122292, 84734, 17835, 114839, 100660, 115106, 1210, 1282, 1, 1743, 794, 330, 58126, 21121, 102772, 95713, 111068, 120155, 87097, 103168, 97237, 87097, 103168, 123926, 114839, 16582, 119978, 9388, 128009, 128006, 1217, 220, 128007, 198, 13094, 98934, 102335, 103236, 30446, 56154, 19954, 44215, 35495, 41953, 1981, 100981, 29102, 24486, 101603, 101096, 65677, 38187, 58083, 113110, 126015, 104019, 110513, 198, 101136, 103655, 55421, 41953, 84618, 66965, 56154, 12432, 105131, 102997, 62841, 101003, 58189, 33931, 58083, 115777, 104019, 1981, 114039, 103168, 87097, 32428, 102517, 62060, 71682, 110513, 107545, 103168, 101532, 55430, 106359, 101254, 101136, 29102, 110834, 1981, 107752, 82233, 104019, 108289, 58083, 113110, 126015, 102786, 110208, 66965, 116604, 101834, 101103, 1981, 60861, 101151, 101482, 101193, 96677, 103304, 23955, 98934, 102335, 104193, 123061, 103655, 104841, 55421, 41953, 13, 105429, 108785, 52688, 112003, 21121, 26799, 23955, 98934, 102335, 104193, 123061, 103655, 104841, 55421, 124788, 101003, 58189, 33931, 104019, 107545, 22035, 57575, 103123, 21121, 29833, 108964, 33931, 103686, 42771, 18918, 107472, 101480, 29102, 24486, 101603, 101096, 103686, 115096, 65677, 38187, 34983, 115087, 107387, 220, 20, 33177, 103153, 64189, 101528, 13, 23955, 104684, 103551, 111097, 102517, 107152, 64189, 109126, 55421, 29102, 101136, 57002, 66338, 71682, 106304, 423, 14899, 220, 18, 101353, 101015, 66610, 60798, 20565, 127245, 107205, 103659, 106725, 83719, 38187, 33931, 58083, 113110, 126015, 78102, 423, 14899, 115839, 63171, 104065, 109231, 19954, 102597, 29833, 36811, 20565, 122862, 48936, 29833, 65621, 63207, 118009, 58083, 115777, 104019, 19954, 106603, 102517, 101709, 101327, 66406, 3396, 109864, 104684, 105771, 49085, 127264, 101528, 13, 23955, 102467, 124788, 23955, 106223, 106010, 72043, 89359, 50467, 58189, 84618, 83628, 101136, 123061, 109567, 62841, 57575, 105069, 102423, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 106287, 66406, 101090, 26799, 12432, 110863, 21028, 105131, 102997, 62841, 57575, 101003, 58189, 33931, 58083, 115777, 19954, 106603, 102517, 24486, 125718, 18359, 89946, 56773, 21121, 82818, 103272, 13447, 13, 84618, 66965, 117396, 29833, 83628, 117211, 13094, 47782, 21121, 109644, 101003, 58189, 33931, 58083, 115777, 20565, 107120, 114213, 82068, 109816, 125959, 102612, 103684, 58083, 115777, 112373, 107022, 101015, 101266, 124983, 104019, 48936, 108289, 20565, 91786, 49208, 108, 107779, 80816, 24486, 111850, 101555, 21028, 101003, 58189, 111490, 103686, 42771, 44005, 62398, 104790, 103123, 21121, 29833, 108964, 33931, 103686, 42771, 18918, 107472, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 106593, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 16969, 65677, 38187, 83290, 56773, 21121, 82818, 103272, 13447, 101254, 108537, 13, 121856, 23955, 102467, 124788, 84618, 66965, 117396, 84618, 66965, 109126, 97096, 101066, 78102, 119262, 33931, 103213, 44966, 18359, 110155, 127002, 21028, 65677, 101136, 18359, 66610, 104684, 101360, 112795, 45618, 101711, 101136, 29102, 69508, 59134, 104303, 45618, 66610, 104684, 19954, 123851, 38389, 103618, 113610, 48936, 29833, 91786, 13, 103294, 65677, 101136, 103678, 27797, 118408, 33390, 57575, 36609, 101015, 67945, 71023, 34804, 126761, 104182, 107545, 103168, 24486, 95303, 102156, 13094, 106359, 101360, 119864, 67945, 71023, 34804, 108360, 125189, 101508, 13094, 121751, 113890, 29515, 62060, 71023, 78102, 86503, 58189, 86157, 107022, 102757, 19954, 104441, 101711, 33308, 120, 119567, 116492, 19954, 107138, 103655, 102893, 126652, 84696, 121969, 49208, 108, 84618, 66965, 115296, 65677, 101136, 93917, 104684, 14260, 94772, 27797, 57002, 103966, 24140, 33931, 43139, 107545, 103168, 87097, 32428, 102517, 17835, 112521, 101464, 24486, 62060, 71682, 20565, 108289, 108907, 101254, 109012, 13, 23955, 102467, 124788, 220, 2366, 15, 100392, 101327, 102757, 124141, 126906, 121296, 103185, 113360, 102249, 124141, 777, 113610, 122964, 84618, 66965, 109126, 80307, 107235, 101164, 30446, 20565, 103686, 67945, 65219, 104448, 84618, 66965, 109126, 126902, 97096, 101066, 13094, 112024, 57002, 72043, 101353, 106910, 116686, 72043, 44690, 102193, 84618, 66965, 117396, 29833, 74623, 100551, 63375, 101003, 58189, 33931, 106460, 17835, 19954, 105164, 33390, 24486, 82818, 91786, 49208, 108, 121066, 220, 21, 100551, 111323, 84618, 66965, 109126, 80307, 107235, 101164, 30446, 20565, 220, 2366, 15, 100392, 101003, 58189, 33931, 46810, 21121, 122964, 106287, 101838, 220, 6083, 18287, 117615, 59134, 62841, 116429, 65677, 101136, 93917, 104684, 84618, 101868, 13094, 127992, 115809, 57390, 116039, 91786, 101254, 109012, 13, 106237, 104448, 23955, 102467, 124788, 65677, 50643, 104182, 64432, 24140, 103684, 116492, 18359, 127271, 34983, 101003, 58189, 33931, 123706, 116273, 107573, 54289, 18918, 125744, 101360, 75086, 57002, 65677, 101136, 66610, 104684, 119623, 49085, 106327, 126403, 106313, 109070, 34983, 56773, 21121, 82818, 103272, 13447, 49208, 108, 69508, 103684, 62060, 71023, 102657, 103686, 112037, 106593, 62060, 55430, 55430, 109682, 101482, 101193, 101003, 57002, 102249, 26799, 65677, 101136, 111302, 78102, 103686, 42771, 120908, 110155, 63207, 110534, 54542, 86503, 109126, 18918, 65677, 50643, 104182, 59134, 66338, 48936, 29833, 123644, 107779, 80816, 24486, 111850, 101555, 21028, 101003, 58189, 33931, 103686, 42771, 20565, 108289, 108907, 101254, 102258, 93917, 101528, 13, 49508, 102275, 61394, 23955, 102467, 124788, 36609, 101015, 67945, 71023, 18359, 96270, 30381, 104182, 104019, 101360, 104423, 101272, 103402, 94, 24140, 122298, 116688, 103686, 112037, 44005, 103659, 49085, 104441, 101711, 34983, 104685, 105771, 103153, 64189, 101528, 13, 108154, 84618, 66965, 115296, 36609, 101015, 67945, 71023, 34804, 107545, 103168, 101532, 55430, 20565, 106359, 44005, 101254, 101136, 29102, 109231, 13094, 127002, 18359, 103213, 22035, 101360, 112795, 104193, 29102, 59134, 104303, 45618, 103521, 66965, 115602, 102678, 16582, 113191, 101834, 101103, 20565, 91786, 49208, 108, 107545, 103168, 101532, 55430, 19954, 102597, 101254, 101136, 29102, 62060, 71023, 107545, 102662, 45618, 103213, 109627, 59134, 66338, 122298, 29854, 19954, 107625, 16969, 62060, 71023, 107545, 102662, 93851, 101066, 13094, 37155, 111283, 113191, 29833, 123644, 125718, 18359, 89946, 56773, 30426, 21121, 82818, 103272, 13447, 101254, 109012, 13, 23955, 102467, 124788, 23955, 104684, 103551, 127245, 53400, 423, 14899, 220, 18, 101353, 101015, 66610, 60798, 111323, 103055, 101136, 125935, 83719, 38187, 33931, 58083, 113110, 126015, 78102, 423, 14899, 115839, 116464, 57575, 63171, 104065, 107205, 109231, 19954, 102597, 29833, 36811, 20565, 122862, 48936, 29833, 36439, 34609, 117622, 58083, 115777, 104019, 19954, 64432, 13447, 101327, 66406, 3396, 109864, 55430, 106103, 82818, 103272, 13447, 49208, 108, 125578, 104423, 101272, 103402, 94, 24140, 122298, 29854, 103686, 112037, 18359, 106958, 101412, 54542, 57519, 105115, 18359, 64432, 24140, 104182, 66980, 34983, 62060, 111270, 112037, 65895, 101136, 18359, 107779, 80816, 101709, 103607, 102365, 48936, 108289, 20565, 91786, 101254, 101012, 100, 44852, 247, 102563, 13, 23955, 102467, 124788, 119864, 67945, 71023, 13094, 103966, 30381, 107022, 102757, 19954, 105613, 101711, 119222, 51796, 108438, 84618, 83628, 102612, 56154, 101824, 33229, 102252, 101106, 106064, 102258, 57390, 34983, 109720, 124859, 104064, 29854, 101528, 13, 108154, 84618, 66965, 117396, 104219, 93292, 220, 605, 100392, 63375, 102678, 101136, 29102, 55216, 93917, 101824, 44215, 108955, 106213, 57390, 17835, 29515, 62060, 71023, 78102, 86503, 58189, 86157, 107022, 102757, 18359, 122169, 43139, 119864, 67945, 71023, 18359, 103686, 67945, 34983, 119929, 102772, 101254, 101314, 101096, 100981, 65677, 86157, 18359, 84415, 54780, 102893, 73444, 112039, 113295, 27796, 113469, 86503, 58189, 86157, 112994, 16582, 105316, 19954, 102597, 101834, 101103, 20565, 118957, 106313, 18359, 125714, 34983, 62060, 71023, 107545, 102662, 45618, 110038, 42771, 101438, 13094, 116548, 104965, 100981, 59134, 66338, 122298, 29854, 46810, 55430, 17835, 84618, 83628, 102612, 111636, 107973, 62060, 71023, 107545, 102662, 111323, 102772, 103213, 109627, 101327, 27797, 82001, 102005, 124476, 84618, 64189, 18918, 56773, 21121, 104182, 106313, 109070, 48936, 108289, 20565, 91786, 101254, 108537, 13, 121856, 23955, 102467, 124788, 84618, 66965, 56154, 101266, 124983, 119864, 58126, 83628, 106213, 56154, 101824, 33229, 102252, 101106, 106064, 102258, 57390, 101360, 119262, 116492, 115809, 57390, 19954, 62060, 71682, 34983, 62060, 111270, 112037, 65895, 101136, 69508, 103607, 102365, 109018, 110671, 115954, 56773, 30426, 21121, 82818, 103272, 13447, 49208, 108, 104193, 103655, 55421, 34804, 107036, 29515, 62060, 71023, 19954, 102597, 115888, 33931, 116090, 18918, 125744, 44005, 78102, 119864, 67945, 71023, 62085, 225, 250, 18918, 106313, 109070, 101360, 55925, 99901, 18918, 82818, 120378, 43139, 107022, 101015, 81673, 119864, 58126, 83628, 106213, 56154, 101824, 33229, 102252, 106974, 55170, 108860, 105633, 102611, 117615, 96677, 103304, 48936, 119623, 23955, 105771, 109012, 13, 23955, 102467, 124788, 124141, 777, 109682, 113052, 99458, 64356, 78102, 19954, 62060, 71682, 24486, 107545, 103168, 101532, 55430, 109682, 109018, 125718, 18359, 103153, 64189, 101528, 13, 108154, 84618, 66965, 114333, 65677, 50643, 107065, 72043, 32428, 118711, 126709, 54059, 120916, 78102, 104965, 100981, 93917, 30381, 109682, 113052, 18359, 120952, 34983, 84656, 30426, 104182, 102888, 100981, 82068, 46230, 97, 66406, 19954, 72747, 24486, 103213, 55430, 20565, 66610, 109509, 48918, 101096, 19954, 107067, 113047, 48936, 29833, 123644, 103607, 110616, 103684, 109682, 18359, 121121, 30446, 102423, 13447, 49208, 108, 104350, 34983, 220, 23, 123096, 127798, 102517, 104193, 29102, 32428, 16582, 36811, 89359, 103131, 107065, 101272, 82068, 100994, 30426, 20565, 127245, 65219, 117622, 116534, 103603, 102258, 57390, 120908, 110155, 101327, 27797, 123360, 74623, 101151, 53400, 116534, 21028, 104193, 29102, 64189, 102997, 13094, 44215, 103655, 113191, 29833, 123644, 110187, 125718, 18359, 89946, 56773, 30426, 21121, 82818, 103272, 13447, 101254, 102258, 93917, 101528, 13, 106237, 104448, 23955, 102467, 124788, 119929, 106359, 101136, 106446, 13094, 122862, 44005, 83719, 38187, 33931, 58083, 113110, 126015, 34804, 107545, 103168, 101532, 109627, 59134, 66338, 86503, 102997, 18359, 84656, 30426, 104182, 109720, 58126, 115087, 29833, 65621, 102027, 101838, 13094, 36439, 102077, 104193, 44690, 101661, 57002, 104193, 123061, 125399, 19954, 95713, 88525, 51796, 54059, 102786, 110208, 66965, 116604, 19954, 102597, 101834, 101103, 20565, 65621, 124859, 112024, 23955, 51440, 101203, 104193, 103655, 55421, 34804, 104193, 123061, 82001, 114080, 62841, 81673, 106999, 104193, 123061, 44690, 71682, 26799, 109969, 108964, 63171, 35495, 18918, 106958, 58083, 113110, 126015, 114942, 27796, 101327, 102546, 107545, 103168, 101532, 55430, 127287, 45618, 61816, 102477, 111411, 125744, 104193, 29102, 105178, 30381, 67236, 101577, 103603, 104193, 29102, 100994, 30426, 56773, 21121, 103123, 106734, 122733, 74623, 101151, 101482, 101193, 18359, 96677, 103304, 72043, 19954, 91786, 13, 106603, 103236, 30446, 56154, 12432, 127737, 49085, 74623, 101151, 101482, 101193, 96677, 103304, 57519, 102704, 116534, 19954, 102597, 114942, 101412, 102735, 94, 78102, 43139, 59777, 34983, 102786, 110208, 66965, 116604, 20565, 113610, 88525, 51796, 108438, 65677, 50643, 104182, 104019, 18918, 102258, 57390, 34983, 56773, 30426, 106647, 103153, 64189, 30446, 102423, 13447, 101254, 109012, 13, 23955, 102467, 124788, 84618, 66965, 101096, 101015, 44215, 108955, 29854, 102258, 117216, 107472, 111850, 38187, 107123, 57390, 78102, 126950, 82068, 109682, 18359, 49508, 110833, 22035, 51796, 103373, 105453, 118183, 49085, 116283, 35859, 104828, 13, 108154, 105638, 22035, 117419, 57519, 66338, 45618, 124784, 107625, 13094, 34983, 104193, 123061, 101096, 54780, 75086, 101136, 123061, 101096, 21028, 44215, 101015, 20565, 108785, 101438, 32179, 119073, 103924, 13, 125578, 84618, 66965, 117396, 102558, 227, 102953, 82233, 81673, 21028, 44215, 108955, 106213, 57390, 17835, 84618, 101109, 107022, 102757, 107988, 123851, 108699, 19954, 72747, 34983, 36439, 34609, 117622, 116453, 102132, 41953, 58189, 116688, 97096, 108214, 48936, 29833, 123644, 109682, 16582, 125684, 49208, 108, 105638, 22035, 117419, 57519, 66338, 58935, 42529, 18918, 125714, 34983, 24839, 116, 101090, 101824, 86503, 24140, 101096, 100981, 21028, 115483, 82001, 84618, 66965, 101096, 102517, 107545, 102662, 96451, 107022, 100981, 21028, 50152, 104193, 123061, 101096, 54780, 78453, 101106, 53400, 115888, 19954, 122115, 16969, 104193, 123061, 82001, 19954, 103686, 124784, 103521, 21028, 16582, 125684, 13, 103294, 123102, 102464, 71023, 45618, 109018, 104193, 103655, 123645, 123102, 103315, 29726, 126709, 18918, 120952, 83290, 84618, 66965, 115296, 106460, 17835, 115790, 18359, 61816, 44690, 48936, 29833, 123644, 110671, 113026, 125684, 101254, 108537, 13, 110154, 43139, 23955, 102467, 124788, 104193, 123061, 30426, 41953, 116492, 13094, 103123, 117534, 19954, 74623, 101151, 119222, 120903, 111590, 96717, 57002, 65219, 117622, 122352, 92143, 102735, 94, 18359, 120693, 58083, 115777, 104019, 81673, 104193, 123061, 44690, 71682, 26799, 126110, 19954, 104441, 101711, 34983, 56773, 30426, 106647, 103153, 64189, 30446, 102423, 13447, 49208, 108, 104193, 103655, 55421, 49085, 84618, 66965, 101096, 101015, 81673, 122352, 105711, 101709, 101228, 102233, 116429, 104414, 101096, 64189, 112343, 44215, 108955, 29854, 102258, 117216, 106958, 106434, 111850, 38187, 18918, 74623, 101151, 101360, 62085, 248, 101, 33931, 63171, 35495, 18918, 107472, 102058, 29854, 49085, 67890, 102130, 48936, 72208, 23955, 105771, 109012, 13, 128009, 128006, 18328, 220, 128007, 198, 13922, 285, 31641, 54356, 1232, 3082, 11, 364, 43324, 37888, 533, 1284, 26246, 1232, 2570, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 518, 364, 101436, 30446, 56154, 4181, 364, 43324, 52454, 1232, 2570, 101314, 58189, 33931, 58083, 115777, 518, 364, 29102, 113110, 126015, 518, 364, 35495, 82001, 102005, 65677, 86157, 518, 364, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 4181, 364, 31587, 37888, 533, 1284, 26246, 1232, 10277, 364, 31587, 52454, 1232, 10277, 364, 20489, 5595, 54965, 37888, 533, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 101003, 58189, 33931, 58083, 115777, 104019, 18918, 102258, 93917, 108859, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 102258, 117216, 67890, 30426, 24486, 110005, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 81673, 103236, 30446, 56154, 114026, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 103924, 13, 125578, 11, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 101824, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 65677, 38187, 20565, 126999, 65219, 104448, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 109720, 29833, 103924, 16045, 364, 20489, 5595, 55260, 37888, 533, 1232, 9158, 364, 1743, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 103236, 30446, 56154, 81673, 84618, 83628, 66965, 52688, 101136, 123061, 62841, 111636, 116464, 43139, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 18918, 103153, 64189, 108859, 11, 101003, 58189, 33931, 58083, 115777, 81673, 107545, 103168, 101532, 55430, 62060, 71023, 19954, 102597, 56773, 21028, 18918, 102258, 93917, 101528, 13, 127063, 95713, 104193, 123061, 56154, 106001, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 91786, 3238, 92, 128009]\n"]}]},{"cell_type":"markdown","source":["collate_fn은 들어온 데이터에 대해서 챗 템플릿을 적용 후에 정수 인코딩까지 진행합니다"],"metadata":{"id":"29sfJOyt4oft"}},{"cell_type":"code","source":["print(\"레이블에 대한 정수 인코딩 결과:\")\n","print(batch[\"labels\"][0].tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4wmpK7cP4mMy","executionInfo":{"status":"ok","timestamp":1755517843046,"user_tz":-540,"elapsed":44,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"44ee9efd-0845-46a8-8204-39abd149360f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["레이블에 대한 정수 인코딩 결과:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"]}]},{"cell_type":"markdown","source":["출력 결과를 살펴보면 입력에 대한 정수 인코딩 결과와 레이블에 대한 정수 인코딩 결과는 길이는 동일합\n","니다. 하지만 레이블에 대한 정수 인코딩 결과에서는 거대 언어 모델의 실제 답변 부분을 제외하고는 전부\n","‑100 으로 채워진 값이 출력됩니다. 디코딩된 결과도 확인할 수 있습니다. 먼저 input_ids를 디코딩한\n","결과입니다.\n"],"metadata":{"id":"z569yh0A423Q"}},{"cell_type":"code","source":["# 디코딩된 input_ids 출력\n","decoded_text = tokenizer.decode(\n","    batch[\"input_ids\"][0].tolist(),\n","    skip_special_tokens=False,\n","    clean_up_tokenization_spaces=False,\n",")\n","print(\"\\ninput_ids 디코딩 결과:\")\n","print(decoded_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7LcYZMB40rb","executionInfo":{"status":"ok","timestamp":1755517001849,"user_tz":-540,"elapsed":74,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8318a9fc-4345-4ea5-a5f2-87e246bd21d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","input_ids 디코딩 결과:\n","<|begin_of_text|><|start_header_id|> system <|end_header_id|>\n","당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n","두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n","큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n","아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n","해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n","\n","만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n","\n","답변:\n","{\"is_stock_related\": False,\n","\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n","\n","만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n","\n","답변:\n","{\"is_stock_related\": True,\n","\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n","\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n","\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n","\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n","\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n","\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n","\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|> user <|end_header_id|>\n","이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n","금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|eot_id|><|start_header_id|> assistant <|end_header_id|>\n","{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|eot_id|>\n"]}]},{"cell_type":"markdown","source":["챗 템플릿이 적용된 결과를 확인할 수 있습니다. 이제 labels의 디코딩 된 결과를 확인해보겠습니다.\n","단, ‑100 은 실제 맵핑되는 토큰이 없으므로 ‑100 은 제외하고 출력합니다."],"metadata":{"id":"Ufx00cE25WgF"}},{"cell_type":"code","source":["# -100이 아닌 부분만 골라 디코딩\n","label_ids = [token_id for token_id in batch[\"labels\"][0].tolist() if token_id != -100]\n","\n","decoded_labels = tokenizer.decode(\n","    label_ids,\n","    skip_special_tokens=False,\n","    clean_up_tokenization_spaces=False\n",")\n","\n","print(\"\\nlabels 디코딩 결과 (-100 제외):\")\n","print(decoded_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELqwK4975OdL","executionInfo":{"status":"ok","timestamp":1755518239610,"user_tz":-540,"elapsed":51,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"23c2bd5c-67b8-40ef-fbca-716a54e205bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","labels 디코딩 결과 (-100 제외):\n","\n"]}]},{"cell_type":"markdown","source":["시스템 프롬프트와 유저 프롬프트는 제외하고 어시스턴트의 응답만 출력됩니다. collate_fn을 통과\n","한 후의 input_ids와 labels에 대해서 배웠습니다. 이제 마지막으로 attention_mask에 대해서\n","설명해보겠습니다"],"metadata":{"id":"jxex04F05-HB"}},{"cell_type":"markdown","source":["### 6. 어텐션 마스크 확인\n","어텐션 마스크에 대해서 이해하기 위해 배치 연산에 대해서 설명해보겠습니다. 배치 연산은 다수의 샘플\n","을 동시에 처리하는 것을 말하며 배치 크기란 모델이 한 번에 학습하는 데이터 샘플의 수를 의미합니다.\n","예를 들어 배치 크기가 3 이면, 모델은 세 개의 데이터 샘플을 동시에 처리합니다. 이렇게 병렬적으로 학\n","습하면 계산 효율성이 높아지고 학습 속도가 빨라지는 이점이 있어서 학습 시에는 주로 배치 학습을 합니\n","다. 실제로 위에서 SFTConfig() 설정 안에서 per_device_train_batch_size=2의 값을 사용\n","하였던 것을 상기합니다. 이는 GPU 장비당 배치 크기가 2 라는 의미인데, 이번 실습에서 GPU 장비는 1 개\n","를 사용합니다. 다시 말해 이번 실습에서의 배치 크기는 2 입니다. 그런데 배치 연산을 위해서는 배치 내의 모든 샘플의 길이가 동일해야 한다는 조건이 붙습니다. 하지만 자연어 처리에서 각각의 샘플은 길이가 다양합니다. 첫번째 샘플은 장문의 문장일 수도 있고, 두번째 샘플은 짧은 문장일 수도 있습니다. 즉, 자연어 처리에서 전체 데이터 내의 샘플들끼리 모두 길이가 똑같을 가능성은 거의 없다는 겁니다."],"metadata":{"id":"gk2LhYDf5_cH"}},{"cell_type":"markdown","source":["그런데 배치 연산을 위해서는 배치 내의 모든 샘플의 길이가 동일해야 한다는 조건이 붙습니다. 하지만 자\n","연어 처리에서 각각의 샘플은 길이가 다양합니다. 첫번째 샘플은 장문의 문장일 수도 있고, 두번째 샘플\n","은 짧은 문장일 수도 있습니다. 즉, 자연어 처리에서 전체 데이터 내의 샘플들끼리 모두 길이가 똑같을 가\n","능성은 거의 없다는 겁니다. 예를 들어 배치 크기 3 인 경우를 가정해봅시다.\n"],"metadata":{"id":"7YkltVNxqJAq"}},{"cell_type":"code","source":["\"\"\"샘플1: \"인공지능이란 무엇인가요?\"\n","→ [101, 4089, 8024, 6356, 102] (5 토 큰)\n","샘플2: \"오늘 날씨가 정말 좋네요\"\n","→ [101, 3157, 2533, 4120, 2642, 8730, 6824, 102] (8 토 큰)\n","샘플3: \"딥러닝 모델을 학습 시키는 방법을 알려주세요.\"\n","→ [101, 2982, 3478, 4567, 2053, 8276, 5036, 2355, 4602, 7312, 102] (11 토 큰)\n","\"\"\""],"metadata":{"id":"2UTfV9G7qPl6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["위 3 개의 샘플을 배치 크기 3 으로 데이터를 처리하려면 임의로 데이터의 길이를 조절하는 기술’ 패딩\n","(Padding)’ 을 사용하여 3 개의 샘플의 길이를 동일하게 맞추어야 합니다. ’ 패딩 (Padding)’ 이란 길이가\n","짧은 샘플들을 긴 샘플들의 길이에 강제로 맞추는 기술로 보통 뒤에 숫자 0 을 붙여서 해결합니다. 현재는\n","각 샘플의 길이기 5, 8, 11 이므로 위 예시에서는 가장 긴 샘플에 세번째 샘플로 길이는 11 입니다. 이에 따\n","라서 다른 샘플들에 숫자 0 을 추가합니다."],"metadata":{"id":"UT6fjVU3qsT4"}},{"cell_type":"code","source":["\"\"\"샘플1: [101, 4089, 8024, 6356, 102, 0, 0, 0, 0, 0, 0] (5 실제 + 6 패딩)\n","샘플2: [101, 3157, 2533, 4120, 2642, 8730, 6824, 102, 0, 0, 0] (8 실제 + 3 패딩)\n","샘플3: [101, 2982, 3478, 4567, 2053, 8276, 5036, 2355, 4602, 7312, 102] (11 실제 토큰)\"\"\""],"metadata":{"id":"5B22IPNRqx_p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["샘플 1 과 샘플 2 에 뒤에 숫자 0 들을 강제로 추가하여 이렇게 하면 모든 샘플이 길이 11 이 되도록 했습니\n","다. 이제 샘플 3 개가 길이가 모두 동일하므로 배치 크기 3 의 배치 연산이 가능해졌습니다. 학습 과정에서\n","는 저 3 개가 한 번에 묶여서 input_ids로 사용될 것입니다.\n","그런데 패딩을 위해 추가된 0 은 실제 의미가 있는 정수는 아니고 순전히 길이를 맞추기 위한 용도로 아\n","무 의미도 없는 정수입니다. 모델이 학습되는 과정에서 패딩 용도로 들어간 정수 0 에 대해서 어떠한\n","의미를 부여하고 학습을 할 필요가 없습니다. 따라서 모델에게 여기까지는 실제 의미가 있는 토큰들\n","이고 여기서부터는 실제 의미가 없는 토큰 (숫자 0 으로 채워진 구간) 들임을 명시적으로 알려줘야 합\n","니다. 이를 위해 사용하는 것이 어텐션 마스크입니다. 어텐션 마스크는 input_ids, labels와 함께\n","attention_mask라는 값으로 함께 모델의\n","입력으로 사용됩니다. 어텐션 마스크는 실제 패딩 토큰이\n","아니었던 구간은 1, 패딩 토큰인 구간은 0 으로 채워진 값입니다. 예를 들어 위의 예시에 따르면 어텐션 마\n","스크는 아래의 값을 가집니다.\n"],"metadata":{"id":"ppAgTv9-rF2a"}},{"cell_type":"code","source":["\"\"\"샘플1 마스크: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n","샘플2 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n","샘플3 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","\"\"\""],"metadata":{"id":"-TPgpQfGrIGf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["실제 출력을 통해 이해해봅시다. 두 개의 데이터를 동시에 collate_fn에 전달하여 배치 크기 2 인 상\n","황에 대해서 input_ids와 labels와 attention_mask가 어떤 값을 가지는지 확인해볼 것입니다.\n","먼저 학습 데이터 중 0 번 데이터와 1 번 데이터의 길이를 확인해봅시다. 두 개의 데이터 각각을 챗 템플릿\n","을 적용하고, 각각에 대해서 총 토큰의 수를 확인하면 될 것입니다.\n"],"metadata":{"id":"DT-zmmAtrTHo"}},{"cell_type":"code","source":["# 0번과 1번 데이터의 길이 확인\n","example0 = train_dataset[0]\n","example1 = train_dataset[1]"],"metadata":{"id":"3jbKd1Lm57v1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 개별 길이 확인 (토큰화 후)\n","tokenized0 = tokenizer(\n","    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n","    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\"\n","     for msg in example0[\"messages\"]]),\n","    truncation=True,\n","    max_length=max_seq_length,\n","    padding=False,\n","    return_tensors=None,\n",")\n","\n","tokenized1 = tokenizer(\n","    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n","    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\"\n","    for msg in example1[\"messages\"]]),\n","    truncation=True,\n","    max_length=max_seq_length,\n","    padding=False,\n","    return_tensors=None,\n",")\n","print(f\"0번 데이터 길이: {len(tokenized0['input_ids'])}\")\n","print(f\"1번 데이터 길이: {len(tokenized1['input_ids'])}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rx0dalGX7Kb_","executionInfo":{"status":"ok","timestamp":1755517206088,"user_tz":-540,"elapsed":36,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"86ca2985-c249-41be-e7ab-c799bc86d939"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0번 데이터 길이: 2884\n","1번 데이터 길이: 974\n"]}]},{"cell_type":"markdown","source":["챗 템플릿을 적용했을 때 0 번 데이터의 길이는 2,884 이며 1 번 데이터의 길이는 974 입니다. 다시 말해 이\n","두 개의 데이터 길이는 서로 상이하며 첫번째 데이터의 길이가 훨씬 긴 상황입니다. 이제 이 두 개의 데이\n","터를 동시에 collate_fn에 전달해보겠습니다. 즉, 두 개의 데이터가 배치 크기 2 로 전달되는 상황입\n","니다."],"metadata":{"id":"fKADJlJ2rXib"}},{"cell_type":"code","source":["# 데이터의 최대 길이 제한\n","max_seq_length = 8192\n","\n","# 배치로 처리하여 어텐션 마스크 비교\n","batch = collate_fn([example0, example1])"],"metadata":{"id":"17Dqay5e8epx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터의 최대 길이를 8192 로 정하고 두 개의 데이터를 리스트로 묶어서 collate_fn에 전달하여 그 결\n","과를 batch라는 변수에 저장했습니다. 현재 batch에는 input_ids, labels, attention_mask\n","라는 키 값들이 존재합니다. 먼저 input_ids와 attention_mask의 크기 (shape) 를 출력해보겠습\n","니다."],"metadata":{"id":"b3RueHdprdoF"}},{"cell_type":"code","source":["print(\"\\n배치 처리 후:\")\n","print(f\"입력 ID 형태: {batch['input_ids'].shape}\")\n","print(f\"어텐션 마스크 형태: {batch['attention_mask'].shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXCVQas2Bvep","executionInfo":{"status":"ok","timestamp":1755518030325,"user_tz":-540,"elapsed":41,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d611eb29-fa07-4d3e-a00d-16bb2db264a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","배치 처리 후:\n","입력 ID 형태: torch.Size([2, 2887])\n","어텐션 마스크 형태: torch.Size([2, 2887])\n"]}]},{"cell_type":"markdown","source":["input_ids와 attention_mask 모두 2 행 2,884 열이라는 크기가 출력됩니다. 또는 길이가 2,884 인\n","데이터가 2 개가 있다고 해석해볼 수도 있을 것입니다. 이 말은 1 번 데이터가 0 번 데이터의 길이로 강제\n","로 맞춰졌음을 의미하며 내부에서 패딩이 이루어졌음을 의미합니다. 이제 1 번 데이터의 input_ids와\n","attention_mask에는 길이 974 에서 강제로 길이 2,884 로 맞추기 위해서 그만큼 뒤에 숫자 0 이 1,910\n","개가 추가되었을 것입니다.\n","실 제 로 0 번 데 이 터 의 attention_mask에 서 의 숫 자 1 과 0 의 개 수 와 1 번 데 이 터 의\n","attention_mask에서의 숫자 1 과 0 의 개수를 출력해봅시다."],"metadata":{"id":"FuCTcqPrrgN5"}},{"cell_type":"code","source":["print(f\"0번 샘 플 어 텐 션 마 스 크 1의 개 수: {batch['attention_mask'][0].sum().item()}\")\n","print(f\"0번 샘 플 어 텐 션 마 스 크 0의 개 수: {(batch['attention_mask'][0] == 0).sum().item()}\")\n","print(f\"1번 샘 플 어 텐 션 마 스 크 1의 개 수: {batch['attention_mask'][1].sum().item()}\")\n","print(f\"1번 샘 플 어 텐 션 마 스 크 0의 개 수: {(batch['attention_mask'][1] == 0).sum().item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MyH7G1oTB1Bo","executionInfo":{"status":"ok","timestamp":1755518103547,"user_tz":-540,"elapsed":50,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"f9e342be-3715-41a9-eaae-958db5874d8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0번 샘 플 어 텐 션 마 스 크 1의 개 수: 2887\n","0번 샘 플 어 텐 션 마 스 크 0의 개 수: 0\n","1번 샘 플 어 텐 션 마 스 크 1의 개 수: 977\n","1번 샘 플 어 텐 션 마 스 크 0의 개 수: 1910\n"]}]},{"cell_type":"markdown","source":["0 번 샘플과 1 번 샘플의 어텐션 마스크를 직접 출력한 결과 0 번 샘플의 어텐션 마스크의 1 의 개수는 2,884\n","개입니다. 길이가 2,884 개이므로 모두 1 으로 채워져 있다는 의미입니다. 그도 그럴게 0 번 샘플은 패딩\n","이 되지 않았기 때문입니다. 반면, 1 번 샘플의 어텐션 마스크의 1 의 개수는 974 개이고 0 의 개수는 1,910\n","개인데 이는 앞은 974 개의 1 로 채워져있지만 뒷 부분은 0 이 1,910 개가 채워져있음을 의미합니다. 이처\n","럼 어텐션 마스크는 input_ids에서 어느 구간까지가 패딩이 아니고, 어느 구간이 패딩인지를 알려주\n","는 역할을 합니다. 학습 시 모델에는 input_ids, labels, attention_mask가 모두 입력으로 사용\n","됩니다.\n"],"metadata":{"id":"9CYYtpBsrkKw"}},{"cell_type":"markdown","source":["###7. 학습하기\n","이제 전처리가 끝났으므로 실제 학습을 진행합니다."],"metadata":{"id":"92s6vF4nrlp6"}},{"cell_type":"code","source":["trainer = SFTTrainer(\n","model=model,\n","args=args,\n","max_seq_length=max_seq_length, # 최대 시퀀스 길이 설정\n","train_dataset=train_dataset,\n","data_collator=collate_fn,\n","peft_config=peft_config,\n",")\n"],"metadata":{"id":"_-P08KoKB64h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["학습을 시작하면 3 번의 에포크를 도는 동안 총 372 번의 업데이트가 필요하다고 출력됩니다. 왜 3 번\n","의 에포크 동안 372 번 업데이트를 하는 것일까요? 앞서 학습 데이터의 개수는 496 개였습니다. 그리\n","고 SFTConfig() 설정에서 num_train_epochs=3, per_device_train_batch_size=2,\n","gradient_accumulation_steps=2 값 을 설 정 한 것 을 기 억 합 시 다. num_train_epochs\n","=3의 값 의 설 정 에 따 라 서 전 체 데 이 터 에 대 한 학 습 횟 수. 즉, 에 포 크 는 총 3 회 입 니 다.\n","per_device_train_batch_size=2는 GPU 장비 당 배치 크기를 의미합니다. 현재 GPU 는 1 개 사\n","용하고 있으므로 다시 말해 배치 크기가 2 라는 의미입니다. gradient_accumulation_steps는\n","배치를 몇 개까지 모아두었다가 한 번에 모델을 업데이트 할 것이냐를 의미합니다. 정리하면 다음과 같습\n","니다."],"metadata":{"id":"qF7J-y6qr9n8"}},{"cell_type":"markdown","source":["- 배치 크기 (batch size): 2 ‑ 한 번에 처리하는 데이터 샘플의 수입니다. 메모리 효율을 위해 전체 데\n","이터를 작은 배치로 나누어 처리하며, 여기서는 2 개씩 묶어서 처리합니다.\n","- 누적 단계 (accumulation steps): 2 ‑ 모델을 실제로 업데이트하기 전에 여러 배치의 정보를 모으\n","는 수입니다. 여기서는 2 개의 배치 (총 4 개의 샘플) 를 처리한 후에야 실제 모델 업데이트가 일어납\n","니다.\n","- 에포크 1 회당 업데이트 횟수: 496 ÷ (2 × 2) = 124 회 ‑ 한 에포크에서 모델이 업데이트되는 횟수입\n","니다. 전체 데이터 496 개를 유효 배치 크기 4(배치 크기 2 × 누적 단계 2) 로 나누면 124 번의 업데이\n","트가 발생합니다.\n","- 총 업데이트 계산 방법: (데이터 크기 × 에포크) ÷ (배치 크기 × 누적 단계) ‑ 학습 과정 전체에서 발\n","생하는 모델 업데이트의 총 횟수를 계산하는 공식입니다. 전체 처리 샘플 수를 유효 배치 크기로 나\n","눕니다.\n","- 총 업데이트 계산 과정: (496 × 3) ÷ (2 × 2) = 1,488 ÷ 4 = 372 ‑ 3 개의 에포크 동안 총 1,488 개의 샘\n","플이 처리되고, 유효 배치 크기인 4 개의 샘플마다 한 번씩 모델이 업데이트되므로 총 372 번의 모\n","델 업데이트가 발생합니다"],"metadata":{"id":"2q6MD_DusD2V"}},{"cell_type":"markdown","source":["###8. 테스트 데이터 준비하기\n","테스트 데이터를 전처리하여 토크나이저의 apply_chat_template()를 사용하여 챗 템플릿을\n","적용해봅시다. 챗 템플릿 적용 시에는 시스템 프롬프트와 유저 프롬프트에 이어 입력의 뒤에 <|\n","start_header_id|>assistant<|end_header_id|>\\n가 부착되어서 넣는 것이 좋습니다.\n","그러면 모델이 조금 더 안정적으로 답변을 생성합니다. 이는 앞서 챗 템플릿에서 설명했던 생성 프롬프트\n","(generation prompt) 에 해당됩니다.\n"],"metadata":{"id":"lqsLaGlGsX3j"}},{"cell_type":"code","source":["prompt_lst = []\n","label_lst = []\n","\n","for messages in test_dataset[\"messages\"]:\n","  text = tokenizer.apply_chat_template(messages, tokenize=False,\n","    add_generation_prompt=False)\n","  input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|\n","    start_header_id|>assistant<|end_header_id|>\\n'\n","  label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n","  prompt_lst.append(input)\n","  label_lst.append(label)\n","\n"],"metadata":{"id":"qRWDyTGtr4kz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["챗 템플릿을 적용하고 테스트 데이터의 입력과 레이블을 분리했습니다. 임의로 200 번 샘플을 출력해보\n","겠습니다.\n"],"metadata":{"id":"QSXrCdAEsyvP"}},{"cell_type":"code","source":["print(prompt_lst[200])"],"metadata":{"id":"SGPdCJizr4h8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(label_lst[200])"],"metadata":{"id":"JalyKn1pr4eg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["테스트 데이터에 대해서 챗 템플릿을 적용하고 생성 프롬프트를 부착하는 전처리가 끝났습니다. 이제 파\n","인 튜닝 모델의 입력으로 테스트 데이터를 넣어 모델의 예측을 확인해봅시다."],"metadata":{"id":"VRByqxhOs6Pv"}},{"cell_type":"markdown","source":["###9. 파인튜닝 모델 테스트\n","AutoPeftModelForCausalLM()의 입력으로 LoRA Adapter 가 저장된 체크포인트의 주소를 넣으\n","면 LoRA Adapter 가 기존의 LLM 과 부착되어 로드됩니다. 이 과정은 LoRA Adapter 의 가중치를 사전 학\n","습된 언어 모델 (LLM) 에 통합하여 미세 조정된 모델을 완성하는 것을 의미합니다.\n"],"metadata":{"id":"9wsVx-vms7jk"}},{"cell_type":"code","source":["import torch\n","from peft import AutoPeftModelForCausalLM\n","from transformers import AutoTokenizer, pipeline\n","peft_model_id = \"llama3-8b-summarizer-ko/checkpoint-372\"\n","fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n","pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n"],"metadata":{"id":"iJB7vdfCs813"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["peft_model_id 변수는 미세 조정된 가중치가 저장된 체크포인트의 경로를 나타냅니다. \"llama3\n","-8b-summarizer-ko/checkpoint-372\"는 LoRA Adapter 가중치가 저장된 위치로, 이 경로에서\n","해당 가중치를 불러옵니다.\n","fine_tuned_model은 AutoPeftModelForCausalLM.from_pretrained 메서드를통해 체\n","크포인트를 로드하여 생성됩니다. 이 메서드는 LLM 과 LoRA Adapter 를 결합하고, 최적화된 설정으로 모\n","델을 메모리에 로드합니다. device_map=\"auto\" 옵션은 모델을 자동으로 GPU 에 배치합니다.\n","pipeline은 허깅페이스의 고수준 유틸리티로 모델과 토크나이저만 전달하면 모델로 전달한 입력에\n","대해서 바로 답변을 반환하는 일종의 LLM 호출 함수를 자동으로 만들어주는 역할을 합니다. 이 코드에\n","서 사용된 pipeline(\"text-generation\")은 텍스트 생성 작업을 수행하기 위한 파이프라인 객\n","체를 생성합니다.\n"],"metadata":{"id":"vZ9sEkX9tYbj"}},{"cell_type":"code","source":["eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]\n","def test_inference(pipe, prompt):\n","  outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n","  return outputs[0]['generated_text'][len(prompt):].strip()\n"],"metadata":{"id":"QXdXvuYvr4ac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["eos_token은 모델이 텍스트 생성을 멈추는 시점을 알려주는 특수 토큰, 구체적으로는 종료 토큰의 정\n","수를 저장하는 변수입니다. 종료 토큰이란 모델이 해당 토큰을 생성하는 순간 답변할 내용은 다 생성하\n","였으니 더 이상 생성을 할 필요가 없으므로 생성을 중단시키는 일종의 신호 역할을 합니다. 라마에서는\n","\"<|eot_id|>\"에 해당하며 해당 토큰을 토크나이저를 통해 정수 인코딩하여 그 토큰과 맵핑되는 정수\n","를 eos_token 변수에 저장합니다.\n","test_inference 함수는 실제로 모델이 텍스트를 생성하는 부분입니다. max_new_tokens=1024\n","는 최대 1024 개의 새로운 토큰을 생성하도록 제한합니다. 다시 말해 답변은 해당 길이를 넘어서 생성될\n","수 없습니다. 이제 테스트 데이터 중에서 10 번부터 14 번 샘플까지 모델을 연속으로 호출하여 실제 레이\n","블과 모델의 예측값을 비교하며 출력해보도록 하겠습니다."],"metadata":{"id":"HuIs4---tiAb"}},{"cell_type":"code","source":["for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n","  print(f\" response:\\n{test_inference(pipe, prompt)}\")\n","  print(f\" label:\\n{label}\")\n","  print(\"-\"*50)\n"],"metadata":{"id":"mxQ_tRIttikQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["파인 튜닝된 모델은 레이블과 비교해서도 거의 유사한 내용을 예측하며 준수한 답변을 생성합니다. 그렇\n","다면 학습이 전혀 되지 않은 기본 모델의 답변은 어떨지 동일한 데이터를 기본 모델에 입력하여 답변을 생\n","성해보겠습니다."],"metadata":{"id":"wktGalrHtv65"}},{"cell_type":"markdown","source":["###10. 기본 모델 테스트\n","이번에는 LoRA Adapter 를 merge 하지 않은 기본 모델로 테스트 데이터에 대해서 답변을 출력해보겠습니다."],"metadata":{"id":"Bu0ZcmNrty3i"}},{"cell_type":"code","source":["base_model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n","model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n","  print(f\" response:\\n{test_inference(pipe, prompt)}\")\n","  print(f\" label:\\n{label}\")\n","  print(\"-\"*50)\n"],"metadata":{"id":"XGxgPLFDtjoM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["파인 튜닝하지 않은 기본 모델은 파인튜닝 모델과는 달리 전반적으로 답변이 정돈되지 못하고 레이블과\n","비교하면 오답도 많이 포함된 것으로 보입니다. 이처럼 파인 튜닝을 통해 특정 도메인에서 사용되는 거대\n","언어 모델의 답변 성능을 높일 수 있습니다. 여러분들도 여러분들만의 도메인에 맞는 학습 데이터를 구축\n","하여 모델의 성능을 높여보시기 바랍니다.\n"],"metadata":{"id":"NXTm9xeruG81"}},{"cell_type":"code","source":[],"metadata":{"id":"SCLzF5ywtjko"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5Krv5CDhNrO99ru/FlAck"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"067381e6e1e746859d32d244aaf76ac3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a36f1c4894714cd68c79c338f613035f","IPY_MODEL_fc905cc9d4304b03befca94fb9fd4d4a","IPY_MODEL_973a19e297864e44b990ac84982f09c3"],"layout":"IPY_MODEL_4e6350c122bd4ea3a66f2bab416c82d1"}},"a36f1c4894714cd68c79c338f613035f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfbecc04b30a45d08f6c6274f1501b2b","placeholder":"​","style":"IPY_MODEL_a2d14ade52af44029bbfe87a47842969","value":"README.md: 100%"}},"fc905cc9d4304b03befca94fb9fd4d4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_544f176a482a4dc99cbec65234a1dfac","max":781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b199b82d1514a018a487514152c4c3e","value":781}},"973a19e297864e44b990ac84982f09c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29ab92ac32034afa8298aa6f66b376de","placeholder":"​","style":"IPY_MODEL_fcf3dd0e1fe8413895a0cd9d9f02a567","value":" 781/781 [00:00&lt;00:00, 56.1kB/s]"}},"4e6350c122bd4ea3a66f2bab416c82d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfbecc04b30a45d08f6c6274f1501b2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2d14ade52af44029bbfe87a47842969":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"544f176a482a4dc99cbec65234a1dfac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b199b82d1514a018a487514152c4c3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29ab92ac32034afa8298aa6f66b376de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf3dd0e1fe8413895a0cd9d9f02a567":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0052ace845d74edd9254c6fd9e63f8e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a003f836fb13433b95cadea2eb61ce6f","IPY_MODEL_c6435a7ac5204835adfc47bccd1a8e8d","IPY_MODEL_9b12a33a20b04300a02a5f4c876dfc67"],"layout":"IPY_MODEL_e1e38b70b0cf41d885dafad8db87d203"}},"a003f836fb13433b95cadea2eb61ce6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d983c14eb7a143dc8f0c36da88a95554","placeholder":"​","style":"IPY_MODEL_423e4910deae487c82fc9abd33f25b2a","value":"data/train-00000-of-00001.parquet: 100%"}},"c6435a7ac5204835adfc47bccd1a8e8d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeace58a9616405ab004eccc3b3d1e25","max":1704007,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9041a75d9bf415c992afead6d61d272","value":1704007}},"9b12a33a20b04300a02a5f4c876dfc67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9a4a99dd09f423f86af47520bcd66fc","placeholder":"​","style":"IPY_MODEL_e4c983b0ff24434d8584a7ea571dc37b","value":" 1.70M/1.70M [00:00&lt;00:00, 1.85MB/s]"}},"e1e38b70b0cf41d885dafad8db87d203":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d983c14eb7a143dc8f0c36da88a95554":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"423e4910deae487c82fc9abd33f25b2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeace58a9616405ab004eccc3b3d1e25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9041a75d9bf415c992afead6d61d272":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9a4a99dd09f423f86af47520bcd66fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4c983b0ff24434d8584a7ea571dc37b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"624acb8dfdc54aa6a02a662ce813b4ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58c0f089847e42f9a65bfe8f92f58e9c","IPY_MODEL_2e6ed55a0b60476eb26c485fa69e4f45","IPY_MODEL_fa456217341b4cccb69752122c74f9c5"],"layout":"IPY_MODEL_40e69c697d904f4db1b33846ba89345f"}},"58c0f089847e42f9a65bfe8f92f58e9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_444231a58ad3454383d55e25e6fa048e","placeholder":"​","style":"IPY_MODEL_b474e78324aa41ae9fa4afba6d6527c5","value":"Generating train split: 100%"}},"2e6ed55a0b60476eb26c485fa69e4f45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_513eace20de448ada204ca685f07b609","max":991,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b9461dc681e4af99b074f07938f02ae","value":991}},"fa456217341b4cccb69752122c74f9c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddc9940c23f64fc6bb7a4112f6f9e371","placeholder":"​","style":"IPY_MODEL_50886c8de04b4a7794563f7ed4e66ea3","value":" 991/991 [00:00&lt;00:00, 5233.58 examples/s]"}},"40e69c697d904f4db1b33846ba89345f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"444231a58ad3454383d55e25e6fa048e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b474e78324aa41ae9fa4afba6d6527c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"513eace20de448ada204ca685f07b609":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b9461dc681e4af99b074f07938f02ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddc9940c23f64fc6bb7a4112f6f9e371":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50886c8de04b4a7794563f7ed4e66ea3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e84cc62ef0f4df7b9429b1267d8d09a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31925282af894b4598c1e0b5a2963c3e","IPY_MODEL_7d8db26d14ff4a03abed9fc2260cec6f","IPY_MODEL_42d8306f40fd4393bb8b3d0dfbcb943c"],"layout":"IPY_MODEL_97691aeee22a41eca50211c8f9f30e7a"}},"31925282af894b4598c1e0b5a2963c3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e8245b8a8c04341bb09dce4b2bd00f8","placeholder":"​","style":"IPY_MODEL_1a95031e561d4712b7983fef15d1c31b","value":"config.json: 100%"}},"7d8db26d14ff4a03abed9fc2260cec6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_251eab513b1c4ce6b808b9cc16ca21ef","max":777,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ad257a411d44dfb8acb74b866a0c31a","value":777}},"42d8306f40fd4393bb8b3d0dfbcb943c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89e2555c27e749be8194b5627621bc9e","placeholder":"​","style":"IPY_MODEL_1138b5d39c7346199f7406d9ca4fa5da","value":" 777/777 [00:00&lt;00:00, 19.2kB/s]"}},"97691aeee22a41eca50211c8f9f30e7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e8245b8a8c04341bb09dce4b2bd00f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a95031e561d4712b7983fef15d1c31b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"251eab513b1c4ce6b808b9cc16ca21ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ad257a411d44dfb8acb74b866a0c31a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89e2555c27e749be8194b5627621bc9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1138b5d39c7346199f7406d9ca4fa5da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d7a5ee09d25481e8c0f98099152b04c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24b3107431fa468a81e5485e94573afe","IPY_MODEL_af2e7adec9934721af8189787fa46acf","IPY_MODEL_5cef37a51a6b4684b50ce6b44fd980a3"],"layout":"IPY_MODEL_2e791b9765b14e5f9a5dcdfea2fd6f29"}},"24b3107431fa468a81e5485e94573afe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e22beddc911e4cd4981897c6ce3c455c","placeholder":"​","style":"IPY_MODEL_83f90e593f7e475185073f1dd716873e","value":"model.safetensors.index.json: "}},"af2e7adec9934721af8189787fa46acf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed5c9febffee4524a9795be63aa47529","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a8832e681684e0ea10200fc86641bac","value":1}},"5cef37a51a6b4684b50ce6b44fd980a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8831d1d24fb64a9ca0a2973dc72bc9fc","placeholder":"​","style":"IPY_MODEL_770cdc1201ad4b90bd2ec39bd9fa4474","value":" 23.9k/? [00:00&lt;00:00, 437kB/s]"}},"2e791b9765b14e5f9a5dcdfea2fd6f29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e22beddc911e4cd4981897c6ce3c455c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83f90e593f7e475185073f1dd716873e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed5c9febffee4524a9795be63aa47529":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"8a8832e681684e0ea10200fc86641bac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8831d1d24fb64a9ca0a2973dc72bc9fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"770cdc1201ad4b90bd2ec39bd9fa4474":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e38838e9d6343dfbbc3d6a9138a464e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dca95ac78d574a3a8a4effdea3d50278","IPY_MODEL_c43339f7bba749079fa60a9a2d7fc979","IPY_MODEL_c8d1aafe4891404b8d05705d9ef92b6e"],"layout":"IPY_MODEL_d9c9850ddfa24df6b0dbd730a332bebd"}},"dca95ac78d574a3a8a4effdea3d50278":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3f87422cb88460d8612cfdbedd715d7","placeholder":"​","style":"IPY_MODEL_7ee8765f526b48e4bdeb330d921dcb21","value":"Downloading shards: 100%"}},"c43339f7bba749079fa60a9a2d7fc979":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40d9b992cb354dc68d3bab5745717eb6","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c883d3003a64d81b6f3ef4e7d20983b","value":4}},"c8d1aafe4891404b8d05705d9ef92b6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbc58fa2b60041d292d9e96656e9d7f2","placeholder":"​","style":"IPY_MODEL_12648b520560484580c0d4a7c0c03ce8","value":" 4/4 [08:44&lt;00:00, 122.21s/it]"}},"d9c9850ddfa24df6b0dbd730a332bebd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3f87422cb88460d8612cfdbedd715d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ee8765f526b48e4bdeb330d921dcb21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40d9b992cb354dc68d3bab5745717eb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c883d3003a64d81b6f3ef4e7d20983b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbc58fa2b60041d292d9e96656e9d7f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12648b520560484580c0d4a7c0c03ce8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70b2f63fd6d74642a13209a1767934b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fee7f511dcac471ebb74013cf7451606","IPY_MODEL_909aa29c1eb6459e8d5be9eec4e4ddf0","IPY_MODEL_0e1e05a278e94a1282f520481bfb909a"],"layout":"IPY_MODEL_cf93934250dc4312880a0be40da5f372"}},"fee7f511dcac471ebb74013cf7451606":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f4f5c4138cd4bc8a2f762e5454e0b1c","placeholder":"​","style":"IPY_MODEL_7b3f198f23544241afba94091e427361","value":"model-00001-of-00004.safetensors: 100%"}},"909aa29c1eb6459e8d5be9eec4e4ddf0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e13537b5a7fd4c0dba768934c1c659ad","max":4976698672,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09d6d9fcd40e4552ae9f036f63963b11","value":4976698672}},"0e1e05a278e94a1282f520481bfb909a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d68d88c37e94003a5a6e0f7c6dbe749","placeholder":"​","style":"IPY_MODEL_530b76003ff84677b31667fde1367039","value":" 4.98G/4.98G [02:28&lt;00:00, 34.8MB/s]"}},"cf93934250dc4312880a0be40da5f372":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f4f5c4138cd4bc8a2f762e5454e0b1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b3f198f23544241afba94091e427361":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e13537b5a7fd4c0dba768934c1c659ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09d6d9fcd40e4552ae9f036f63963b11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d68d88c37e94003a5a6e0f7c6dbe749":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"530b76003ff84677b31667fde1367039":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac5cb4e92d2f42848697d66ee9923200":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5d31b8381854e248ee5472c4e78272e","IPY_MODEL_6b6221e3b1444002832b14b2d10256e3","IPY_MODEL_43ce795974414799ae00d0723719eec3"],"layout":"IPY_MODEL_ea20f57d1eb34ca5aedbd414c2550940"}},"a5d31b8381854e248ee5472c4e78272e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c68b050570247549e2df37657ba75e1","placeholder":"​","style":"IPY_MODEL_a9fd8a070eee4c64b18a009dbc76a141","value":"model-00002-of-00004.safetensors: 100%"}},"6b6221e3b1444002832b14b2d10256e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bf3abdc5f3b447189933e70a8e20210","max":4999802720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3753517e9130497f804016e3e82c9566","value":4999802720}},"43ce795974414799ae00d0723719eec3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3efe0bc7eb93448a8ec70752de24035e","placeholder":"​","style":"IPY_MODEL_1e8f6bcaff0d45958cb510dad0d05b73","value":" 5.00G/5.00G [02:30&lt;00:00, 151MB/s]"}},"ea20f57d1eb34ca5aedbd414c2550940":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c68b050570247549e2df37657ba75e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9fd8a070eee4c64b18a009dbc76a141":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bf3abdc5f3b447189933e70a8e20210":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3753517e9130497f804016e3e82c9566":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3efe0bc7eb93448a8ec70752de24035e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e8f6bcaff0d45958cb510dad0d05b73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eacc14318ccd4cc4ac577ab1886d17e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a17d4867fd054f7a8a95608a0eb864a1","IPY_MODEL_de4bf52de0d0493aa455ab087d6dff13","IPY_MODEL_56a1ea97cecb4425a70afa0634b032ae"],"layout":"IPY_MODEL_4ec89108edf54193952d12bb0487ed75"}},"a17d4867fd054f7a8a95608a0eb864a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0581f003932049378fef038784a95bfc","placeholder":"​","style":"IPY_MODEL_6d98015a0dd8448bb564971a076f52d0","value":"model-00003-of-00004.safetensors: 100%"}},"de4bf52de0d0493aa455ab087d6dff13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6106a5ba52f24495a86d5453cee47a78","max":4915916176,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8618624614834d6b9a1ce39cc132e67f","value":4915916176}},"56a1ea97cecb4425a70afa0634b032ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82ac1dec63a94f9c8bf096fe3e92af57","placeholder":"​","style":"IPY_MODEL_7d5e2dc7e293483498bdde648a9e1b8b","value":" 4.92G/4.92G [02:13&lt;00:00, 40.0MB/s]"}},"4ec89108edf54193952d12bb0487ed75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0581f003932049378fef038784a95bfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d98015a0dd8448bb564971a076f52d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6106a5ba52f24495a86d5453cee47a78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8618624614834d6b9a1ce39cc132e67f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82ac1dec63a94f9c8bf096fe3e92af57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d5e2dc7e293483498bdde648a9e1b8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c8b9b48b6a540e8a69202cbb4677bf0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d74c0f6d03064b30a42dec657be16147","IPY_MODEL_6686d61aebbb476eb3a2a8441830a885","IPY_MODEL_4c3a35155307498ba6b0d5b0ebf9b4b1"],"layout":"IPY_MODEL_8a7ccc55a8564168b8dfa714cfafeb73"}},"d74c0f6d03064b30a42dec657be16147":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_831d61cf8f8647d6b512355e08d9f7a0","placeholder":"​","style":"IPY_MODEL_f6c32b57424045cb955b98bab1dd7603","value":"model-00004-of-00004.safetensors: 100%"}},"6686d61aebbb476eb3a2a8441830a885":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5177f24262e462b8e1eb084009536f1","max":1168138808,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71399d7634c54f65b97e1e18f8d264ac","value":1168138808}},"4c3a35155307498ba6b0d5b0ebf9b4b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e7f9e2f184f455f955956f80b0818d7","placeholder":"​","style":"IPY_MODEL_319ad8dc21ad4070955dbfbba77a1dca","value":" 1.17G/1.17G [01:30&lt;00:00, 25.0MB/s]"}},"8a7ccc55a8564168b8dfa714cfafeb73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"831d61cf8f8647d6b512355e08d9f7a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6c32b57424045cb955b98bab1dd7603":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5177f24262e462b8e1eb084009536f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71399d7634c54f65b97e1e18f8d264ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e7f9e2f184f455f955956f80b0818d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"319ad8dc21ad4070955dbfbba77a1dca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3eac6e98f634d1a88117ff53583781b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0e7f59b315c47758cc289f40aa616ba","IPY_MODEL_e29bcfd1911244b8828628479c778f0f","IPY_MODEL_7dcf2dbfa3bb414a90ce20d69ac31936"],"layout":"IPY_MODEL_35fd944a56db4d27a5c87b645c30edfa"}},"a0e7f59b315c47758cc289f40aa616ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2af4eeef2a9c4ee7b79d92407337804c","placeholder":"​","style":"IPY_MODEL_24568b23824e476fa8180083fe4fc32a","value":"Loading checkpoint shards: 100%"}},"e29bcfd1911244b8828628479c778f0f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_90d2bee43e6f4eea8983ed98bd5d3ab2","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b054db9dde804095b39eb0eb355dec2a","value":4}},"7dcf2dbfa3bb414a90ce20d69ac31936":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_661462ba03d841d7b2d2eba26ffa05dd","placeholder":"​","style":"IPY_MODEL_5796f2926a06423f82f51ba868ba9809","value":" 4/4 [00:02&lt;00:00,  1.42s/it]"}},"35fd944a56db4d27a5c87b645c30edfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2af4eeef2a9c4ee7b79d92407337804c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24568b23824e476fa8180083fe4fc32a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90d2bee43e6f4eea8983ed98bd5d3ab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b054db9dde804095b39eb0eb355dec2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"661462ba03d841d7b2d2eba26ffa05dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5796f2926a06423f82f51ba868ba9809":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"beec095dd73147aab7562bd121c7a1bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa00af306678462e9caeb067d3359435","IPY_MODEL_74973a0a67d849f4983fdbea76d5ae94","IPY_MODEL_0af1da908413495fbc9021732cf8aa47"],"layout":"IPY_MODEL_46ee9e5c224d41bca98fab794e7fec41"}},"aa00af306678462e9caeb067d3359435":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86a04700627f415ca2e02a642d5dc7c5","placeholder":"​","style":"IPY_MODEL_549a4ba41df940139a2d30885b798dca","value":"tokenizer_config.json: "}},"74973a0a67d849f4983fdbea76d5ae94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_617fa085f75b4bd38c4ee511e9a47fbd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcd80c3ef5df4dd99b046c2fd7a517f5","value":1}},"0af1da908413495fbc9021732cf8aa47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd521c6fd31d4b4ba728589b83d831be","placeholder":"​","style":"IPY_MODEL_6ea49000f7f446148c389070e0daf12d","value":" 51.2k/? [00:00&lt;00:00, 1.26MB/s]"}},"46ee9e5c224d41bca98fab794e7fec41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86a04700627f415ca2e02a642d5dc7c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"549a4ba41df940139a2d30885b798dca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"617fa085f75b4bd38c4ee511e9a47fbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"fcd80c3ef5df4dd99b046c2fd7a517f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd521c6fd31d4b4ba728589b83d831be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ea49000f7f446148c389070e0daf12d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbd5e155cb924f78ab6639b42030463a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c3d12a4758541ac9da9a8665a11a6b5","IPY_MODEL_1466467fe10c42c3bedf7a07858e54a0","IPY_MODEL_c8686c9089f44a64868eae14de52becb"],"layout":"IPY_MODEL_88e3f7a43a984cd99467be9c44732379"}},"7c3d12a4758541ac9da9a8665a11a6b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d0dbae1c7624ff28830ab2b251cf11e","placeholder":"​","style":"IPY_MODEL_ca11c605680b4e21a786b260e7d39045","value":"tokenizer.json: "}},"1466467fe10c42c3bedf7a07858e54a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f1e48bb2d3b4189bb966f1d7bdc0841","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0c9414f5202435e90b32201345aaaf7","value":1}},"c8686c9089f44a64868eae14de52becb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c21d73c52273499d8ab17151d73a6b75","placeholder":"​","style":"IPY_MODEL_cdf0176695c84da7a1ae7cdbd0454749","value":" 9.08M/? [00:00&lt;00:00, 30.8MB/s]"}},"88e3f7a43a984cd99467be9c44732379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d0dbae1c7624ff28830ab2b251cf11e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca11c605680b4e21a786b260e7d39045":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f1e48bb2d3b4189bb966f1d7bdc0841":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f0c9414f5202435e90b32201345aaaf7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c21d73c52273499d8ab17151d73a6b75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdf0176695c84da7a1ae7cdbd0454749":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d81d042fc1a445379f3715202a991ebf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9a7e28f278e42e8a80a7541ea0e6a09","IPY_MODEL_6303d18b132e469e88fe93ecddc19c3c","IPY_MODEL_dbd158d36d4c4c1995b59bb1e28eff1d"],"layout":"IPY_MODEL_9fd23e9141e841faabbdb39b4e089bc7"}},"c9a7e28f278e42e8a80a7541ea0e6a09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08632377a63141e1a661b152e80ae8dd","placeholder":"​","style":"IPY_MODEL_28e25eacff744109b9aafd974e775b96","value":"special_tokens_map.json: 100%"}},"6303d18b132e469e88fe93ecddc19c3c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a9a9215a9a14c988ff9855d64ac3390","max":430,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bee122f197204e49aa366562a8388765","value":430}},"dbd158d36d4c4c1995b59bb1e28eff1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a775d6d98b774490bb4b3ce545541ce0","placeholder":"​","style":"IPY_MODEL_ab280b53cb7045a697e207f33d91f24c","value":" 430/430 [00:00&lt;00:00, 8.39kB/s]"}},"9fd23e9141e841faabbdb39b4e089bc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08632377a63141e1a661b152e80ae8dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28e25eacff744109b9aafd974e775b96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a9a9215a9a14c988ff9855d64ac3390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bee122f197204e49aa366562a8388765":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a775d6d98b774490bb4b3ce545541ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab280b53cb7045a697e207f33d91f24c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}