{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1Vuusw8vTDN/LXlGT+kvJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###1.패딩(Padding)\n","자연어 처리를 하다보면 각 문장(또는 문서)은 서로 길이가 다를 수 있습니다. 그런데 기계는 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리할 수 있습니다. 다시 말해 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있습니다"],"metadata":{"id":"JWCi2RU3F-ks"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkzMpJUoF8jr","executionInfo":{"status":"ok","timestamp":1749110309022,"user_tz":-540,"elapsed":6716,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"c8dee3ea-c161-43d3-cd8a-c5c5c1d53cab"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","\n","nltk.download('punkt') # Download the necessary punkt resource\n","nltk.download('punkt_tab') # Download the necessary punkt_tab resource\n","nltk.download('stopwords') # Download the necessary stopwords resource"]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"],"metadata":{"id":"04w2FfxjGQQz","executionInfo":{"status":"ok","timestamp":1749110364293,"user_tz":-540,"elapsed":18,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["단어 집합을 만들고, 정수 인코딩을 수행합니다."],"metadata":{"id":"2yOjJn8FGjz6"}},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(preprocessed_sentences) # 단어 빈도수 기준 집합 생성\n","encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n","print(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMFLMVY5GRs3","executionInfo":{"status":"ok","timestamp":1749110692650,"user_tz":-540,"elapsed":57,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"3f9d5bb9-73ff-46be-f1f3-199f91b54b26"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"]}]},{"cell_type":"markdown","source":["모두 동일한 길이로 맞춰주기 위해서 이중에서 가장 길이가 긴 문장의 길이를 계산해보겠습니다."],"metadata":{"id":"GX0ByCGcHz4t"}},{"cell_type":"code","source":["max_len = max(len(item) for item in encoded)\n","print(\"최대 길이: \", max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dyIweXopGRqM","executionInfo":{"status":"ok","timestamp":1749110759706,"user_tz":-540,"elapsed":36,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"e22a3422-c52b-4267-9c70-8bfa3cb9520e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["최대 길이:  7\n"]}]},{"cell_type":"markdown","source":["가장 길이가 긴 문장의 길이는 7입니다. 모든 문장의 길이를 7로 맞춰주겠습니다. 이때 가상의 단어 'PAD'를 사용합니다. 'PAD'라는 단어가 있다고 가정하고, 이 단어는 0번 단어라고 정의합니다. 길이가 7보다 짧은 문장에는 숫자 0을 채워서 길이 7로 맞춰줍니다."],"metadata":{"id":"76vtJ5IrINIF"}},{"cell_type":"code","source":["for sentence in encoded:\n","  while len(sentence) < max_len:\n","    sentence.append(0)\n","\n","padded_np = np.array(encoded)\n","padded_np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n0KzfVfAGRnp","executionInfo":{"status":"ok","timestamp":1749110874635,"user_tz":-540,"elapsed":19,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"a4fef3f6-48e4-452a-d7f2-7fa3be0c87f0"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  5,  0,  0,  0,  0,  0],\n","       [ 1,  8,  5,  0,  0,  0,  0],\n","       [ 1,  3,  5,  0,  0,  0,  0],\n","       [ 9,  2,  0,  0,  0,  0,  0],\n","       [ 2,  4,  3,  2,  0,  0,  0],\n","       [ 3,  2,  0,  0,  0,  0,  0],\n","       [ 1,  4,  6,  0,  0,  0,  0],\n","       [ 1,  4,  6,  0,  0,  0,  0],\n","       [ 1,  4,  2,  0,  0,  0,  0],\n","       [ 7,  7,  3,  2, 10,  1, 11],\n","       [ 1, 12,  3, 13,  0,  0,  0]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["길이가 7보다 짧은 문장에는 전부 숫자 0이 뒤로 붙어서 모든 문장의 길이가 전부 7이된 것을 알 수 있습니다. 기계는 이들을 하나의 행렬로 보고, 병렬 처리를 할 수 있습니다. 또한, 0번 단어는 사실 아무런 의미도 없는 단어이기 때문에 자연어 처리하는 과정에서 기계는 0번 단어를 무시하게 될 것입니다. 이와 같이 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것을 패딩(padding)이라고 합니다. 숫자 0을 사용하고 있다면 제로 패딩(zero padding)이라고 합니다."],"metadata":{"id":"NeefUQH2IkFi"}},{"cell_type":"markdown","source":["###2.케라스 전처리 도구로 패딩하기"],"metadata":{"id":"16axqEr1Irkg"}},{"cell_type":"markdown","source":["encoded 값이 위에서 이미 패딩 후의 결과로 저장되었기 때문에 패딩 이전의 값으로 다시 되돌리겠습니다."],"metadata":{"id":"MF_O_KcbJCZx"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","encoded =tokenizer.texts_to_sequences(preprocessed_sentences)\n","print(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVGCwiwXGRkz","executionInfo":{"status":"ok","timestamp":1749110998301,"user_tz":-540,"elapsed":27,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"c4e1c81a-1dfa-455f-cd2f-96ebf763e4ad"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"]}]},{"cell_type":"code","source":["#케라스의 pad_sequences를 사용하여 패딩\n","padded = pad_sequences(encoded)\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoeTE83WGRiE","executionInfo":{"status":"ok","timestamp":1749111081589,"user_tz":-540,"elapsed":20,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"9b5e9926-576b-419c-fa45-1d1ff1ec0a3f"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  0,  0,  0,  0,  1,  5],\n","       [ 0,  0,  0,  0,  1,  8,  5],\n","       [ 0,  0,  0,  0,  1,  3,  5],\n","       [ 0,  0,  0,  0,  0,  9,  2],\n","       [ 0,  0,  0,  2,  4,  3,  2],\n","       [ 0,  0,  0,  0,  0,  3,  2],\n","       [ 0,  0,  0,  0,  1,  4,  6],\n","       [ 0,  0,  0,  0,  1,  4,  6],\n","       [ 0,  0,  0,  0,  1,  4,  2],\n","       [ 7,  7,  3,  2, 10,  1, 11],\n","       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라 앞에 0으로 채우기 때문입니다. 뒤에 0을 채우고 싶다면 인자로 padding='post'를 주면됩니다."],"metadata":{"id":"mr41yodMJV0g"}},{"cell_type":"code","source":["padded = pad_sequences(encoded, padding='post')\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgRzDSJiGRfE","executionInfo":{"status":"ok","timestamp":1749111132814,"user_tz":-540,"elapsed":19,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"4f864d95-4e20-4bae-98bd-77f54825fc72"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  5,  0,  0,  0,  0,  0],\n","       [ 1,  8,  5,  0,  0,  0,  0],\n","       [ 1,  3,  5,  0,  0,  0,  0],\n","       [ 9,  2,  0,  0,  0,  0,  0],\n","       [ 2,  4,  3,  2,  0,  0,  0],\n","       [ 3,  2,  0,  0,  0,  0,  0],\n","       [ 1,  4,  6,  0,  0,  0,  0],\n","       [ 1,  4,  6,  0,  0,  0,  0],\n","       [ 1,  4,  2,  0,  0,  0,  0],\n","       [ 7,  7,  3,  2, 10,  1, 11],\n","       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":[" 지금까지는 가장 긴 길이를 가진 문서의 길이를 기준으로 패딩을 한다고 가정하였지만, 실제로는 꼭 가장 긴 문서의 길이를 기준으로 해야하는 것은 아닙니다. 가령, 모든 문서의 평균 길이가 20인데 문서 1개의 길이가 5,000이라고 해서 굳이 모든 문서의 길이를 5,000으로 패딩할 필요는 없을 수 있습니다. 이와 같은 경우에는 길이에 제한을 두고 패딩할 수 있습니다. maxlen의 인자로 정수를 주면, 해당 정수로 모든 문서의 길이를 동일하게 합니다."],"metadata":{"id":"3MJ7CKXFJyBs"}},{"cell_type":"code","source":["padded = pad_sequences(encoded, padding='post', maxlen=5)\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7_6PFWiGRcm","executionInfo":{"status":"ok","timestamp":1749111261406,"user_tz":-540,"elapsed":22,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"68aa0c5f-5ae0-47d2-eaf4-e5a9f4389205"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  5,  0,  0,  0],\n","       [ 1,  8,  5,  0,  0],\n","       [ 1,  3,  5,  0,  0],\n","       [ 9,  2,  0,  0,  0],\n","       [ 2,  4,  3,  2,  0],\n","       [ 3,  2,  0,  0,  0],\n","       [ 1,  4,  6,  0,  0],\n","       [ 1,  4,  6,  0,  0],\n","       [ 1,  4,  2,  0,  0],\n","       [ 3,  2, 10,  1, 11],\n","       [ 1, 12,  3, 13,  0]], dtype=int32)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["길이가 5보다 짧은 문서들은 0으로 패딩되고, 기존에 5보다 길었다면 데이터가 손실됩니다. 가령, 뒤에서 두번째 문장은 본래 [ 7, 7, 3, 2, 10, 1, 11]였으나 현재는 [ 3, 2, 10, 1, 11]로 변경된 것을 볼 수 있습니다. 만약, 데이터가 손실될 경우에 앞의 단어가 아니라 뒤의 단어가 삭제되도록 하고싶다면 truncating이라는 인자를 사용합니다. truncating='post'를 사용할 경우 뒤의 단어가 삭제됩니다."],"metadata":{"id":"rQULuwG2KCLe"}},{"cell_type":"code","source":["padded = pad_sequences(encoded, padding='post', maxlen=5, truncating = 'post')\n","padded"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0F4z9mwGRZ1","executionInfo":{"status":"ok","timestamp":1749111342149,"user_tz":-540,"elapsed":23,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"274c630b-fec3-4394-cafd-4782b4475628"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1,  5,  0,  0,  0],\n","       [ 1,  8,  5,  0,  0],\n","       [ 1,  3,  5,  0,  0],\n","       [ 9,  2,  0,  0,  0],\n","       [ 2,  4,  3,  2,  0],\n","       [ 3,  2,  0,  0,  0],\n","       [ 1,  4,  6,  0,  0],\n","       [ 1,  4,  6,  0,  0],\n","       [ 1,  4,  2,  0,  0],\n","       [ 7,  7,  3,  2, 10],\n","       [ 1, 12,  3, 13,  0]], dtype=int32)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["###3.원-핫 인코딩(One-Hot Encoding)\n","자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법들이 있습니다. 원-핫 인코딩(One-Hot Encoding)은 그 많은 기법 중에서 단어를 표현하는 가장 기본적인 표현 방법입니다."],"metadata":{"id":"X3rQdkiZK6Uh"}},{"cell_type":"markdown","source":["원-핫 인코딩을 위해서 먼저 해야할 일은 단어 집합을 만드는 일입니다. 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 이를 단어 집합이라고 합니다. 그리고 이 단어 집합에 고유한 정수를 부여하는 정수 인코딩을 진행합니다. 텍스트에 단어가 총 5,000개가 존재한다면, 단어 집합의 크기는 5,000입니다. 5,000개의 단어가 있는 이 단어 집합의 단어들마다 1번부터 5,000번까지 인덱스를 부여한다고 해보겠습니다. 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 부여할 수 있습니다."],"metadata":{"id":"rlmzdQJ0Lku0"}},{"cell_type":"markdown","source":["원-핫 인코딩을 두 가지 과정으로 정리해보겠습니다. 첫째, 정수 인코딩을 수행합니다. 다시 말해 각 단어에 고유한 정수를 부여합니다. 둘째, 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여합니다. 한국어 문장을 예제로 원-핫 벡터를 만들어보겠습니다."],"metadata":{"id":"ay11oM47L9Bm"}},{"cell_type":"code","source":["# Okt형태소 분석기를 통해서 문장에 대해서 토큰화를 수행합니다.\n","!pip install konlpy\n","from konlpy.tag import Okt\n","\n","okt = Okt()\n","tokens = okt.morphs(\"나는 자연어 처리를 배운다\")\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUr9SHJKGRWw","executionInfo":{"status":"ok","timestamp":1749111900760,"user_tz":-540,"elapsed":15127,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"bad3ef6d-6258-472f-f7d9-2ffb07ca5e08"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.2 konlpy-0.6.0\n","['나', '는', '자연어', '처리', '를', '배운다']\n"]}]},{"cell_type":"markdown","source":["각 토큰에 대해서 고유한 정수를 부여합니다. 지금은 문장이 짧기 때문에 각 단어의 빈도수를 고려하지 않지만, 빈도수 순으로 단어를 정렬하여 정수를 부여하는 경우가 많습니다."],"metadata":{"id":"hwgHlWilMiIB"}},{"cell_type":"code","source":["word_to_index = {word : index for index, word in enumerate(tokens)}\n","\n","print('단어 집합: ', word_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ch3vNXavGRT5","executionInfo":{"status":"ok","timestamp":1749111992337,"user_tz":-540,"elapsed":38,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"33b84823-62bc-46ee-889f-bd42404ddc3a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합:  {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"]}]},{"cell_type":"markdown","source":["토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수를 만들었습니다."],"metadata":{"id":"02XoZEPlM8ig"}},{"cell_type":"code","source":["def one_hot_encoding(word, word_to_index):\n","  one_hot_vector = [0]*(len(word_to_index))\n","  index = word_to_index[word]\n","  one_hot_vector[index] = 1\n","  return one_hot_vector"],"metadata":{"id":"pXPjBIjzGRRI","executionInfo":{"status":"ok","timestamp":1749112150627,"user_tz":-540,"elapsed":30,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# '자연어'라는 단어의 원-핫 벡터를 얻어 봅시다.\n","one_hot_encoding('자연어', word_to_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipI6lrMpGROY","executionInfo":{"status":"ok","timestamp":1749112312624,"user_tz":-540,"elapsed":50,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d07d8a68-6a62-42fe-84fa-85f73a562e16"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 1, 0, 0, 0]"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["###4.케라스(Keras)를 이용한 원-핫 인코딩(One-Hot_Encoding)\n","케라스는 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원합니다. 이번에는 케라스만으로 정수 인코딩과 원-핫 인코딩을 순차적으로 진행해보도록 하겠습니다."],"metadata":{"id":"kFKV6MmFOG3p"}},{"cell_type":"code","source":["text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""],"metadata":{"id":"g762bdXEGRLg","executionInfo":{"status":"ok","timestamp":1749112431803,"user_tz":-540,"elapsed":43,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","print('단어 집합:', tokenizer.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9NhDtMlGRIu","executionInfo":{"status":"ok","timestamp":1749112547661,"user_tz":-540,"elapsed":22,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8682a775-8f60-4314-9715-2db5b66790ed"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 집합: {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"]}]},{"cell_type":"markdown","source":["위와 같이 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면, texts_to_sequences()를 통해서 이를 정수 시퀀스로 변환가능합니다. 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 만들어 확인해보겠습니다."],"metadata":{"id":"D0VEZ7I7PJ8y"}},{"cell_type":"code","source":["sub_text = \"점심 먹으러 갈래 메누는 햄버거 최고야\"\n","encoded = tokenizer.texts_to_sequences([sub_text])[0]\n","print(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BbPv8_XaO_vv","executionInfo":{"status":"ok","timestamp":1749112741223,"user_tz":-540,"elapsed":19,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"900fbeca-01af-4fe7-e352-aa097bf5c4f1"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 5, 1, 3, 7]\n"]}]},{"cell_type":"code","source":["one_hot = to_categorical(encoded)\n","print(one_hot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anBotTunPBoD","executionInfo":{"status":"ok","timestamp":1749112774056,"user_tz":-540,"elapsed":23,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"10e8d1f5-4e72-4d86-9e0d-16d14eaa63ea"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 1.]]\n"]}]},{"cell_type":"markdown","source":["###5.원-핫 인코딩(One-Hot Encoding)의 한계\n","이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있습니다. 다른 표현으로는 벡터의 차원이 늘어난다고 표현합니다. 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됩니다. 가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됩니다. 다시 말해 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 되는데 이는 저장 공간 측면에서는 매우 비효율적인 표현 방법입니다."],"metadata":{"id":"P1F8ZEJvP-wV"}},{"cell_type":"markdown","source":["또한 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점이 있습니다. 예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 합시다. 이때 원-핫 벡터로는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없습니다. 좀 더 극단적으로는 강아지, 개, 냉장고라는 단어가 있을 때 강아지라는 단어가 개와 냉장고라는 단어 중 어떤 단어와 더 유사한지도 알 수 없습니다."],"metadata":{"id":"BflMIzweQKRL"}},{"cell_type":"markdown","source":["단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서는 문제가 될 소지가 있습니다. 가령, 여행을 가려고 웹 검색창에 '삿포로 숙소'라는 단어를 검색한다고 합시다. 제대로 된 검색 시스템이라면, '삿포로 숙소'라는 검색어에 대해서 '삿포로 게스트 하우스', '삿포로 료칸', '삿포로 호텔'과 같은 유사 단어에 대한 결과도 함께 보여줄 수 있어야 합니다. 하지만 단어간 유사성을 계산할 수 없다면, '게스트 하우스'와 '료칸'과 '호텔'이라는 연관 검색어를 보여줄 수 없습니다."],"metadata":{"id":"tTjh-hK5QOBt"}},{"cell_type":"markdown","source":["이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있습니다. 첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다. 그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 존재합니다.\n","\n","여기서 언급한 방법들 중 대부분은 워드 임베딩 챕터에서 다루게 됩니다."],"metadata":{"id":"RDP4zAS6QQNI"}}]}