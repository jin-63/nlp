{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMdrAonZ4nQxkQbeLXcTC6M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8eb183840f5f4f44a5d809dc59cca800":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c226cd539c034da999d6c1f2a14f4a01","IPY_MODEL_5e4094cd37a04c0d90879dbe9c453671","IPY_MODEL_e3d83587c1e54c838af9359a29aa1ac8"],"layout":"IPY_MODEL_63e872232b8746cfbf1d1eac955e83ce"}},"c226cd539c034da999d6c1f2a14f4a01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99632c7ff6f6496fb3be34c8c20131e9","placeholder":"​","style":"IPY_MODEL_2288ea19e15f40f5a31bc9d83097e568","value":"config.json: "}},"5e4094cd37a04c0d90879dbe9c453671":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_389a652e41ce4a4f993f7b62af3b0ab4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ff5689d806049bfb6e0227520fa4c52","value":1}},"e3d83587c1e54c838af9359a29aa1ac8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_466835692f2d46b79373d8e185096bce","placeholder":"​","style":"IPY_MODEL_6deaf349e6f444c09f94fafe1261994d","value":" 1.00k/? [00:00&lt;00:00, 70.9kB/s]"}},"63e872232b8746cfbf1d1eac955e83ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99632c7ff6f6496fb3be34c8c20131e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2288ea19e15f40f5a31bc9d83097e568":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"389a652e41ce4a4f993f7b62af3b0ab4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"4ff5689d806049bfb6e0227520fa4c52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"466835692f2d46b79373d8e185096bce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6deaf349e6f444c09f94fafe1261994d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cdf79cc4379453789c2340ce4144bad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1776682da781450baf44f27031363f5b","IPY_MODEL_cfe91d6d7c5940b99fcc14fe7a27383e","IPY_MODEL_7ec1c46ea1a04b14b76e95b49a6efe4d"],"layout":"IPY_MODEL_302822572d324edea787bea9bcdf03ad"}},"1776682da781450baf44f27031363f5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2731003e331a456887cca6c68208dfb6","placeholder":"​","style":"IPY_MODEL_b24cb0d74c3a45d98fac94a8a49b9ea1","value":"pytorch_model.bin: 100%"}},"cfe91d6d7c5940b99fcc14fe7a27383e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86f5ebcde0054240b4bc8701436d3feb","max":513302779,"min":0,"orientation":"horizontal","style":"IPY_MODEL_224d4a3da48f4e80b8b26854e608ce17","value":513302779}},"7ec1c46ea1a04b14b76e95b49a6efe4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb5c0e6e053e46f89e1212c6d05e993c","placeholder":"​","style":"IPY_MODEL_f8ae8d16ba644b5cb49f573c89f26ef4","value":" 513M/513M [00:01&lt;00:00, 431MB/s]"}},"302822572d324edea787bea9bcdf03ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2731003e331a456887cca6c68208dfb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b24cb0d74c3a45d98fac94a8a49b9ea1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86f5ebcde0054240b4bc8701436d3feb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"224d4a3da48f4e80b8b26854e608ce17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb5c0e6e053e46f89e1212c6d05e993c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8ae8d16ba644b5cb49f573c89f26ef4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"352c794262e34c338c7f0c5720f3215c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_563ef46431784667969f7f92fa03ca83","IPY_MODEL_0ec0ba644f7840d791afe9c03b12d945","IPY_MODEL_56cbf3a8bd0c4a9da71d5046aa727cf8"],"layout":"IPY_MODEL_5bc7eebaee6247e5a104d070e4f5469f"}},"563ef46431784667969f7f92fa03ca83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed729a424cdd4edfaa26af7b47d12178","placeholder":"​","style":"IPY_MODEL_f6c4814d88b14359b432f863d3de9547","value":"tokenizer.json: "}},"0ec0ba644f7840d791afe9c03b12d945":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_430b91d76f304a03a5eb89c967a81c12","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc5fc2927d94470daa877844bbdf9033","value":1}},"56cbf3a8bd0c4a9da71d5046aa727cf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_872ce3c1f91a4a5ba544969eac7f375c","placeholder":"​","style":"IPY_MODEL_0a09b4f74d1047c5816cb27110c7f94a","value":" 2.83M/? [00:00&lt;00:00, 89.8MB/s]"}},"5bc7eebaee6247e5a104d070e4f5469f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed729a424cdd4edfaa26af7b47d12178":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6c4814d88b14359b432f863d3de9547":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"430b91d76f304a03a5eb89c967a81c12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"dc5fc2927d94470daa877844bbdf9033":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"872ce3c1f91a4a5ba544969eac7f375c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a09b4f74d1047c5816cb27110c7f94a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd9f2f4497484fe690273ad4d100fb5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b26ae73173b4e298fca9a699272e857","IPY_MODEL_5e9a0431ac9a453db36f17ec85a27808","IPY_MODEL_82deb2bd60104c0fbca2062b98ed85f9"],"layout":"IPY_MODEL_7306e73785794a38877bb526f24b628b"}},"5b26ae73173b4e298fca9a699272e857":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d3b2d44538e4b25bfa684e023ee4f02","placeholder":"​","style":"IPY_MODEL_76726f616cf040bb8acb9303a9a15ffa","value":"100%"}},"5e9a0431ac9a453db36f17ec85a27808":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec6649907048435bafeb90bb1f22ae18","max":370,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c1c483d92034ea1ae329c0e0b095ab2","value":370}},"82deb2bd60104c0fbca2062b98ed85f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6491ab48fb2244c3bca4640604ea7dc0","placeholder":"​","style":"IPY_MODEL_a17bc2ea12ce4415b90ff4efe66b30ac","value":" 370/370 [04:39&lt;00:00,  1.45it/s]"}},"7306e73785794a38877bb526f24b628b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d3b2d44538e4b25bfa684e023ee4f02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76726f616cf040bb8acb9303a9a15ffa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec6649907048435bafeb90bb1f22ae18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c1c483d92034ea1ae329c0e0b095ab2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6491ab48fb2244c3bca4640604ea7dc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a17bc2ea12ce4415b90ff4efe66b30ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e28a1144f4c240d5bef27fb4e22827b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3efe4141ffc4d849594315d8b680472","IPY_MODEL_ac8a224e62b0468facdb520ec71e654d","IPY_MODEL_9e6e76cb2b054d359dcc71c1154cf7ec"],"layout":"IPY_MODEL_c66a03890b04442380a0dda189fe9030"}},"a3efe4141ffc4d849594315d8b680472":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df24ae7324504c628e1493f3e645a8f2","placeholder":"​","style":"IPY_MODEL_b31b7df1638a4c8e953003d419f5bcfc","value":"100%"}},"ac8a224e62b0468facdb520ec71e654d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c498ce762b646e4b9546d3b625bcf28","max":370,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25c20d7a909940a2baba31e549052374","value":370}},"9e6e76cb2b054d359dcc71c1154cf7ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a2d74bded5410cadb5264edb03cf69","placeholder":"​","style":"IPY_MODEL_372609047ed54c8381bc6d15bd63a4af","value":" 370/370 [04:20&lt;00:00,  1.44it/s]"}},"c66a03890b04442380a0dda189fe9030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df24ae7324504c628e1493f3e645a8f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b31b7df1638a4c8e953003d419f5bcfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c498ce762b646e4b9546d3b625bcf28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25c20d7a909940a2baba31e549052374":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a2d74bded5410cadb5264edb03cf69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"372609047ed54c8381bc6d15bd63a4af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18938a2a9916449ca8d7fbe15b44dd13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b2c0fc093d741c89b3de6b5203dff23","IPY_MODEL_49652476ca2148608f54fa3abb66f287","IPY_MODEL_95cf313a980c43c888c22ebc6a880786"],"layout":"IPY_MODEL_4656fc68267f4575a6f6b58aa6ede933"}},"9b2c0fc093d741c89b3de6b5203dff23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a1b3600f3e84eea911e4bea60a1aa78","placeholder":"​","style":"IPY_MODEL_8397f3a29b724ce88baa71e197abc2b7","value":"100%"}},"49652476ca2148608f54fa3abb66f287":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc26b6b43760497590f2da5799c80b1f","max":370,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd0e10b1531e4bc2a034f853f0f1a3d9","value":370}},"95cf313a980c43c888c22ebc6a880786":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b1fa745459b46dbac10703a184274c0","placeholder":"​","style":"IPY_MODEL_4d39e6a7206b478f95ea21bdf84b093a","value":" 370/370 [04:20&lt;00:00,  1.43it/s]"}},"4656fc68267f4575a6f6b58aa6ede933":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a1b3600f3e84eea911e4bea60a1aa78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8397f3a29b724ce88baa71e197abc2b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc26b6b43760497590f2da5799c80b1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd0e10b1531e4bc2a034f853f0f1a3d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b1fa745459b46dbac10703a184274c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d39e6a7206b478f95ea21bdf84b093a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["##GPT-2를 이용한 문장 생성\n","사전 학습된 한국어 GPT‐2 를 이용하여 다음 문장 예측을 실습해봅시다. 이번 실습을 위해서만이 아니라\n","앞으로 사전 학습된 GPT 를 사용할 때는 transformers 라는 패키지를 자주 사용하게 됩니다. 실습 환경에\n","transformers 패키지를 설치해둡시다."],"metadata":{"id":"shlaVkOsia9s"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvzXVKzViYov","executionInfo":{"status":"ok","timestamp":1752834752627,"user_tz":-540,"elapsed":7264,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"1d29ccb8-2277-4a5b-e319-01044ebe45a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["###1. KoGPT-2로 문장 생성하기\n","transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT 와 마찬가지로 GPT 는 이미 누\n","군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야\n","합니다."],"metadata":{"id":"xfZMGhM0inY1"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","import tensorflow as tf\n","from transformers import AutoTokenizer, TFGPT2LMHeadModel"],"metadata":{"id":"6C09eAItimUx","executionInfo":{"status":"ok","timestamp":1752834771186,"user_tz":-540,"elapsed":18516,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["TFGPT2LMHeadModel.from_pretrained(‘GPT 모델 이름’) 을 넣으면 두 개의 문장이 이어지는 문장 관계\n","인지 여부를 판단하는 GPT 구조를 로드합니다. AutoTokenizer.from_pretrained(‘모델 이름’) 을 넣으면\n","해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다."],"metadata":{"id":"FS1uItxVi9U2"}},{"cell_type":"code","source":["model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n","tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351,"referenced_widgets":["8eb183840f5f4f44a5d809dc59cca800","c226cd539c034da999d6c1f2a14f4a01","5e4094cd37a04c0d90879dbe9c453671","e3d83587c1e54c838af9359a29aa1ac8","63e872232b8746cfbf1d1eac955e83ce","99632c7ff6f6496fb3be34c8c20131e9","2288ea19e15f40f5a31bc9d83097e568","389a652e41ce4a4f993f7b62af3b0ab4","4ff5689d806049bfb6e0227520fa4c52","466835692f2d46b79373d8e185096bce","6deaf349e6f444c09f94fafe1261994d","4cdf79cc4379453789c2340ce4144bad","1776682da781450baf44f27031363f5b","cfe91d6d7c5940b99fcc14fe7a27383e","7ec1c46ea1a04b14b76e95b49a6efe4d","302822572d324edea787bea9bcdf03ad","2731003e331a456887cca6c68208dfb6","b24cb0d74c3a45d98fac94a8a49b9ea1","86f5ebcde0054240b4bc8701436d3feb","224d4a3da48f4e80b8b26854e608ce17","eb5c0e6e053e46f89e1212c6d05e993c","f8ae8d16ba644b5cb49f573c89f26ef4","352c794262e34c338c7f0c5720f3215c","563ef46431784667969f7f92fa03ca83","0ec0ba644f7840d791afe9c03b12d945","56cbf3a8bd0c4a9da71d5046aa727cf8","5bc7eebaee6247e5a104d070e4f5469f","ed729a424cdd4edfaa26af7b47d12178","f6c4814d88b14359b432f863d3de9547","430b91d76f304a03a5eb89c967a81c12","dc5fc2927d94470daa877844bbdf9033","872ce3c1f91a4a5ba544969eac7f375c","0a09b4f74d1047c5816cb27110c7f94a"]},"id":"xcwEW0OJi7YZ","executionInfo":{"status":"ok","timestamp":1752834780590,"user_tz":-540,"elapsed":9415,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"0f5a8a65-c37b-40d7-e60b-134c56679f7b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eb183840f5f4f44a5d809dc59cca800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cdf79cc4379453789c2340ce4144bad"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.0.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'lm_head.weight', 'transformer.h.3.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n","- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"352c794262e34c338c7f0c5720f3215c"}},"metadata":{}}]},{"cell_type":"markdown","source":["GPT 가 생성할 문장의 방향성을 알려주기 위해서 시작 문자열을 정해줍니다. 여기서는 ‘근육이 커지기 위\n","해서는’ 이라는 문자열을 주고 GPT 에게 이어서 문장을 생성해보라고 해봅시다."],"metadata":{"id":"0JswfiRljWL3"}},{"cell_type":"code","source":["sent = '근육이 커지기 위해서는'"],"metadata":{"id":"F2_4y0XUjQvF","executionInfo":{"status":"ok","timestamp":1752834780628,"user_tz":-540,"elapsed":11,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["GPT 의 입력으로는 정수 인코딩 된 결과가 입력되어야 하므로 tokenizer.encode() 를 통해서’ 근육이 커지\n","기 위해서는’ 이라는 문자열을 정수 시퀀스로 변환해줍니다."],"metadata":{"id":"eil8eLs3jb6k"}},{"cell_type":"code","source":["input_ids = tokenizer.encode(sent)\n","\n","# tf.convert_to_tensor() 함수는 리스트, 튜플, 넘파이 배열, 스칼라 값 등 거의 모든 Python 객체를 Tensor로 바꿈\n","input_ids = tf.convert_to_tensor([input_ids])\n","\n","print(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WHSvXqOjapL","executionInfo":{"status":"ok","timestamp":1752834780666,"user_tz":-540,"elapsed":30,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"b3c49e39-a660-4138-942d-bd8926c8cf68"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[33245 10114 12748 11357]], shape=(1, 4), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["33245 10114 12748 11357 라는 5 개의 정수 시퀀스를 얻습니다. 해당 정수 시퀀스를 GPT 의 입력으로 사\n","용하여 GPT 가 이어서 문장을 생성하도록 해봅시다. 주어진 문장으로부터 이어서 문장을 생성하도록 하\n","는 것은 model.generate() 를 사용합니다."],"metadata":{"id":"YFODruZikTX7"}},{"cell_type":"code","source":["output = model.generate(input_ids, max_length=128, repetition_penalty=2.0,\n","                        use_cache=True)\n","output_ids = output.numpy().tolist()[0]\n","print(output_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1SWPGX4kDBc","executionInfo":{"status":"ok","timestamp":1752834811829,"user_tz":-540,"elapsed":31159,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"a0fc1963-9e74-4407-edef-30f0f71ce2b9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[33245, 10114, 12748, 11357, 23879, 39306, 9684, 7884, 10211, 15177, 26421, 387, 17339, 7889, 9908, 15768, 6903, 15386, 8146, 12923, 9228, 18651, 42600, 9564, 17764, 9033, 9199, 14441, 7335, 8704, 12557, 32030, 9510, 18595, 9025, 10571, 25741, 10599, 13229, 9508, 7965, 8425, 33102, 9122, 21240, 9801, 32106, 13579, 12442, 13235, 19430, 8022, 12972, 9566, 11178, 9554, 24873, 7198, 9391, 12486, 8711, 9346, 7071, 36736, 9693, 12006, 9038, 10279, 36122, 9960, 8405, 10826, 18988, 25998, 9292, 7671, 9465, 7489, 9277, 10137, 9677, 9248, 9912, 12834, 11488, 13417, 7407, 8428, 8137, 9430, 14222, 11356, 10061, 9885, 19265, 9377, 20305, 7991, 9178, 9648, 9133, 10021, 10138, 30315, 21833, 9362, 9301, 9685, 11584, 9447, 42129, 10124, 7532, 17932, 47123, 37544, 9355, 15632, 9124, 10536, 13530, 12204, 9184, 36152, 9673, 9788, 9029, 11764]\n"]}]},{"cell_type":"markdown","source":["기존의 33245 10114 12748 11357 라는 5 개의 정수 시퀀스 뒤에도 여러 정수들이 추가로 생성된 것을 볼\n","수 있습니다. 정수들이 단순히 나열된 것만으로는 GPT 가 실제로 어떤 문장을 생성했는지 알기 어려우니\n","해당 정수 시퀀스를 한국어로 변환해봅시다. 이 과정은 tokenizer.decode() 를 사용하여 가능합니다."],"metadata":{"id":"1Sk83Pq2kzc2"}},{"cell_type":"code","source":["tokenizer.decode(output_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"id":"6u-OWvFCkxaE","executionInfo":{"status":"ok","timestamp":1752834811849,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"cd8d81f1-3ff1-4830-cc58-7beb8cdddbf8"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\\n특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\\n또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\\n아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\\n운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\\n운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\\n운동을'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["‘근육이 커지기 위해서는’ 라는 문자열에 이어서 그 뒤에 근육이 커지기 위한이라는 문맥에 맞는듯한 글들\n","을 생성합니다. 물론 GPT 가 생성한 문장들은 문맥상 그럴듯해 보이지만 실제 사실 여부와는 다를 수 있\n","으니 이 점은 늘 주의해야 합니다."],"metadata":{"id":"VZHsH9HIlX_r"}},{"cell_type":"markdown","source":["###2. Numpy로 Top5 뽑기\n","GPT 는 기본적으로 이전 단어들로부터 다음 단어를 예측하는 언어 모델 (Language Model) 입니다. 위의\n","실습에서 확인한 바와 같이 ‘근육이 커지기 위해서는’ 이라는 입력을 넣었을 때 GPT 는 다음 단어로 ‘무엇\n","보다’ 라는 단어를 예측했었는데요. 실제로는 수많은 후보의 다음 단어들이 있었지만, 그 중 가장 확률이\n","높은 단어. 즉, Top 1 의 단어인’ 무엇보다’ 를 예측한 것입니다. 그렇다면 다음 단어로 또 어떤 후보들이\n","있었는지 Top 5 의 단어들을 뽑아봅시다. model() 에다가’ 근육이 커지기 위해서는’ 의 정수 인코딩 된 결\n","과를 입력으로 넣은 후 가장 확률이 높은 Top 5 의 단어들을 뽑아냅니다."],"metadata":{"id":"7fKC28D4lZ4r"}},{"cell_type":"code","source":["output = model(input_ids)\n","top5 = tf.math.top_k(output.logits[0, -1], k=5)"],"metadata":{"id":"Q-CFeHgflEUB","executionInfo":{"status":"ok","timestamp":1752834812060,"user_tz":-540,"elapsed":199,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 그 후에 TOP5의 단어를 한국어로 변환하여 출력해봅시다.\n","tokenizer.convert_ids_to_tokens(top5.indices.numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jYJoZyel52q","executionInfo":{"status":"ok","timestamp":1752834812096,"user_tz":-540,"elapsed":22,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"ad7b24fd-f63c-4b65-fcf9-7a75e6239ca4"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['▁무엇보다', '▁우선', '▁반드시', '▁피부', '▁무엇보다도']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["‘무엇보다’ 라는 단어 외에도 ‘우선’, ‘반드시’, ‘피부’, ‘무엇보다도’ 라는 4 개의 단어가 높은 확률로 선택될\n","수 있었음을 보여줍니다."],"metadata":{"id":"HYGCrx7NmdDu"}},{"cell_type":"markdown","source":["###3. Numpy Top5로 문장 생성하기\n","앞서 문장을 생성했을 당시에는 각 시점(time step)마다 가장 확률이 높은 단어를 예측했지만, 이번에는 매 시점마다 Top5개의 단어들 중에서 랜덤으로 선택하는 방식을 택하여 문장을 생성해 봅시다."],"metadata":{"id":"5CPPsMZ_mf98"}},{"cell_type":"code","source":["sent ='근육이 커지기 위해서는'\n","input_ids = tokenizer.encode(sent)\n","\n","while len(input_ids) < 50:\n","  output = model(np.array(input_ids))\n","\n","  # Top5의 단어들을 추출\n","  top5 = tf.math.top_k(output.logits[0,-1], k=5)\n","\n","  # Top5의 단어들 중 랜덤으로 다음 단어 선택\n","  token_id = random.choice(top5.indices.numpy())\n","  input_ids.append(token_id)\n","\n","tokenizer.decode(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"U4K52uFvmJw3","executionInfo":{"status":"ok","timestamp":1752834821526,"user_tz":-540,"elapsed":9426,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"c1bc23c9-f14a-4391-d154-be9fe528e4c5"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'근육이 커지기 위해서는 피부 속 콜라젠의 활성산소 농도를 높이는 것도 도움이 될 것이다.\\n특히 요즘은 미세 먼지와 황사가 심해져 피부 트러스가 심해지는 계절이라, 미세 입자가 많은 화장품 사용을 줄이고 싶지만 피부 깊숙이 침투하는 활성산'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## GPT-2를 이용한 한국어 챗봇\n"],"metadata":{"id":"du0cQFGooC0z"}},{"cell_type":"markdown","source":["###1. KoGPT-2의 모델과 토크나이저 로드\n","transformers 패키지를 사용하여 모델과 토크나이저를 로드합니다. BERT 와 마찬가지로 GPT 는 이미 누\n","군가가 학습해둔 모델을 사용하는 것이므로 우리가 사용하는 모델과 토크나이저는 항상 맵핑 관계여야\n","합니다."],"metadata":{"id":"kcr9wkKeoKwa"}},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import TFGPT2LMHeadModel"],"metadata":{"id":"PfQPE_KWnmpB","executionInfo":{"status":"ok","timestamp":1752834821682,"user_tz":-540,"elapsed":9,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["TFGPT2LMHeadModel.from_pretrained(‘GPT 모델 이름’) 을 넣으면 두 개의 문장이 이어지는 문장 관계\n","인지 여부를 판단하는 GPT 구조를 로드합니다. AutoTokenizer.from_pretrained(‘모델 이름’) 을 넣으면\n","해당 모델이 학습되었을 당시에 사용되었던 토크나이저를 로드합니다. 토크나이저를 로드할 때 텍스\n","트 생성을 시작할 때 사용하는 시작 토큰을 bos_token, 텍스트 생성을 종료할 때 사용하는 종료 토큰을\n","eos_token 이라는 이름으로 지정할 수 있습니다. 여기서는 <</s>/s>라는 토큰을 시작 토큰이자 종료 토큰으\n","로 사용하고자 합니다."],"metadata":{"id":"_N4rbhDdpN6j"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2', bos_token='</s>',\n","                                          eos_token='</s>', pad_token='<pad>')\n","model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uJT0pZCpBIA","executionInfo":{"status":"ok","timestamp":1752834825094,"user_tz":-540,"elapsed":3403,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"ef2635f8-7aab-465e-f4c7-0152ced2e13e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.0.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'lm_head.weight', 'transformer.h.3.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n","- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["GPT에는 특별한 용도로 사용하기 위해서 할당해놓은 특별 토큰(special token)들이 존재합니다. Ko-GPT2에서 사용되는 특별 토큰들의 정수 그리고 반대로 정수 1,2,3,4는 각각 어떤 토큰들을 의미하는지 확인해봅시다."],"metadata":{"id":"_joFdCS8qWb6"}},{"cell_type":"code","source":["print(tokenizer.bos_token_id)\n","print(tokenizer.eos_token_id)\n","print(tokenizer.pad_token_id)\n","print('-'*25)\n","print(tokenizer.decode(1))\n","print(tokenizer.decode(2))\n","print(tokenizer.decode(3))\n","print(tokenizer.decode(4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8YyjNHop4rO","executionInfo":{"status":"ok","timestamp":1752834825120,"user_tz":-540,"elapsed":17,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8943b64a-524c-49eb-aac1-c0ea28bb7d0b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","1\n","3\n","-------------------------\n","</s>\n","<usr>\n","<pad>\n","<sys>\n"]}]},{"cell_type":"markdown","source":["주로 문장의 시작을 의미하는 용도로 사용되는 BOS 토큰과 주로 문장의 종료를 의미하는 용도로 사용되는\n","EOS token 이 정수로는 몇 번인지 확인하기 위해서 tokenizer.bos_token_id 와 tokenizer.eos_token_id\n","를 각각 출력했습니다. 또한 패딩 토큰이 정수로는 몇 번인지 확인하기 위해서 tokenizer.pad_token_id\n","를 출력했습니다. 확인 결과, KoGPT‐2 에서는 BOS 토큰과 EOS 토큰이 동일한 정수 1 로 맵핑되는 것으로\n","확인되었으며 (이것은 KoGPT‐2 제작자들이 이렇게 정한 것입니다.) 패딩 토큰은 정수 3 으로 맵핑되는 것\n","을 확인했습니다."],"metadata":{"id":"iX2TOJY2siwX"}},{"cell_type":"markdown","source":["반대로 정수 1, 2, 3, 4 를 단어로 표현할 경우를 출력해보았습니다. 정수 1. 즉, BOS 토큰이기도 하고, EOS\n","토큰이기도 한 토큰은 </s>로, 패딩 토큰인 정수 3 은 <pad>로 출력되는 것을 확인했습니다. 정수 2 와\n","정수 4 의 경우에는 <usr>와 <sys>라는 토큰으로 맵핑되어져 있었는데, 이는 GPT 로 대화 모델을 만들\n","경우에 각각 유저 (User) 와 시스템 (System) 을 구분하기 위한 용도입니다. 이번 실습에서도 실제로 이 두\n","토큰을 이용하여 챗봇을 구현할 것이므로 어떠한 의미인지는 뒤에서 실습을 통해 이해해보겠습니다."],"metadata":{"id":"8Jx8zbBetxD6"}},{"cell_type":"markdown","source":["### 2. 챗봇 데이터 로드\n"],"metadata":{"id":"AKKzKyeGty_l"}},{"cell_type":"code","source":["import pandas as pd\n","import tqdm\n","import urllib.request"],"metadata":{"id":"ipkGAXDxr-jZ","executionInfo":{"status":"ok","timestamp":1752834825128,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n","train_data = pd.read_csv('ChatBotData.csv')\n","print('챗 봇 데 이 터 의 개 수 :', len(train_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x1K7_dDht950","executionInfo":{"status":"ok","timestamp":1752834825318,"user_tz":-540,"elapsed":182,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"862bca70-dcb6-4d96-9678-b84a16a0d7d7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["챗 봇 데 이 터 의 개 수 : 11823\n"]}]},{"cell_type":"markdown","source":["챗봇 데이터의 개수는 11,823 개로 해당 데이터는 앞서 **'BERT 의 문장 임베딩 (SBERT) 을 이용한 한국어 챗봇'** 에서 사용한 데이터와 동일하므로 데이\n","터에 대한 설명은 생략합니다."],"metadata":{"id":"n1AabpnluH0n"}},{"cell_type":"markdown","source":["###3. 챗봇 데이터 전처리\n","챗봇 데이터를 전처리하기 위한 함수 get_chat_data()를 만듭니다."],"metadata":{"id":"Ozi9rGs5uh-f"}},{"cell_type":"code","source":["def get_chat_data():\n","  for question, answer in zip(train_data.Q.to_list(), train_data.A.to_list()):\n","    bos_token = [tokenizer.bos_token_id]\n","    eos_token = [tokenizer.eos_token_id]\n","    sent = tokenizer.encode('<usr>' + question + '<sys>' + answer)\n","\n","    yield bos_token + sent + eos_token\n","  # yield는 값을 하나씩“잠깐 내보내고”함수 실행을 멈춘 뒤, 다음 호출 때 다시 이어서 실행할 수 있게 해줘요.\n"],"metadata":{"id":"ALc02vAJuAPl","executionInfo":{"status":"ok","timestamp":1752834825335,"user_tz":-540,"elapsed":11,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["위의 전처리는 <usr<usr>> 다음에 사용자의 질문을 부착하고, 그 후 <sys<sys>> 다음에 챗봇의 답변을 부착하는\n","형식으로 전처리를 진행합니다. 그리고 앞 뒤에 시작 토큰 (BOS 토큰) 과 종료 토큰 (EOS 토큰) 을 부착하\n","는 전처리입니다. 앞에서 확인했듯이 두 토큰은 동일하며 </s</s>>입니다. 전처리 후에 데이터가 어떻게 변\n","경되는지는 뒤에서 확인합니다. 배치 크기를 32 로 하고, 패딩 토큰으로 패딩을 진행합니다. 위에서 확인\n","했듯이 패딩 토큰의 정수는 3 입니다."],"metadata":{"id":"X43-xZ3yvy82"}},{"cell_type":"code","source":["batch_size = 32\n","dataset = tf.data.Dataset.from_generator(get_chat_data, output_types=tf.int32, output_shapes=(None,))\n","\n","# 배치 크기 만큼 데이터를 구성하되 패딩 토큰으로 패딩을 진행.\n","dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=(None,),\n","                               padding_values=tokenizer.pad_token_id)"],"metadata":{"id":"rd1J7Feyvi62","executionInfo":{"status":"ok","timestamp":1752834825378,"user_tz":-540,"elapsed":36,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["첫번째 배치 (첫 32 개의 데이터 묶음) 을 출력해봅시다."],"metadata":{"id":"zdJEf5G8xAVH"}},{"cell_type":"code","source":["# 첫 배치 출력\n","for batch in dataset:\n","  print(batch)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GV1IH_sGw9-K","executionInfo":{"status":"ok","timestamp":1752834825426,"user_tz":-540,"elapsed":40,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"36ffb381-c4f0-4c63-f01f-1075f7c19f48"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[    1     2  9349  7888   739  7318   376     4 12557  6824  9108  9028\n","   7098 25856     1     3     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9020  8263  7497 10192 11615  8210  8006     4 12422  8711\n","   9535  7483 12521     1     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9085  7597   395  8149 10624  7397 24224 13358  7182     4\n","  12079  8135 16899  9677  8234   389     1     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9085  7597   395  8149  9465 10624  7397 24224 13358  7182\n","      4 12079  8135 16899  9677  8234   389     1     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9943   422   418  9327  8702  7098     4  9847 16912 18328\n","   8671  7415  8263  8234   389     1     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9815   410 21249 10174  6824  8210  8006     4  9427 11056\n","  11594 10137 10556  9266  8711 25856     1     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9815   410 21249  9183  7249     4  9427 11056 11594 10137\n","  10556  9266  8711 25856     1     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9815 37655  9622  8619 10401  9183  9328   216     4  9443\n","  29490  9846  9788  9341 25856     1     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9815 37655 10135  7066 39488  9122  9050  9668 16576  9277\n","   9044     4 15148 19658  9098  7652  7801 25856     1     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9815 37655 10135  7066  7692 11848  9042  7019 20284  7254\n","      4 15148 19658  9098  7652  7801 25856     1     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9815 37655 18381  9063  7489 29615  9054 15730 29452  8030\n","      4 33254 10300 23775 25856     1     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 19319 48397  8711     4  9022 19858 27031  9122  8046 25856\n","      1     3     3     3     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 19319 46651 27481 48397  8711     4  9022 19858 27031  9122\n","   8046 25856     1     3     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 19319  8135  9749 10225  6866  9677  7182     4  9749  9589\n","  20540  7801 25856     1     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 17230 17429  9160  8098     4 10855  8135  9427 35813  9122\n","   8046 25856     1     3     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 47980 22227 26992  7058  7182     4 26992  8137  9376  8737\n","   8236  7801 25856     1     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 26629 23799   739  8308  7304 10174  8707     4  9105  7788\n","  16346  6889  9282  8400  7601  9078  7801 25856     1     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 15983  7673 24648  6889 25880  8006     4 16173 15582 46439\n","  35557  6889 12252  7801 25856     1     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 15983  7673 24648 15010 10926  6853 27511     4 16173 15582\n","  46439 35557  6889 12252  7801 25856     1     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 15983  7692 12371  9564 16409  9016     4  9536  9271  9052\n","   9267 27545  8711  7661 25856     1     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 15983  7692 36684  7220  9244  6958  9539  7478  6872  8006\n","      4 46503  9024  7801  8084   376     1     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 15983  7692 26873  9050  7177     4  9536  9271  9052  9267\n","  27545  8711  7661 25856     1     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2  9278 20861  9193   739  7570 47804     4  9278 20861 32392\n","  10070 10828 25856  9105 12114  9094 12191 12700 31279  8702 38887 15148\n","  35441  9328  9109  7801 25856     1]\n"," [    1     2 10464 12079  9028  9926  9651  8006     4  9586 27820  9432\n","  23100 21833 14247 29462  7801 25856     1     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 10464 12079 17577     4  9586 27820  9432 23100 21833 14247\n","  29462  7801 25856     1     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 10464 12079 42076  9340   406     4  9586 27820  9432 23100\n","  21833 14247 29462  7801 25856     1     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 10464  9341   406     4  9265  7470  9659  9701 11389 11676\n","   7177   387  9265  7380 11120  8711 10764 11389  9728 12245 22238  9341\n","   8084     1     3     3     3     3]\n"," [    1     2 10464 10143  9666   739  8244     4  9265  7470  9659  9701\n","  11389 11676  7177   387  9265  7380 11120  8711 10764 11389  9728 12245\n","  22238  9341  8084     1     3     3]\n"," [    1     2 10464 18264 12079  6826  9016     4  9267 25772  8267 25012\n","   9069  6872  7098 25856     1     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 10464  7285 10056 25799     4  9265  7235 25856     1     3\n","      3     3     3     3     3     3     3     3     3     3     3     3\n","      3     3     3     3     3     3]\n"," [    1     2 10464  9136  7380  9071  7513  8711     4  9054  7285  9117\n","   7703  7788 11120  8705 14553 10667  8718  7055  7661 25856     1     3\n","      3     3     3     3     3     3]\n"," [    1     2 10464  9136  7380  9071  7513  8711  8210  8006     4  9054\n","   7285  9117  7703  7788 11120  8705 14553 10667  8718  7055  7661 25856\n","      1     3     3     3     3     3]], shape=(32, 30), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["각 데이터의 맨 앞에는 정수 1, 2 가 부착되어져 있습니다. 정수 1, 2 는 앞에서\n","확인했던 것과 같이 각각 </s>와 <user>에 해당됩니다. 첫번째 배치는 현재 batch 라는 변수에 저장되\n","어져 있으므로 32 개의 데이터 중 가장 첫번째 샘플을 출력해봅시다. 이때 이미 정수 인코딩이 진행된 상\n","황이므로 tokenizer.decode() 를 통해 정수 인코딩 된 결과를 다시 복원하여 출력합니다."],"metadata":{"id":"n3out_jMzS-C"}},{"cell_type":"code","source":["# 첫 번 째 배 치(32개 의 샘 플) 중 첫 번 째 샘 플 출 력\n","print(tokenizer.decode(batch[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKO_jpPQxKQF","executionInfo":{"status":"ok","timestamp":1752834825506,"user_tz":-540,"elapsed":73,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"58408cfe-b014-49d4-ac41-074da2d0e7d5"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["</s><usr> 12시 땡!<sys> 하루가 또 가네요.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"]}]},{"cell_type":"markdown","source":["위의 get_chat_data() 함수로 전처리를 진행할 당시의 의도대로 사용자의 질문 앞에 <usr>가 붙어있고,\n","챗봇의 답변 앞에는 <sys>가 붙어있습니다. 그리고 사용자의 질문과 챗봇의 답변 앞 뒤로는 시작 토큰\n","과 종료 토큰에 해당하는 </s>이 붙어있으며, 그 뒤에는 패딩 토큰인 <pad>가 붙습니다. 이제 학습을\n","진행해봅시다."],"metadata":{"id":"890s8S7EzsQN"}},{"cell_type":"markdown","source":["###4. 챗봇 학습하기\n"],"metadata":{"id":"NCHPVODFzx5q"}},{"cell_type":"code","source":["# 옵티마이저 결정\n","adam = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n","\n","# 전체 데이터의 개수를 배치 크기로 나누면 하나의 에포크에서 실행되는 학습 횟수가 계산됨\n","steps = len(train_data) // batch_size + 1\n","\n","EPOCHS = 3\n","\n","for epoch in range(EPOCHS):\n","  epoch_loss = 0\n","\n","  for batch in tqdm.notebook.tqdm(dataset, total=steps):\n","    with tf.GradientTape() as tape:\n","      result = model(batch, labels=batch)\n","      loss = result[0]\n","      batch_loss = tf.reduce_mean(loss)\n","\n","    grads = tape.gradient(batch_loss, model.trainable_variables)\n","    adam.apply_gradients(zip(grads, model.trainable_variables))\n","    epoch_loss += batch_loss / steps\n","  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, epoch_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224,"referenced_widgets":["dd9f2f4497484fe690273ad4d100fb5a","5b26ae73173b4e298fca9a699272e857","5e9a0431ac9a453db36f17ec85a27808","82deb2bd60104c0fbca2062b98ed85f9","7306e73785794a38877bb526f24b628b","2d3b2d44538e4b25bfa684e023ee4f02","76726f616cf040bb8acb9303a9a15ffa","ec6649907048435bafeb90bb1f22ae18","1c1c483d92034ea1ae329c0e0b095ab2","6491ab48fb2244c3bca4640604ea7dc0","a17bc2ea12ce4415b90ff4efe66b30ac","e28a1144f4c240d5bef27fb4e22827b1","a3efe4141ffc4d849594315d8b680472","ac8a224e62b0468facdb520ec71e654d","9e6e76cb2b054d359dcc71c1154cf7ec","c66a03890b04442380a0dda189fe9030","df24ae7324504c628e1493f3e645a8f2","b31b7df1638a4c8e953003d419f5bcfc","8c498ce762b646e4b9546d3b625bcf28","25c20d7a909940a2baba31e549052374","e6a2d74bded5410cadb5264edb03cf69","372609047ed54c8381bc6d15bd63a4af","18938a2a9916449ca8d7fbe15b44dd13","9b2c0fc093d741c89b3de6b5203dff23","49652476ca2148608f54fa3abb66f287","95cf313a980c43c888c22ebc6a880786","4656fc68267f4575a6f6b58aa6ede933","5a1b3600f3e84eea911e4bea60a1aa78","8397f3a29b724ce88baa71e197abc2b7","fc26b6b43760497590f2da5799c80b1f","cd0e10b1531e4bc2a034f853f0f1a3d9","2b1fa745459b46dbac10703a184274c0","4d39e6a7206b478f95ea21bdf84b093a"]},"id":"pUO7TZtFyG-l","executionInfo":{"status":"ok","timestamp":1752835625524,"user_tz":-540,"elapsed":800015,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"02b8d776-ff49-4276-8d3d-bf6243bb5961"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/370 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd9f2f4497484fe690273ad4d100fb5a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f320c2fa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f320c2fa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch:    1] cost = 2.12697864\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/370 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28a1144f4c240d5bef27fb4e22827b1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[Epoch:    2] cost = 1.69872737\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/370 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18938a2a9916449ca8d7fbe15b44dd13"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[Epoch:    3] cost = 1.37745833\n"]}]},{"cell_type":"markdown","source":["###5. 챗봇 실행하기\n","이제 챗봇에게 답변을 얻기 위해서 어떤 전처리를 진행해야 할 지 차근차근 정리해봅시다. 우선 앞서 모\n","델 학습 전에 전처리를 진행했던 것처럼 사용자의 질문의 앞 뒤에 <usr<usr>>와 <sys<sys>>를 부착합니다. 그 후\n","해당 문장 앞에 시작 토큰에 해당하는 </s</s>>를 부착합니다."],"metadata":{"id":"NL-ECWXfDquS"}},{"cell_type":"code","source":["text = '오늘도 좋은 하루!'\n","sent = '<usr>' + text + '<sys>'\n","input_ids = [tokenizer.bos_token_id] + tokenizer.encode(sent)\n","input_ids = tf.convert_to_tensor([input_ids])\n","print('정 수 인 코 딩 후 :', input_ids)\n","print('정 수 인 코 딩 을 재 복 원 :', tokenizer.decode(input_ids[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHYaDPxSzfwN","executionInfo":{"status":"ok","timestamp":1752835625578,"user_tz":-540,"elapsed":17,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"6bcd6c22-2148-47b7-aeb6-f55f0d8f23bf"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["정 수 인 코 딩 후 : tf.Tensor([[    1     2 10070  7235 10586 12557   376     4]], shape=(1, 8), dtype=int32)\n","정 수 인 코 딩 을 재 복 원 : </s><usr> 오늘도 좋은 하루!<sys>\n"]}]},{"cell_type":"markdown","source":["현재의 입력은 챗봇이 학습할 당시의 형태 (</s</s>><usr<usr>>사용자의 질문<sys<sys>>챗봇의 답변</s</s>>) 에서\n","챗봇의 답변이 바로 시작되기 전까지의 형태입니다. 기존에 학습하였을 당시에는 <sys<sys>> 뒤에 챗봇의 답변이 있었으므로 현재의 입력을 KoGPT‐2 에 입력으로 사용하면 챗봇은 학습때 봤던 데이터의 형식 그대\n","로 챗봇의 답변을 작성하려는 관성을 갖고 있습니다. 현재의 입력을 모델의 입력으로 넣고 모델이 생성하\n","는 문장을 얻습니다."],"metadata":{"id":"xIQUIEBwEIGC"}},{"cell_type":"code","source":["output = model.generate(input_ids, max_length=50, early_stopping=True,\n","                        eos_token_id=tokenizer.eos_token_id)\n","decoded_sentence = tokenizer.decode(output[0].numpy().tolist())\n","print(decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LyNzm_G722W7","executionInfo":{"status":"ok","timestamp":1752835626620,"user_tz":-540,"elapsed":1037,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"4ec22457-206c-48ae-8e67-59ae7d60dfb4"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["</s><usr> 오늘도 좋은 하루!<sys> 좋은 하루네요.</s>\n"]}]},{"cell_type":"markdown","source":["KoGPT‐2 의 출력은 사용자가 넣어준 입력이었던 </s</s>><usr<usr>> 오늘도 좋은 하루!<sys<sys>>도 포함하여\n","반환되므로 챗봇의 답변만 확인하기 위해서는 <sys<sys>>를 기준으로 분할하여 뒷 부분만 가져와야 합니\n","다."],"metadata":{"id":"u4Ky6FouEdom"}},{"cell_type":"code","source":["print(decoded_sentence.split('<sys> ')[1].replace('</s>',''))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tybp3u53-sL5","executionInfo":{"status":"ok","timestamp":1752835626642,"user_tz":-540,"elapsed":18,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d06f272a-8b14-42d5-c87d-1da128e31b55"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["좋은 하루네요.\n"]}]},{"cell_type":"markdown","source":["만약 동일한 질문에도 KoGPT‐2 의 답변이 랜덤성을 갖기를 원한다면 do_sample=True, top_k=10 이라\n","는 파라미터를 통해서 후보가 되는 단어 10 개 중 랜덤으로 선택하여 생성하도록 유도할 수 있습니다."],"metadata":{"id":"xZ2WRS-LE5zX"}},{"cell_type":"code","source":["output = model.generate(input_ids, max_length=50, do_sample=True, top_k=10)\n","decoded_sentence = tokenizer.decode(output[0].numpy().tolist())\n","print(decoded_sentence.split('<sys> ')[1].replace('</s>', ''))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3v6MQ2fJigA","executionInfo":{"status":"ok","timestamp":1752835709673,"user_tz":-540,"elapsed":1328,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"928a66e1-a282-4972-a487-729172717334"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["오늘도 좋은 하루!\n"]}]},{"cell_type":"code","source":["def return_answer_by_chatbot(user_text):\n","  sent = '<usr>' + user_text + '<sys>'\n","  input_ids = [tokenizer.bos_token_id]+ tokenizer.encode(sent)\n","  input_ids = tf.convert_to_tensor([input_ids])\n","  output = model.generate(input_ids, max_length=50)\n","  sentence = tokenizer.decode(output[0].numpy().tolist())\n","  chatbot_response = sentence.split('<sys> ')[1].replace('</s>', '')\n","  return chatbot_response"],"metadata":{"id":"dunSD3ozE6jq","executionInfo":{"status":"ok","timestamp":1752835747402,"user_tz":-540,"elapsed":22,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["return_answer_by_chatbot('안녕! 반가워~')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"v4uZuDrDFxTB","executionInfo":{"status":"ok","timestamp":1752835749296,"user_tz":-540,"elapsed":1385,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"afbca0c1-fe5c-4835-a3cf-e96ca208b28f"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'반가워요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["return_answer_by_chatbot('너는 누구야?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"QbG2Z6VLF3YP","executionInfo":{"status":"ok","timestamp":1752835750578,"user_tz":-540,"elapsed":1268,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"7c7bd0e9-055b-4733-bd3c-4a305ebe6ce4"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'저는 짝사랑하는 사람입니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["return_answer_by_chatbot('너무 심심한데 나랑 놀자')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"gg4OUKGsF65S","executionInfo":{"status":"ok","timestamp":1752835752619,"user_tz":-540,"elapsed":2000,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8bbfc853-602f-4c60-d1c6-11e5238016ad"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'짝사랑은 영원할 거예요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["return_answer_by_chatbot('영화 해리포터 재밌어?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kTr4J9GzF_g4","executionInfo":{"status":"ok","timestamp":1752835754622,"user_tz":-540,"elapsed":1996,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"dcb5f424-7824-436f-b5a3-ea4e0f2b0937"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'영화 추천해달라고 해보세요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["return_answer_by_chatbot('너 딥러닝 잘해?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"U29Xpk98GDoX","executionInfo":{"status":"ok","timestamp":1752835756352,"user_tz":-540,"elapsed":1705,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"96de00a6-63f8-4135-9626-18f33c6d35b5"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'잘할 수 있을 거예요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["KoGPT‐2 는 상대적으로 모델의 크기가 작습니다. 챗봇의 성능을 올리고 싶다면 데이터의 양을 증량하거\n","나 T5‐Large 와 같은 보다 큰 모델을 시도하시기를 권합니다."],"metadata":{"id":"V_TMlFO1GIvU"}}]}