{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSndZBG4WiUI9eY9bbxw0l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 셀프 어텐션을 이용한 텍스트 분류(Multi-head Self Attention for Text Classification)\n","트랜스포머는 RNN 계열의 seq2seq를 대체하기 위해서 등장했습니다. 그리고 트랜스포머의 인코더는 RNN 인코더를, 트랜스포머의 디코더는 RNN 디코더를 대체할 수 있었습니다.\n","\n","트랜스포머의 인코더는 셀프 어텐션이라는 메커니즘을 통해 문장을 이해합니다. RNN과 그 동작 방식은 다르지만, RNN이 텍스트 분류나 개체명 인식과 같은 다양한 자연어 처리 태스크에 쓰일 수 있다면 트랜스포머의 인코더 또한 가능할 것입니다.\n","\n","실제로 트랜스포머의 인코더는 다양한 분야의 자연어 처리 태스크에서 사용될 수 있었고, 이 아이디어는 후에 배우게 될 BERT라는 모델로 이어지게 됩니다. 이번 챕터에서는 트랜스포머의 인코더를 사용하여 텍스트 분류를 수행합니다."],"metadata":{"id":"L__EjvcpVbqK"}},{"cell_type":"markdown","source":["### 1. 멀티 헤드 어텐션\n","우선 트랜스포머의 인코더의 첫번째 서브층인 멀티 헤드 어텐션층을 클래스로 구현합니다."],"metadata":{"id":"fmtt7Y7gV2CC"}},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"Yi2VGWlyWAoV","executionInfo":{"status":"ok","timestamp":1751789749017,"user_tz":-540,"elapsed":6483,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads = 8):\n","    super(MultiHeadAttention, self).__init__()\n","    self.embedding_dim = embedding_dim # d_model\n","    self.num_heads = num_heads\n","\n","    assert embedding_dim % self.num_heads == 0\n","\n","    self.projection_dim = embedding_dim // num_heads\n","    self.query_dense = tf.keras.layers.Dense(embedding_dim)\n","    self.key_dense = tf.keras.layers.Dense(embedding_dim)\n","    self.value_dense = tf.keras.layers.Dense(embedding_dim)\n","    self.dense = tf.keras.layers.Dense(embedding_dim)\n","\n","\n","  def scaled_dot_product_attention(self, query, key, value):\n","    matmul_qk = tf.matmul(query, key, transpose_b=True)\n","    depth = tf.cast(tf.shape(key)[-1], tf.float32) # tf.shape(key)[-1]: 키 벡터의 차원의 수\n","    logits = matmul_qk / tf.math.sqrt(depth)\n","    attention_weights = tf.nn.softmax(logits, axis = -1)\n","    output = tf.matmul(attention_weights, value)\n","    return output, attention_weights\n","\n","  def split_heads(self, x, batch_size):\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","    return tf.transpose(x, perm=[0,2,1,3])\n","\n","  def call(self, inputs):\n","    # x.shape = [batch_size, seq_len, embedding_dim]\n","    batch_size = tf.shape(inputs)[0]\n","\n","    # (batch_size, seq_len, embedding_dim)\n","    query = self.query_dense(inputs)\n","    key = self.key_dense(inputs)\n","    value = self.value_dense(inputs)\n","\n","    # (batch_size, num_heads, seq_len, projection_dim)\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n","    # (batch_size, seq_len, num_heads, projection_dim)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n","\n","    # (batch_size, seq_len, embedding_dim)\n","    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n","    outputs = self.dense(concat_attention)\n","    return outputs"],"metadata":{"id":"jjiXa3VbUHrL","executionInfo":{"status":"ok","timestamp":1751790788277,"user_tz":-540,"elapsed":50,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["###2. 인코더 설계하기\n","멀티 헤드 어텐션에 두번째 서브층인 포지션 와이즈 피드 포워드 신경망을 추가하여 인코더 클래스를 설계합니다.\n"],"metadata":{"id":"pXXq7truaJ2B"}},{"cell_type":"code","source":["class TransformerBlock(tf.keras.layers.Layer):\n","  def __init__(self, embedding_dim, num_heads, dff, rate = 0.1):\n","    super(TransformerBlock, self).__init__()\n","    self.att = MultiHeadAttention(embedding_dim, num_heads)\n","    self.ffn = tf.keras.Sequential(\n","        [tf.keras.layers.Dense(dff, activation=\"relu\"),\n","         tf.keras.layers.Dense(embedding_dim),]\n","    )\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, inputs, training):\n","    attn_output = self.att(inputs) # 첫번째 서브층 : 멀티 해드 어텐션\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(inputs + attn_output) # Add & Norm\n","    ffn_output = self.ffn(out1) # 두번째 서브층 : FFNN\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    return self.layernorm2(out1 + ffn_output) # Add & Norm"],"metadata":{"id":"Xxam-asEZ2q8","executionInfo":{"status":"ok","timestamp":1751790788712,"user_tz":-540,"elapsed":30,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### 3. 포지셔 임베딩\n","앞서 트랜스포머를 설명할 때는 포지셔널 인코딩을 사용하였지만, 이번에는 위치 정보 자체를 학습을 하도록 하는 포지션 임베딩이라는 방법을 사용합니다. 이는 뒤에서 배우게 될 BERT에서 사용하는 방법이기도 합니다. 포지션 임베딩은 임베딩 층(Embedding layer)를 사용하되, 위치 벡터를 학습하도록 하므로 임베딩 층의 첫번째 인자로 단어 집합의 크기가 아니라 문장의 최대 길이를 넣어줍니다."],"metadata":{"id":"dwomVQOlzlVk"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, max_len, vocab_size, embedding_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim)\n","\n","  def call(self, x):\n","    max_len = tf.shape(x)[-1]\n","    positions = tf.range(start=0, limit = max_len, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    return x + positions"],"metadata":{"id":"6lB0w0Skz2pe","executionInfo":{"status":"ok","timestamp":1751790789097,"user_tz":-540,"elapsed":72,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### 4. 데이터 로드 및 전처리"],"metadata":{"id":"uPLGZsWe1Onn"}},{"cell_type":"code","source":["vocab_size = 20000 # 빈도수 상위 2만개의 단어 사용\n","max_len = 200 # 문자의 최대 길이\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words = vocab_size)\n","print('훈련용 리뷰 개수 : {}'.format(len(X_train)))\n","print('테스트용 리뷰 개수 : {}'.format(len(X_test)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sly3qWPd05I-","executionInfo":{"status":"ok","timestamp":1751790793385,"user_tz":-540,"elapsed":3431,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"128d8fdc-d070-4540-c702-2b2d6994b378"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련용 리뷰 개수 : 25000\n","테스트용 리뷰 개수 : 25000\n"]}]},{"cell_type":"code","source":["X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n","X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"],"metadata":{"id":"lxp80J9F1Nb4","executionInfo":{"status":"ok","timestamp":1751790793913,"user_tz":-540,"elapsed":523,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### 5. 트랜스포머를 이용한 IMDB리뷰 분류"],"metadata":{"id":"aZ1bPiVX1iWZ"}},{"cell_type":"code","source":["embedding_dim =32 # 각 단어의 임베딩 벡터의 차워\n","num_heads = 2  # 어텐션 헤드의 수\n","dff = 32 # 포지션 와이즈 피드 포워드 신경망의 은닉층의 크기\n","\n","inputs = tf.keras.layers.Input(shape=(max_len,))\n","embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embedding_dim, num_heads, dff)\n","x= transformer_block(x, training=True) # Pass training=True during model construction for now\n","x= tf.keras.layers.GlobalAveragePooling1D()(x)\n","x = tf.keras.layers.Dropout(0.1)(x)\n","x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n","x = tf.keras.layers.Dropout(0.1)(x)\n","outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"a5mbWmKy1hQZ","executionInfo":{"status":"ok","timestamp":1751790794404,"user_tz":-540,"elapsed":486,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(X_train, y_train, batch_size=32, epochs=2, validation_data=(X_test, y_test))\n","\n","print(\"테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"syQqzKnc2oxD","executionInfo":{"status":"ok","timestamp":1751791122350,"user_tz":-540,"elapsed":208508,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2a3e3065-c51a-4552-c797-e4439b4743b1"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 101ms/step - accuracy: 0.7128 - loss: 0.5183 - val_accuracy: 0.8790 - val_loss: 0.2870\n","Epoch 2/2\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 102ms/step - accuracy: 0.9243 - loss: 0.2036 - val_accuracy: 0.8730 - val_loss: 0.3135\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.8724 - loss: 0.3143\n","테스트 정확도: 0.8730\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tsaEIkv63-1n"},"execution_count":null,"outputs":[]}]}