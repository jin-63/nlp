{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPK3bH9T9qY+n1AZxXRZMNj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a81621e7de874ccf8ffcf2604d4c3f43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a65c1178efbb412085c4d5d76720fcfd","IPY_MODEL_cd9df64dff5e4ca98246ecd2717ca19d","IPY_MODEL_6eb7e37bad89453a9868e49510fa58d8"],"layout":"IPY_MODEL_c13e54a911f2415f99cbab44ad229de9"}},"a65c1178efbb412085c4d5d76720fcfd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0c6228881754ebea231b627d9287d79","placeholder":"​","style":"IPY_MODEL_1bc4358413d14521a7247b702d400e6b","value":"tokenizer_config.json: 100%"}},"cd9df64dff5e4ca98246ecd2717ca19d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_29ed56906339446eaa805252a269a74f","max":289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e46087f4e85e4f0580fa0f8bab8c2eb6","value":289}},"6eb7e37bad89453a9868e49510fa58d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9533fbd6c6a461c8eeae16d1cbc933e","placeholder":"​","style":"IPY_MODEL_92e54194d7b0490b9481bad2926fa3ca","value":" 289/289 [00:00&lt;00:00, 31.9kB/s]"}},"c13e54a911f2415f99cbab44ad229de9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0c6228881754ebea231b627d9287d79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bc4358413d14521a7247b702d400e6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29ed56906339446eaa805252a269a74f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e46087f4e85e4f0580fa0f8bab8c2eb6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9533fbd6c6a461c8eeae16d1cbc933e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92e54194d7b0490b9481bad2926fa3ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e97dbd7ecd6e4e93a4b468b7c768d431":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a2c935819564d09be20a746d65e2b35","IPY_MODEL_5805efa475864ebca9e25e1d9154fca9","IPY_MODEL_737f551a9da34955ac7adfb058fa6d95"],"layout":"IPY_MODEL_fb6c99a2525c4473bdf5ea0bd8b02f94"}},"5a2c935819564d09be20a746d65e2b35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_744c5719e8ad45b5bb9c28aa98794a58","placeholder":"​","style":"IPY_MODEL_c61f5d96689f4118bdd1e5947ee6dc3e","value":"vocab.txt: "}},"5805efa475864ebca9e25e1d9154fca9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fff2168df87a4bf69184b07c73af3619","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c1c1670ed81411094e0b159f88a0cc9","value":1}},"737f551a9da34955ac7adfb058fa6d95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8c5ccd22b044b45a95de8d3c43cb5ba","placeholder":"​","style":"IPY_MODEL_25604141b54e45139c87603eb5a3faec","value":" 248k/? [00:00&lt;00:00, 14.3MB/s]"}},"fb6c99a2525c4473bdf5ea0bd8b02f94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744c5719e8ad45b5bb9c28aa98794a58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c61f5d96689f4118bdd1e5947ee6dc3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fff2168df87a4bf69184b07c73af3619":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9c1c1670ed81411094e0b159f88a0cc9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8c5ccd22b044b45a95de8d3c43cb5ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25604141b54e45139c87603eb5a3faec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49e5c5ee00b44626a6b24ab40b9bdf39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49849b036b354e68b8d5d28ed5d66b78","IPY_MODEL_393f1dfbab5f42499bc145118b80f960","IPY_MODEL_d798cd1d13174b42840fb7a1e28c7134"],"layout":"IPY_MODEL_e15956219c0f4ac9b1e79e65360b30b5"}},"49849b036b354e68b8d5d28ed5d66b78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc5e5fca83dc4aaf88df797ff74d518e","placeholder":"​","style":"IPY_MODEL_1d80f06592204cd3a2e6d215fc0fe961","value":"special_tokens_map.json: 100%"}},"393f1dfbab5f42499bc145118b80f960":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5f2ae703a814b029ec91d80772d2e3b","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_644d779de41a489b86ec9e74aae85b9f","value":125}},"d798cd1d13174b42840fb7a1e28c7134":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d0110f1ec1c4fcbb9ba90948bee8521","placeholder":"​","style":"IPY_MODEL_2e5d8c61bc0c41999a2d6c552af5bd26","value":" 125/125 [00:00&lt;00:00, 12.4kB/s]"}},"e15956219c0f4ac9b1e79e65360b30b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc5e5fca83dc4aaf88df797ff74d518e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d80f06592204cd3a2e6d215fc0fe961":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5f2ae703a814b029ec91d80772d2e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"644d779de41a489b86ec9e74aae85b9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d0110f1ec1c4fcbb9ba90948bee8521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e5d8c61bc0c41999a2d6c552af5bd26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3dd831e55bb7416699f198680a81f80b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e7190952c2b4d53bc89f28abe5b1a69","IPY_MODEL_d92d97b257e84b70ab7eaa45bada70be","IPY_MODEL_dc0b02778997485495f00edf82489956"],"layout":"IPY_MODEL_91c16dabed274af8af462c219b35c488"}},"5e7190952c2b4d53bc89f28abe5b1a69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d7d27385e764d8e8a619ccfdc875f25","placeholder":"​","style":"IPY_MODEL_fbf6228ca2b148d4aa96834817077d74","value":"tokenizer.json: "}},"d92d97b257e84b70ab7eaa45bada70be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0766651260f54bfcaa84185812530dae","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_803756a6316e487a91731a15ee122ba7","value":1}},"dc0b02778997485495f00edf82489956":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac6b13753e704ef8aba8fdcb4f53d7f9","placeholder":"​","style":"IPY_MODEL_cb7211cff3d245aa8a4d4ea984025d79","value":" 495k/? [00:00&lt;00:00, 32.6MB/s]"}},"91c16dabed274af8af462c219b35c488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d7d27385e764d8e8a619ccfdc875f25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbf6228ca2b148d4aa96834817077d74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0766651260f54bfcaa84185812530dae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"803756a6316e487a91731a15ee122ba7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac6b13753e704ef8aba8fdcb4f53d7f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb7211cff3d245aa8a4d4ea984025d79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30d847ce478c49f6a9bc8edf3dd1a761":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02ef928c4d3a4683aad593bc0a97a439","IPY_MODEL_add4d195592a4ff588f2b744ccc4049e","IPY_MODEL_e082cec76eef4aadabaf84515e1cb7b3"],"layout":"IPY_MODEL_0e32208266d2407b9d001de8c51d5a0c"}},"02ef928c4d3a4683aad593bc0a97a439":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d16e68b9928f47e3866ae0ce1fa68a50","placeholder":"​","style":"IPY_MODEL_2df364cd747449dba44d7940c426e2c4","value":"config.json: 100%"}},"add4d195592a4ff588f2b744ccc4049e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b10d648bee142fa8ccb8fb59bf90f74","max":425,"min":0,"orientation":"horizontal","style":"IPY_MODEL_54feb249eb9c4db1bfab7eb36113c51d","value":425}},"e082cec76eef4aadabaf84515e1cb7b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2da45cf705c54c8db9edda008394e71d","placeholder":"​","style":"IPY_MODEL_c179330c23504a23862f78c8e6b539d7","value":" 425/425 [00:00&lt;00:00, 54.4kB/s]"}},"0e32208266d2407b9d001de8c51d5a0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d16e68b9928f47e3866ae0ce1fa68a50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2df364cd747449dba44d7940c426e2c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b10d648bee142fa8ccb8fb59bf90f74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54feb249eb9c4db1bfab7eb36113c51d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2da45cf705c54c8db9edda008394e71d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c179330c23504a23862f78c8e6b539d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b803784b83be49298615e6c12252565d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fca2fa52dd5a47ecb88a0d0a77dab168","IPY_MODEL_d6739bb6dab14d86bfb637d8c543cad6","IPY_MODEL_1f74271c4ccc47ad9944abad3cd32eb5"],"layout":"IPY_MODEL_a4812ddd81b9405da75340d1986d03d8"}},"fca2fa52dd5a47ecb88a0d0a77dab168":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1da29bc415a4265bc2915061a525a02","placeholder":"​","style":"IPY_MODEL_108509957b484abc9afe5bf6c951ca1d","value":"pytorch_model.bin: 100%"}},"d6739bb6dab14d86bfb637d8c543cad6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24e5f9e295054c529bb41bd09f59d7dd","max":445025130,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f436ec7c8ec143a795f00f3c7cb5b6e5","value":445025130}},"1f74271c4ccc47ad9944abad3cd32eb5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d514afe62505429986b8eb44fcc4e3c1","placeholder":"​","style":"IPY_MODEL_cd9773d5ba8142748bca7159efc69b59","value":" 445M/445M [00:02&lt;00:00, 258MB/s]"}},"a4812ddd81b9405da75340d1986d03d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1da29bc415a4265bc2915061a525a02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"108509957b484abc9afe5bf6c951ca1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24e5f9e295054c529bb41bd09f59d7dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f436ec7c8ec143a795f00f3c7cb5b6e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d514afe62505429986b8eb44fcc4e3c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9773d5ba8142748bca7159efc69b59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["##KLUE-BERT를 이용한 개체명 인식\n","한국어BERT를 이용하여 개체명 인식 문제를 풀어보겠습니다."],"metadata":{"id":"pp3NJUv5iKjZ"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"byE9Yo6GiG4J","executionInfo":{"status":"ok","timestamp":1752292293065,"user_tz":-540,"elapsed":5967,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a21da96-8f8c-4f57-9bfe-c09d5a37984f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5733ba9f","executionInfo":{"status":"ok","timestamp":1752292300544,"user_tz":-540,"elapsed":7477,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d53e4f6d-3583-4269-8337-0e757ad29af1"},"source":["!pip install seqeval"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=e5f0fd55401e93c9949df38a7c8cdf11ec518312a7ead1c81597f24140a25a9a\n","  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"markdown","source":["###1. 데이터 로드"],"metadata":{"id":"0pjeG_aHkIq4"}},{"cell_type":"code","metadata":{"id":"fe39d866","executionInfo":{"status":"ok","timestamp":1752292316568,"user_tz":-540,"elapsed":16016,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","from transformers import shape_list, BertTokenizer, TFBertModel\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from seqeval.metrics import f1_score, classification_report\n","import tensorflow as tf\n","import urllib.request"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터와 테스트 데이터 그리고 레이블 정보를 저자의 깃허브에서 다운로드 합니다."],"metadata":{"id":"OqtFmqVokL1g"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_train_data.csv\", filename=\"ner_train_data.csv\")\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_test_data.csv\", filename=\"ner_test_data.csv\")\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/dataset/ner_label.txt\", filename=\"ner_label.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VesllN9jjKzg","executionInfo":{"status":"ok","timestamp":1752292318747,"user_tz":-540,"elapsed":2177,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"0b715df4-8493-4cbd-9995-68492c98b608"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ner_label.txt', <http.client.HTTPMessage at 0x7b9c78776cd0>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["train_ner_df = pd.read_csv(\"ner_train_data.csv\")\n","test_ner_df = pd.read_csv(\"ner_test_data.csv\")"],"metadata":{"id":"2ei-o4K3jxII","executionInfo":{"status":"ok","timestamp":1752292319087,"user_tz":-540,"elapsed":337,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_ner_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"id":"Bgzn4Xf-j9qy","executionInfo":{"status":"ok","timestamp":1752292319318,"user_tz":-540,"elapsed":230,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"0df8fda2-a7f8-4603-e024-5251632f96b3"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            Sentence  \\\n","0                      정은 씨를 힘들게 한 가스나그, 가만둘 수 없겠죠 .   \n","1                          ▶ 쿠마리 한동수가 말하는 '가넷 & 에르덴'   \n","2                    슈나이더의 프레젠테이션은 말 청중을 위한 특별한 쇼다 .   \n","3  지구 최대 연료탱크 수검 회사 구글이 연내 22명 안팎의 인력을 갖춘 연구개발(R&...   \n","4  5. <10:00:TI_HOUR> 도이치증권대 <0:1:QT_SPORTS> 연예오락...   \n","\n","                                                 Tag  \n","0                              PER-B O O O O O O O O  \n","1                      O PER-B PER-I O PER-B O PER-B  \n","2                            PER-B O O CVL-B O O O O  \n","3  O O TRM-B O O ORG-B DAT-B NUM-B O O O ORG-B LO...  \n","4                              NUM-B O ORG-B O ORG-B  "],"text/html":["\n","  <div id=\"df-ef8122ae-69da-4774-9777-79465bdb44e7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>정은 씨를 힘들게 한 가스나그, 가만둘 수 없겠죠 .</td>\n","      <td>PER-B O O O O O O O O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>▶ 쿠마리 한동수가 말하는 '가넷 &amp; 에르덴'</td>\n","      <td>O PER-B PER-I O PER-B O PER-B</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>슈나이더의 프레젠테이션은 말 청중을 위한 특별한 쇼다 .</td>\n","      <td>PER-B O O CVL-B O O O O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>지구 최대 연료탱크 수검 회사 구글이 연내 22명 안팎의 인력을 갖춘 연구개발(R&amp;...</td>\n","      <td>O O TRM-B O O ORG-B DAT-B NUM-B O O O ORG-B LO...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5. &lt;10:00:TI_HOUR&gt; 도이치증권대 &lt;0:1:QT_SPORTS&gt; 연예오락...</td>\n","      <td>NUM-B O ORG-B O ORG-B</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef8122ae-69da-4774-9777-79465bdb44e7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ef8122ae-69da-4774-9777-79465bdb44e7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ef8122ae-69da-4774-9777-79465bdb44e7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-4d1ca398-a4f6-47cd-8b15-a09a07034e61\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d1ca398-a4f6-47cd-8b15-a09a07034e61')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-4d1ca398-a4f6-47cd-8b15-a09a07034e61 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_ner_df","summary":"{\n  \"name\": \"train_ner_df\",\n  \"rows\": 81000,\n  \"fields\": [\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 79859,\n        \"samples\": [\n          \"\\uc774 \\uc911 \\uc77c\\uccb4\\uc778 \\uc0cc\\ud0c0\\ubc14\\ubc84\\ub77c\\uc2dc\\ub294 22\\uc5b5 1535\\ub9cc\\ub144 \\uc804 \\uc544\\ud504\\ub9ac\\uce74\\ub300\\ub959\\uc5d0\\uc11c \\ub5a8\\uc5b4\\uc838 \\ub098\\uc654\\uace0 \\uc6b0\\ub9ac\\uc758 \\uccab\\ubc88\\uc9f8\\uc8fc\\uc778\\uacf5\\uc774 \\uc0b4\\uace0 \\uc788\\ub294 \\uacf3\\uc785\\ub2c8\\ub2e4 .\",\n          \"-\\uae30\\ub825\\uc774 \\ub2ec\\ub9ac\\ub294 \\ub370\\ub3c4 \\uc80a\\uc740 \\uc7a5\\uc2dc \\uaca9\\ud55c \\ubab8\\uc2f8\\uc6c0\\uc744 \\ubc8c\\uc778 \\uadf8 \\uc751\\ud07c\\ud55c \\uc18d\\uc148\\uc740 \\ubb34\\uc5c7\\uc785\\ub2c8\\uae4c, \\ube60\\uc2a4\\ub5c4 .\",\n          \"\\uc5f0\\uccb4\\ub3d9\\ubb3c\\uc5d0 \\ub300\\ud574 \\uc5f0\\uad6c\\ud558\\ub294 \\uc2e4\\ubb34\\uc790\\ub4e4\\uc774 \\uc788\\uac70\\ub4e0\\uc694 .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tag\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47847,\n        \"samples\": [\n          \"LOC-B O TRM-B TRM-B TRM-B NUM-B O NUM-B O NUM-B O DAT-B TRM-B O O O O O O O O\",\n          \"ORG-B O O NUM-B O EVT-B O NUM-B O O O\",\n          \"PER-B ORG-B ORG-B CVL-B FLD-B ORG-B FLD-B CVL-B NUM-B NUM-I NUM-I NUM-I NUM-I CVL-B ORG-B O LOC-B LOC-B CVL-B ORG-B CVL-B CVL-B CVL-I\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["print(\"학습 데이터 샘플 개수: \", len(train_ner_df))\n","print(\"테스트 데이터 샘플 개수: \", len(test_ner_df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WG9dIFakASS","executionInfo":{"status":"ok","timestamp":1752292319331,"user_tz":-540,"elapsed":12,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"a5d6996f-e7d1-47f0-8bef-bda108499ded"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["학습 데이터 샘플 개수:  81000\n","테스트 데이터 샘플 개수:  9000\n"]}]},{"cell_type":"markdown","source":["훈련 데이터와 테스트 데이터에서 문장과 레이블을 각각 분리하여 저장합니다.\n"],"metadata":{"id":"rJyxkzddki_F"}},{"cell_type":"code","source":["train_data_sentence = [sent.split() for sent in train_ner_df['Sentence'].values]\n","test_data_sentence = [sent.split() for sent in test_ner_df['Sentence'].values]\n","train_data_label = [tag.split() for tag in train_ner_df['Tag'].values]\n","test_data_label = [tag.split() for tag in test_ner_df['Tag'].values]"],"metadata":{"id":"GEy1RzrjkgLW","executionInfo":{"status":"ok","timestamp":1752292319875,"user_tz":-540,"elapsed":543,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(train_data_sentence[2])\n","print(train_data_label[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hW9YZVC2lMHu","executionInfo":{"status":"ok","timestamp":1752292319880,"user_tz":-540,"elapsed":10,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"d45be3e6-f956-49ab-e2e1-aaa1923ba31c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['슈나이더의', '프레젠테이션은', '말', '청중을', '위한', '특별한', '쇼다', '.']\n","['PER-B', 'O', 'O', 'CVL-B', 'O', 'O', 'O', 'O']\n"]}]},{"cell_type":"markdown","source":["이 데이터는 형태소 단위가 아니라 어절 단위(띄어쓰기 단위)로 개체명 인식 레이블이 태깅되었다는 특징이 있습니다. ner_label.txt파일을 읽어서 개체명 태깅 정보의 종류를 확인해봅시다."],"metadata":{"id":"FYHU3h6blbnm"}},{"cell_type":"code","source":["labels = [label.strip() for label in open('ner_label.txt', 'r', encoding='utf-8')]\n","print('개 체 명 태 깅 정 보 :', labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPO5NKohlT2B","executionInfo":{"status":"ok","timestamp":1752292319895,"user_tz":-540,"elapsed":14,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"5aecac3e-7080-4eb2-f7b7-99818c8fe3f9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["개 체 명 태 깅 정 보 : ['O', 'PER-B', 'PER-I', 'FLD-B', 'FLD-I', 'AFW-B', 'AFW-I', 'ORG-B', 'ORG-I', 'LOC-B', 'LOC-I', 'CVL-B', 'CVL-I', 'DAT-B', 'DAT-I', 'TIM-B', 'TIM-I', 'NUM-B', 'NUM-I', 'EVT-B', 'EVT-I', 'ANM-B', 'ANM-I', 'PLT-B', 'PLT-I', 'MAT-B', 'MAT-I', 'TRM-B', 'TRM-I']\n"]}]},{"cell_type":"markdown","source":["개체명 태깅 정보를 저장한 labels 리스트로부터 각 개체명 태깅 정보와 정수를 맵핑하는 딕셔너리를 만\n","듭니다. tag_to_index 는 개체명 태깅 정보를 key, 정수를 value 로 하는 딕셔너리이며 index_to_tag 는\n","그 반대 형태를 가지는 딕셔너리입니다."],"metadata":{"id":"RPaygeMHmRM7"}},{"cell_type":"code","source":["tag_to_index = {tag: index for index, tag in enumerate(labels)}\n","index_to_tag = {index: tag for index, tag in enumerate(labels)}"],"metadata":{"id":"SYvq370Gl6GB","executionInfo":{"status":"ok","timestamp":1752292319897,"user_tz":-540,"elapsed":1,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(tag_to_index)\n","print(index_to_tag)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Brm9TNkumuNw","executionInfo":{"status":"ok","timestamp":1752292319904,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"9ebd0ff7-ecb8-481b-9212-36da78123ad0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["{'O': 0, 'PER-B': 1, 'PER-I': 2, 'FLD-B': 3, 'FLD-I': 4, 'AFW-B': 5, 'AFW-I': 6, 'ORG-B': 7, 'ORG-I': 8, 'LOC-B': 9, 'LOC-I': 10, 'CVL-B': 11, 'CVL-I': 12, 'DAT-B': 13, 'DAT-I': 14, 'TIM-B': 15, 'TIM-I': 16, 'NUM-B': 17, 'NUM-I': 18, 'EVT-B': 19, 'EVT-I': 20, 'ANM-B': 21, 'ANM-I': 22, 'PLT-B': 23, 'PLT-I': 24, 'MAT-B': 25, 'MAT-I': 26, 'TRM-B': 27, 'TRM-I': 28}\n","{0: 'O', 1: 'PER-B', 2: 'PER-I', 3: 'FLD-B', 4: 'FLD-I', 5: 'AFW-B', 6: 'AFW-I', 7: 'ORG-B', 8: 'ORG-I', 9: 'LOC-B', 10: 'LOC-I', 11: 'CVL-B', 12: 'CVL-I', 13: 'DAT-B', 14: 'DAT-I', 15: 'TIM-B', 16: 'TIM-I', 17: 'NUM-B', 18: 'NUM-I', 19: 'EVT-B', 20: 'EVT-I', 21: 'ANM-B', 22: 'ANM-I', 23: 'PLT-B', 24: 'PLT-I', 25: 'MAT-B', 26: 'MAT-I', 27: 'TRM-B', 28: 'TRM-I'}\n"]}]},{"cell_type":"code","source":["tag_size = len(tag_to_index)\n","index_size = len(index_to_tag)\n","\n","print('개체명 태깅 정보의 개수:', tag_size)\n","print('정수 인덱스의 개수:', index_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGD_jk-fmyiP","executionInfo":{"status":"ok","timestamp":1752292319926,"user_tz":-540,"elapsed":10,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"00579b71-a4c4-4969-b3e4-0b32c748a62a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["개체명 태깅 정보의 개수: 29\n","정수 인덱스의 개수: 29\n"]}]},{"cell_type":"markdown","source":["###2. 전처리 예시"],"metadata":{"id":"poijO1KYgfrs"}},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"],"metadata":{"id":"VIieI3ganKqM","colab":{"base_uri":"https://localhost:8080/","height":308,"referenced_widgets":["a81621e7de874ccf8ffcf2604d4c3f43","a65c1178efbb412085c4d5d76720fcfd","cd9df64dff5e4ca98246ecd2717ca19d","6eb7e37bad89453a9868e49510fa58d8","c13e54a911f2415f99cbab44ad229de9","d0c6228881754ebea231b627d9287d79","1bc4358413d14521a7247b702d400e6b","29ed56906339446eaa805252a269a74f","e46087f4e85e4f0580fa0f8bab8c2eb6","a9533fbd6c6a461c8eeae16d1cbc933e","92e54194d7b0490b9481bad2926fa3ca","e97dbd7ecd6e4e93a4b468b7c768d431","5a2c935819564d09be20a746d65e2b35","5805efa475864ebca9e25e1d9154fca9","737f551a9da34955ac7adfb058fa6d95","fb6c99a2525c4473bdf5ea0bd8b02f94","744c5719e8ad45b5bb9c28aa98794a58","c61f5d96689f4118bdd1e5947ee6dc3e","fff2168df87a4bf69184b07c73af3619","9c1c1670ed81411094e0b159f88a0cc9","e8c5ccd22b044b45a95de8d3c43cb5ba","25604141b54e45139c87603eb5a3faec","49e5c5ee00b44626a6b24ab40b9bdf39","49849b036b354e68b8d5d28ed5d66b78","393f1dfbab5f42499bc145118b80f960","d798cd1d13174b42840fb7a1e28c7134","e15956219c0f4ac9b1e79e65360b30b5","dc5e5fca83dc4aaf88df797ff74d518e","1d80f06592204cd3a2e6d215fc0fe961","a5f2ae703a814b029ec91d80772d2e3b","644d779de41a489b86ec9e74aae85b9f","4d0110f1ec1c4fcbb9ba90948bee8521","2e5d8c61bc0c41999a2d6c552af5bd26","3dd831e55bb7416699f198680a81f80b","5e7190952c2b4d53bc89f28abe5b1a69","d92d97b257e84b70ab7eaa45bada70be","dc0b02778997485495f00edf82489956","91c16dabed274af8af462c219b35c488","3d7d27385e764d8e8a619ccfdc875f25","fbf6228ca2b148d4aa96834817077d74","0766651260f54bfcaa84185812530dae","803756a6316e487a91731a15ee122ba7","ac6b13753e704ef8aba8fdcb4f53d7f9","cb7211cff3d245aa8a4d4ea984025d79","30d847ce478c49f6a9bc8edf3dd1a761","02ef928c4d3a4683aad593bc0a97a439","add4d195592a4ff588f2b744ccc4049e","e082cec76eef4aadabaf84515e1cb7b3","0e32208266d2407b9d001de8c51d5a0c","d16e68b9928f47e3866ae0ce1fa68a50","2df364cd747449dba44d7940c426e2c4","3b10d648bee142fa8ccb8fb59bf90f74","54feb249eb9c4db1bfab7eb36113c51d","2da45cf705c54c8db9edda008394e71d","c179330c23504a23862f78c8e6b539d7"]},"executionInfo":{"status":"ok","timestamp":1752292322450,"user_tz":-540,"elapsed":2524,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"1a2ee835-1092-413b-83e8-dafa07f34802"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a81621e7de874ccf8ffcf2604d4c3f43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e97dbd7ecd6e4e93a4b468b7c768d431"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49e5c5ee00b44626a6b24ab40b9bdf39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dd831e55bb7416699f198680a81f80b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d847ce478c49f6a9bc8edf3dd1a761"}},"metadata":{}}]},{"cell_type":"markdown","source":["전처리 과정을 이해해봅시다. 임의로 훈련 데이터 중 1번 인덱스의 문장과 레이블을 선택하고 이에 대해서 전처리를 진행해보겠습니다. 해당 문장과, 레이블, 그리고 레이블에 대해서 정수 인코딩을 진행한 결과는 다음과 같습니다."],"metadata":{"id":"6dspY6aogkU3"}},{"cell_type":"code","source":["sent = train_data_sentence[1]\n","label = train_data_label[1]\n","print('문장 :', sent)\n","print('레이블 :',label)\n","print('레이블의 정수 인코딩 :',[tag_to_index[idx] for idx in label])\n","print('문장의 길이 :', len(sent))\n","print('레이블의 길이 :', len(label))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjY6Q56Ug4z9","executionInfo":{"status":"ok","timestamp":1752292322467,"user_tz":-540,"elapsed":13,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"37b3b918-770a-4f7d-914b-5ca299740a96"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["문장 : ['▶', '쿠마리', '한동수가', '말하는', \"'가넷\", '&', \"에르덴'\"]\n","레이블 : ['O', 'PER-B', 'PER-I', 'O', 'PER-B', 'O', 'PER-B']\n","레이블의 정수 인코딩 : [0, 1, 2, 0, 1, 0, 1]\n","문장의 길이 : 7\n","레이블의 길이 : 7\n"]}]},{"cell_type":"markdown","source":["위 문장과 레이블을 예시로 들어 BERT토크나이저를 적용하는 과정을 이해해봅시다. 이미 위와 같이 문장에 단어 토큰화가 진행되어져 있는 상황에서 BERT토크나이저를 적용하기 위해서는 각 단어에 대해서 BERT토크나이저를 사용하여 단어를 서브워드로 분리합니다. 위 문장에 BERT토크나이저를 적용한 결과는 다음과 같습니다."],"metadata":{"id":"BMghpTL3g4G6"}},{"cell_type":"code","source":["tokens = []\n","\n","for one_word in sent:\n","  # 각 단 어 에 대 해 서 서 브 워 드 로 분 리.\n","  # ex) one_word = '쿠 마 리' ===> subword_tokens = ['쿠', '##마 리']\n","  # ex) one_word = '한 동 수 가' ===> subword_tokens = ['한 동', '##수', '##가']\n","  subword_tokens = tokenizer.tokenize(one_word)\n","  tokens.extend(subword_tokens)\n","\n","print('BERT 토 크 나 이 저 적 용 후 문 장 :',tokens)\n","print('레 이 블 :', label)\n","print('레 이 블 의 정 수 인 코 딩 :',[tag_to_index[idx] for idx in label])\n","print('문 장 의 길 이 :', len(tokens))\n","print('레 이 블 의 길 이 :', len(label))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2gbOCsDh--V","executionInfo":{"status":"ok","timestamp":1752292322484,"user_tz":-540,"elapsed":17,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"b60ec18c-8f49-4156-d458-5e67a1e024b7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT 토 크 나 이 저 적 용 후 문 장 : ['▶', '쿠', '##마리', '한동', '##수', '##가', '말', '##하', '##는', \"'\", '가', '##넷', '&', '에르', '##덴', \"'\"]\n","레 이 블 : ['O', 'PER-B', 'PER-I', 'O', 'PER-B', 'O', 'PER-B']\n","레 이 블 의 정 수 인 코 딩 : [0, 1, 2, 0, 1, 0, 1]\n","문 장 의 길 이 : 16\n","레 이 블 의 길 이 : 7\n"]}]},{"cell_type":"markdown","source":["‘쿠마리’ 가’ 쿠’ 와’## 마리’ 로 분리되는 등 단어들이 서브워드로 분리되었습니다. 이제 문장의 길이가 길어지면서 레이블의 길이와 달라지게 됩니다. 레이블의 길이도 문장의 길이와 일치하도록 추가적인 처리를 진행해야 합니다. ‘쿠마리’ 의 레이블은’PER‐B’ 였습니다. 그렇다면’ 쿠’ 와’## 마리’ 의 레이블은 어떻게 해야할까요?\n","이 경우 첫번째 서브워드에 대해서만 기존의 레이블을 부여하고 뒤에 생겨난 서브워드들에 대해서는 레이블을 주지 않는 방법이 있습니다. 가령, 단어 ‘쿠마리’ 가’PER‐B’ 의 레이블을 가지고 있었다면, 분리된 [’쿠’, ’## 마리’] 에 대해서’ 쿠’ 에는’PER‐B’ 를 부여하고 그 뒤의 서브워드인’## 마리’ 에 대해서는 레이블을\n","주지 않는 것입니다. 그 방법으로는 레이블을 정수 인코딩하는 과정에서 첫번째 서브워드가 아닌 경우에\n","는 ‐100 을 부여하는 방식을 사용합니다."],"metadata":{"id":"ui93JRLwiza9"}},{"cell_type":"code","source":["tokens = []\n","labels_ids = []\n","\n","for one_word, label_token in zip(train_data_sentence[1], train_data_label[1]):\n","  subword_tokens = tokenizer.tokenize(one_word)\n","  tokens.extend(subword_tokens)\n","  labels_ids.extend([tag_to_index[label_token]]+ [-100] * (len(subword_tokens) -1))\n","\n","print('토 큰 화 후 문 장 :',tokens)\n","print('레 이 블 :', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx in labels_ids])\n","print('레 이 블 의 정 수 인 코 딩 :', labels_ids)\n","print('문 장 의 길 이 :', len(tokens))\n","print('레 이 블 의 길 이 :', len(labels_ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFe3oeVujXdy","executionInfo":{"status":"ok","timestamp":1752292322495,"user_tz":-540,"elapsed":3,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"79876b70-b5e1-47b6-b0d8-bcfc2ace70fe"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["토 큰 화 후 문 장 : ['▶', '쿠', '##마리', '한동', '##수', '##가', '말', '##하', '##는', \"'\", '가', '##넷', '&', '에르', '##덴', \"'\"]\n","레 이 블 : ['O', 'PER-B', '[PAD]', 'PER-I', '[PAD]', '[PAD]', 'O', '[PAD]', '[PAD]', 'PER-B', '[PAD]', '[PAD]', 'O', 'PER-B', '[PAD]', '[PAD]']\n","레 이 블 의 정 수 인 코 딩 : [0, 1, -100, 2, -100, -100, 0, -100, -100, 1, -100, -100, 0, 1, -100, -100]\n","문 장 의 길 이 : 16\n","레 이 블 의 길 이 : 16\n"]}]},{"cell_type":"markdown","source":["‘레이블의 정수 인코딩’ 결과를 보면 ‘쿠’ 에 대해서는 PER‐B 에 해당하는 정수 1 을 부여하였지만, # 마리\n","에 대해서는 ‐100 을 부여하였습니다. 마찬가지로 기존에는’ 한동수가’ 에 PER‐I 가 부여되어 있었으나, 서\n","브워드 토큰화 과정에서 [’ 한동’, ’## 수’, ’## 가’] 로 분리되었고, 첫번째 서브워드인’ 한동’ 에는 PER‐I 에\n","해당하는 정수인 2 를 부여하지만 그 뒤의 서브워드들인’## 수’, ’## 가’ 에 대해서는 ‐100 을 부여합니다. ’\n","말하는’ 은 레이블이’O’ 였으나 [‘말’, ‘##’ 하,’## 는’] 으로 분리되었고’ 말’ 에 대해서는’O’ 에 해당하는 정수\n","인 0 을 부여하지만, 그 뒤의 서브워드 들에 대해서는 ‐100 을 부여합니다.\n","‐100 을 부여하고나서는 실질적으로 학습할 때는 해당 레이블에 대해서는 학습을 무시하는 정책을 취할\n","예정입니다. 정확히는 ‐100 을 레이블에서는 패딩 토큰 [PAD] 로 사용합니다. 레이블에 대해서는 문장의\n","길이를 맞추는 패딩을 진행할 때도 ‐100 을 사용하겠습니다. 그리고 이후 손실 함수에서 ‐100 을 무시하도\n","록 추가 처리를 진행하겠습니다.\n","레이블에 대한 전처리를 이해하였다면, 이제 문장과 레이블에 대한 정수 인코딩. 그리고 세그먼트 인코딩\n","과 어텐션 마스크까지 생성하는 함수 convert_examples_to_features 를 구현합니다."],"metadata":{"id":"XJgVi_L-pIbG"}},{"cell_type":"code","source":["def convert_examples_to_features(examples, labels, max_seq_len, tokenizer,\n","                                 pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n","\n","  cls_token = tokenizer.cls_token\n","  sep_token = tokenizer.sep_token\n","  pad_token_id = tokenizer.pad_token_id\n","\n","  input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n","\n","  for example, label in tqdm(zip(examples, labels), total=len(examples)):\n","    tokens = []\n","    labels_ids = []\n","    for one_word, label_token in zip(example, label):\n","      # 하나의 단어에 대해서 서브워드로 토큰화\n","      subword_tokens = tokenizer.tokenize(one_word)\n","      tokens.extend(subword_tokens)\n","      # 서브워드 중 첫번째 서브워드만 개체명 레이블을 부여 하고 그 외에는 -100으로 채운다.\n","\n","      labels_ids.extend([tag_to_index[label_token]]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n","\n","    # [CLS]와 [SEP]를 후에 추가 할 것을 고려 하여 최대 길이를 초과 하는 샘플 의 경우 max_seq_len - 2의 길이로 변환.\n","    # ex) max_seq_len = 64라면 길이가 62 보다 긴 샘플은 뒷 부분을 자르고 길이 62로 변환.\n","    special_tokens_count = 2\n","    if len(tokens) > max_seq_len - special_tokens_count:\n","      tokens = tokens[:(max_seq_len - special_tokens_count)]\n","      labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n","\n","    # [SEP]를 추가 하는 코드\n","    # 1. 토큰화 결과의 맨 뒷 부분에 [SEP]토큰 추가\n","    # 2. 레이블에도 맨 뒷 부분에 -100 추가.\n","    tokens += [sep_token]\n","    labels_ids += [pad_token_id_for_label]\n","\n","    # [CLS]를 추가 하는 코드\n","    # 1. 토큰화 결과의 앞 부분에 [CLS] 토큰 추가\n","    # 2. 레이블의 맨 앞 부분에도 -100 추가.\n","    tokens = [cls_token] + tokens\n","    labels_ids = [pad_token_id_for_label] + labels_ids\n","\n","    # 정수 인코딩\n","    input_id = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # 어텐션 마스크 생성\n","    attention_mask = [1] * len(input_id)\n","\n","    # 정수 인코딩에 추가 할 패딩 길이 연산\n","    padding_count = max_seq_len - len(input_id)\n","\n","    # 정수 인코딩, 어텐션 마스크에 패딩 추가\n","    input_id = input_id + ([pad_token_id] * padding_count)\n","    attention_mask = attention_mask + ([0] * padding_count)\n","\n","    # 세 그 먼 트 인 코 딩.\n","    token_type_id = [pad_token_id_for_segment] * max_seq_len\n","\n","    # 레이블 패딩. (단, 이 경우는 패딩 토큰의 ID가 -100)\n","    label = labels_ids + ([pad_token_id_for_label] * padding_count)\n","    assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n","    assert len(attention_mask) == max_seq_len, \"Error with attention masklength {} vs {}\".format(len(attention_mask), max_seq_len)\n","    assert len(token_type_id) == max_seq_len, \"Error with token type length {}vs {}\".format(len(token_type_id), max_seq_len)\n","    assert len(label) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label), max_seq_len)\n","\n","    input_ids.append(input_id)\n","    attention_masks.append(attention_mask)\n","    token_type_ids.append(token_type_id)\n","    data_labels.append(label)\n","\n","  input_ids = np.array(input_ids, dtype=int)\n","  attention_masks = np.array(attention_masks, dtype=int)\n","  token_type_ids = np.array(token_type_ids, dtype=int)\n","  data_labels = np.asarray(data_labels, dtype=np.int32)\n","  return (input_ids, attention_masks, token_type_ids), data_labels"],"metadata":{"id":"LPiGVRNm2UgO","executionInfo":{"status":"ok","timestamp":1752292322498,"user_tz":-540,"elapsed":2,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터와 테스트 데이터에 대해서 전처리를 진행합니다. 단, 문장의 최대 길이는 임의로 128로 지정하였습니다.\n"],"metadata":{"id":"6kslHiNg0bMd"}},{"cell_type":"code","source":["X_train, y_train = convert_examples_to_features(train_data_sentence,\n","                                                train_data_label, max_seq_len=128, tokenizer=tokenizer)\n","X_test, y_test = convert_examples_to_features(test_data_sentence, test_data_label,\n","                                              max_seq_len=128, tokenizer=tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9uKBPxyv-7H","executionInfo":{"status":"ok","timestamp":1752292361360,"user_tz":-540,"elapsed":38861,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"eaaef1d8-35a1-4b3f-a857-875de8857dfa"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 81000/81000 [00:33<00:00, 2443.57it/s]\n","100%|██████████| 9000/9000 [00:03<00:00, 2535.87it/s]\n"]}]},{"cell_type":"markdown","source":["훈련 데이터의 첫번째 샘플을 예시로 보겠습니다. 기존에 주어졌던 원문과 레이블이 BERT토크나이저 적용후 어떻게 변경되었는지 보고, 그후에 실질적으로 BERT의 입력이 되는 정수 인코딩 결과는 어떻게 되었는지 살펴봅시다."],"metadata":{"id":"doIhZVcg1bvN"}},{"cell_type":"code","source":["print('기 존 원 문 :', train_data_sentence[0])\n","print('기 존 레 이 블 :', train_data_label[0])\n","print('-' * 50)\n","print('토 큰 화 후 원 문 :', [tokenizer.decode([word]) for word in X_train[0][0]])\n","print('토 큰 화 후 레 이 블 :', ['[PAD]' if idx == -100 else index_to_tag[idx] for idx\n","in y_train[0]])\n","print('-' * 50)\n","print('정 수 인 코 딩 결 과 :', X_train[0][0])\n","print('정 수 인 코 딩 레 이 블 :', y_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DNj7xPoq06jJ","executionInfo":{"status":"ok","timestamp":1752292361413,"user_tz":-540,"elapsed":52,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"1f9bbc4b-6131-430a-9677-007817990dfd"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["기 존 원 문 : ['정은', '씨를', '힘들게', '한', '가스나그,', '가만둘', '수', '없겠죠', '.']\n","기 존 레 이 블 : ['PER-B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","--------------------------------------------------\n","토 큰 화 후 원 문 : ['[CLS]', '정은', '씨', '##를', '힘들', '##게', '한', '가스', '##나', '##그', ',', '가만', '##둘', '수', '없', '##겠', '##죠', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","토 큰 화 후 레 이 블 : ['[PAD]', 'PER-B', 'O', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', '[PAD]', '[PAD]', 'O', '[PAD]', 'O', 'O', '[PAD]', '[PAD]', 'O', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","--------------------------------------------------\n","정 수 인 코 딩 결 과 : [    2 17915  1370  2138  4390  2318  1891  5809  2075  2029    16  6836\n","  3056  1295  1415  2918  2321    18     3     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n","정 수 인 코 딩 레 이 블 : [-100    1    0 -100    0 -100    0    0 -100 -100 -100    0 -100    0\n","    0 -100 -100    0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n"," -100 -100]\n"]}]},{"cell_type":"markdown","source":["‘토큰화 후 레이\n","블’ 과’ 정수 인코딩 레이블’ 결과를 비교해서 보면, 첫번째 서브워드에는 기존의 레이블을 부여하고, 그 뒤\n","의 서브워드들에 대해서는 ‐100 을 부여하였습니다. 가령, ’ 씨를’ 이 [‘씨’, ’##’ 를] 로 분리되면서 각각 0 과\n","‐100 의 레이블이 부여되었습니다. 그 외에는 [CLS] 토큰과 [SEP] 토큰의 위치에 대해서도 개체명 예측이\n","의미없으므로 ‐100 을 부여했습니다. 세그먼트 인코딩과 어텐션 마스크에 대해서 출력해봅시다."],"metadata":{"id":"QVx8bRq25e9i"}},{"cell_type":"code","source":["print('정수 인코딩: ', X_train[0][0])\n","print('세그먼트 인코딩: ', X_train[2][0])\n","print('어텐션 마스크: ', X_train[1][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEejAgnP1vLg","executionInfo":{"status":"ok","timestamp":1752292361424,"user_tz":-540,"elapsed":10,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"4074c821-bf7b-4792-d408-fd0b053da615"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["정수 인코딩:  [    2 17915  1370  2138  4390  2318  1891  5809  2075  2029    16  6836\n","  3056  1295  1415  2918  2321    18     3     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n","세그먼트 인코딩:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","어텐션 마스크:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"]}]},{"cell_type":"markdown","source":["세그먼트 인코딩과 어텐션 마스크의 경우 네이버 영화 리뷰 분류와 같은 이전 문제들과 부여 방식이 동일 합니다. 세그먼트 인코딩에서는 개체명 인식은 2개 이산의 문장을 구분할 필요가 없는 문제이므로 전부 0으로 채워줍니다. 어텐션 마스크의 경우에는 [CLS]부터 [SEP]까지 1을 부여하고, [SEP]이후의 [PAD]에 대해서 0을 부여 합니다."],"metadata":{"id":"DLOwJwTK6hUu"}},{"cell_type":"markdown","source":["### 4. 모델링\n","개체명 인식을 위한 모델을 설계해봅시다. 텍스트 분류를 수행할 때는 BERT 의 출력인 outputs 에서\n","인덱스 1 로 접근하였습니다. outputs[1] 은 [CLS] 토큰에 접근하는 방법으로 Many‐to‐One 문제에 해\n","당하는 텍스트 분류를 풀 때 사용합니다. 반면, 이번에는 모든 입력에 대해서 출력을 수행해야 하는\n","Many‐to‐Many 문제에 해당하므로 outputs[0] 을 사용하여 문제를 풀어야 합니다.\n","outputs[0] 과 연결되는 출력층에는 레이블의 개수인 num_labels 를 전달합니다. 이번에는 다중 클래스\n","분류 문제임에도 출력층에 소프트맥스 함수를 사용하지 않았는데, 이번에는 출력층에 소프트맥스 함수\n","를 누락시킨 후에 손실 함수에서 이를 처리하도록 하는 구현 방식을 사용해보기 위함입니다.\n"],"metadata":{"id":"Qc_Hry5l7AJ0"}},{"cell_type":"code","source":["class TFBertForTokenClassification(tf.keras.Model):\n","  def __init__(self, model_name, num_labels):\n","    super(TFBertForTokenClassification, self).__init__()\n","    self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n","    self.classifier = tf.keras.layers.Dense(num_labels,\n","                                            kernel_initializer=tf.keras.\n","                                            initializers.TruncatedNormal\n","                                             (0.02),\n","                                            name='classifier')\n","\n","  def call(self, inputs):\n","    input_ids, attention_mask, token_type_ids = inputs\n","    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask,\n","    token_type_ids=token_type_ids)\n","\n","    # 전 체 시 퀀 스 에 대 해 서 분 류 해 야 하 므 로 outputs[0]임 에 주 의\n","    all_output = outputs[0]\n","    prediction = self.classifier(all_output)\n","    return prediction"],"metadata":{"id":"V1NoKNB76hDl","executionInfo":{"status":"ok","timestamp":1752292361427,"user_tz":-540,"elapsed":1,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["###5. 손실 함수에서 -100레이블은 제외시키기(예시 코드)\n","\n","앞서 레이블이 ‐100 인 경우에 대해서는 손실 함수에서 오차를 구현하지 않도록 할 예정이라고 언급했습\n","니다. 다시 말해서 레이블이 ‐100 인 경우에 대해서는 학습에 반영하지 않고자 합니다. 이를 어떻게 구현\n","할 수 있는지 임의의 다중 클래스 분류 모델을 가정하고 예시 코드를 통해서 살펴봅시다.\n","labels 를 실제값에 해당하는 레이블. logits 을 어떤 모델의 예측값이라고 가정해봅시다. 우리가 지금까\n","지 다뤄왔던 다중 클래스 분류 모델은 처음부터 정수 레이블을 예측하지 않습니다. 기본적으로 각 레이블\n","이 정답일 확률값을 예측합니다. 출력층에서 소프트맥스 함수를 통과하고 나면 이 값들의 총 합은 1 이 되\n","기 때문입니다. 가정하기를 현재 가정하고 있는 모델이 예측해야 하는 레이블의 개수가 3 개라고 해봅시\n","다. 다시 말해 logits 의 각 예측값은 3 차원 벡터입니다."],"metadata":{"id":"Vrg6pvE687hk"}},{"cell_type":"code","source":["# tf.constant()는 TensorFlow에서 변하지 않는 상수 텐서를 생성할 때 사용하는 함수\n","labels = tf.constant([[-100, 2, 1, -100]])\n","logits = tf.constant([[[0.8, 0.1, 0.1], [0.06, 0.04, 0.9], [0.75, 0.1, 0.15], [0.4, 0.5, 0.1]]])"],"metadata":{"id":"9IQ8GWEp6N_8","executionInfo":{"status":"ok","timestamp":1752292361655,"user_tz":-540,"elapsed":201,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["active_loss = tf.reshape(labels, (-1,)) != -100\n","print(active_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JQb01J19w2p","executionInfo":{"status":"ok","timestamp":1752292361670,"user_tz":-540,"elapsed":14,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"768f6bc0-efa8-4e68-aeab-6c59aeda0a84"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([False  True  True False], shape=(4,), dtype=bool)\n"]}]},{"cell_type":"markdown","source":["active_loss 는 레이블에서 ‐100 인 경우에는 False ‐100 이 아닌 경우에는 True 의 값을 가집니다. 이제\n","active_loss 를 활용하여 logits 에서 ‐100 의 위치를 가지는 경우의 예측값은 제거합니다. 그 결과는 다음\n","과 같습니다."],"metadata":{"id":"b6pH52soCzXT"}},{"cell_type":"code","source":["reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n","print(reduced_logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1XEHEGy92yK","executionInfo":{"status":"ok","timestamp":1752292361723,"user_tz":-540,"elapsed":52,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"509e4253-2ba5-4e33-e911-1e967acacd13"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[0.06 0.04 0.9 ]\n"," [0.75 0.1  0.15]], shape=(2, 3), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["reduced_logits 을 보면 기존의 4 개의 예측값인 [0.8, 0.1, 0.1], [0.06, 0.04, 0.9], [0.75, 0.1, 0.15], [0.4, 0.5,\n","0.1] 중에서 두번째 위치의 값인 [0.06, 0.04, 0.9] 과 세번째 위치의 값인 [0.75, 0.1, 0.15] 값만이 살아남은\n","것을 볼 수 있는데, 이는 active_loss 에서 두번째 위치와 세번째 위치의 값만이 True 였기 때문입니다. 다\n","시 말해 레이블이 ‐100 이 아닌 위치의 예측값만 남긴 것입니다.\n","마찬가지로 labels 또한 ‐100 이 아닌 위치의 실제값만 남겨봅시다."],"metadata":{"id":"zOi9FE8cDt2U"}},{"cell_type":"code","source":["labels = tf.boolean_mask(tf.reshape(labels, (-1)), active_loss)\n","print(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JT62d1FkDHp5","executionInfo":{"status":"ok","timestamp":1752292361739,"user_tz":-540,"elapsed":15,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"fd6c9482-cd06-4c8d-f082-4aa4695c1db2"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([2 1], shape=(2,), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["동일하게 두번째 위치의 값과 세번째 위치의 값만 살아남고 ‐100 의 값을 가졌던 첫번째 값과 네번째 값은\n","제거되었습니다. 다시 정리하겠습니다. 다음과 같이 레이블. 즉, 실제값과 예측값이 있다고 가정해봅시다.\n","\n","• 실제값: [‐100, 2, 1, ‐100]\n","\n","• 예측값: [예측값 1, 예측값 2, 예측값 3, 예측값 4]\n","\n","레이블의 값이 ‐100 인 경우에는 오차를 구하고 싶지 않습니다. 레이블의 값이 ‐100 이 인 경우에는 실제\n","값과 예측값에 대해서 모두 값을 제거해버립니다.\n","\n","• 실제값: [2, 1]\n","\n","• 예측값: [예측값 2, 예측값 3]\n","\n","이제 레이블의 값이 ‐100 인 경우는 제거되었으니 실제값과 예측값에 대해서 오차를 구하면 레이블의 값\n","이 ‐100 인 경우에 대해서는 오차가 계산되지 않으며 학습에도 반영되지 않게 됩니다."],"metadata":{"id":"n-uZMEDcEDm7"}},{"cell_type":"markdown","source":["###6. 손실 함수 구현하기\n","위의 예시 코드를 통해 이해한 레이블의 값이 -100인 경우를 학습시에 무시하는 방법을 적용하여 손실함수를 구현합니다."],"metadata":{"id":"SJrxJZT3EbOo"}},{"cell_type":"code","source":["def compute_loss(labels, logits):\n","\n","  # 다 중 클 래 스 분 류 문 제 에 서 소 프 트 맥 스 함 수 미 사 용 시 from_logits=True로 설 정.\n","  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n","\n","  # -100의 값 을 가 진 정 수 에 대 해 서 는 오 차 를 반 영 하 지 않 도 록 labels를 수 정.\n","  active_loss = tf.reshape(labels, (-1,)) != -100\n","\n","  # activa_loss로 부 터 reduced_logits과 labels를 각 각 얻 는 다.\n","  reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n","  labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n","  return loss_fn(labels, reduced_logits)"],"metadata":{"id":"NWejd22sOA42","executionInfo":{"status":"ok","timestamp":1752292361741,"user_tz":-540,"elapsed":1,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["손실 함수로는 다중 클래스 분류 문제이므로 우리가 지금까지 다중 클래스 분류 문제에서 사용해왔던\n","것처럼 SparseCategoricalCrossentropy 를 사용합니다. 다만, 우리는 ‐100 이라는 숫자를 레이블에 대한\n","[PAD] 토큰으로 간주하여 loss 를 구하지 않는 것으로 가정하고 전처리를 진행하였으므로 손실 함수에서\n","도 이를 반영해주어야 합니다.\n","한 가지 더 고려해야할 점은 앞서 만든 TFBertForTokenClassification 모델에서 출력층에 소프트맥스 함\n","수를 사용하지 않아 예측 벡터의 총 합이 1 이 되지 않은 상태라는 점입니다. 이 경우 SparseCategorical‐\n","Crossentropy 에서 from_logits 를 True 로 지정하면 예측값이 소프트맥스 함수를 통과하지 않았음을 고\n","려하고 오차를 구합니다.\n","이에 대한 설명은 아래의 SparseCategoricalCrossentropy 의 공식 문서에서 확인할 수 있습니다.\n","https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n","공식 문서에 따르면 from_logits 의 값을 지정해주지 않으면 기본값은 False 로 SparseCategorical‐Crossentropy 는 기본적으로 예측값이 소프트맥스 함수값을 통과한 상태임을 가정합니다. 하지만 예측\n","값에 소프트맥스 함수를 통과시키지 않은 경우라면, from_logits=True 를 해주면 이를 감안하여 오차를\n","구하게 됩니다."],"metadata":{"id":"PdXLrj-0GCP-"}},{"cell_type":"code","source":["model = TFBertForTokenClassification(\"klue/bert-base\", num_labels=tag_size)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","model.compile(optimizer=optimizer, loss=compute_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180,"referenced_widgets":["b803784b83be49298615e6c12252565d","fca2fa52dd5a47ecb88a0d0a77dab168","d6739bb6dab14d86bfb637d8c543cad6","1f74271c4ccc47ad9944abad3cd32eb5","a4812ddd81b9405da75340d1986d03d8","a1da29bc415a4265bc2915061a525a02","108509957b484abc9afe5bf6c951ca1d","24e5f9e295054c529bb41bd09f59d7dd","f436ec7c8ec143a795f00f3c7cb5b6e5","d514afe62505429986b8eb44fcc4e3c1","cd9773d5ba8142748bca7159efc69b59"]},"id":"fzZhr-S7F3ke","executionInfo":{"status":"ok","timestamp":1752292367453,"user_tz":-540,"elapsed":5699,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"bc526882-bed3-4a1c-b569-8da6a3d8c4a5"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b803784b83be49298615e6c12252565d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["한 에포크가 끝날 때마다 테스트 데이터에 대해서 F1 score 를 계산하여 학습이 과적합되고 있지는 않\n","은지 판단하고자 합니다. 이를 위해서 tf.keras.callbacks.Callback 를 상속받아서 커스텀 콜백 클래스인\n","F1score 를 구현했습니다.\n","학습 중 모델의 동작을 정의하는 커스텀 콜백을 구현하는 방식에 대해서는 아래의 링크를 참고하여 구현\n","했습니다.\n","커스텀 콜백 구현 방법: https://www.tensorflow.org/guide/keras/custom_callback?hl=ko\n","커스텀 콜백 클래스에 on_epoch_end 라는 함수를 정의하면, 에포크가 끝날 때마다 해당 함수가 호출됩\n","니다."],"metadata":{"id":"LstZ3FQ_Gbm7"}},{"cell_type":"code","source":["class F1score(tf.keras.callbacks.Callback):\n","  def __init__(self, X_test, y_test):\n","    self.X_test = X_test\n","    self.y_test = y_test\n","\n","  def sequences_to_tags(self, label_ids, pred_ids):\n","    label_list = []\n","    pred_list = []\n","\n","    for i in range(0, len(label_ids)):\n","      label_tag = []\n","      pred_tag = []\n","\n","      # 레 이 블 의 값 이 -100인 경 우 는 F1 score 계 산 시 에 도 제 외\n","      # ex) 레 이 블 디 코 딩 과 정\n","      # label_index : [1 -100 2 -100] ===> [1 2] ===> label_tag : [PER-B PER-I]\n","      for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n","        if label_index != -100:\n","          label_tag.append(index_to_tag[label_index])\n","          pred_tag.append(index_to_tag[pred_index])\n","\n","      label_list.append(label_tag)\n","      pred_list.append(pred_tag)\n","\n","    return label_list, pred_list\n","\n","  # 에 포 크 가 끝 날 때 마 다 실 행 되 는 함 수}\n","  def on_epoch_end(self, epoch, logs={}):\n","    y_predicted = self.model.predict(self.X_test)\n","    y_predicted = np.argmax(y_predicted, axis = 2)\n","\n","    label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n","\n","    score = f1_score(label_list, pred_list, suffix=True)\n","    print(' - f1: {:04.2f}'.format(score * 100))\n","    print(classification_report(label_list, pred_list, suffix=True))"],"metadata":{"id":"T1Ji86bkGQ38","executionInfo":{"status":"ok","timestamp":1752292367495,"user_tz":-540,"elapsed":41,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["f1 스코어를 계산하기 위해서는 예측값과 레이블이 정수 시퀀스가 아니라 개체명 태깅 정보들의 시퀀스\n","이어야만 합니다. 정수 시퀀스로부터 개체명 태깅 정보 시퀀스로 변환해주는 함수가 sequences_to_tags\n","라는 함수입니다.\n","해당 함수의 동작 방식을 살펴봅시다. 예를 들어 실제값에 해당하는 레이블 (y_test) 이 [‐100 1 ‐100 2 ‐100\n","‐100] 이고 예측값 (y_predicted) 이 [0 1 0 2 0 0] 이라고 해봅시다. 우리는 여기서 레이블의 값이 ‐100 인\n","경우에 대해서는 고려하지 않으며 레이블의 값이 ‐100 인 위치에 대해서 레이블과 예측값 모두 제외시킵\n","니다. 이에 따라 레이블과 예측값은 각각 [1 2], [1 2] 가 됩니다. 이를 개체명 태깅 정보 시퀀스로 변환하면\n","실제값과 예측값은 [PER‐B, PER‐I] 와 [PER‐B, PER‐I] 가 됩니다. 그리고 이 결과를 f1 score 계산에 반영합\n","니다. 이 경우 예측값이 실제값을 모두 정확하게 맞춘 경우가 되겠습니다.\n","배치 크기는 32, 3 에포크 학습합니다."],"metadata":{"id":"4kLz9gB-LmbI"}},{"cell_type":"code","source":["f1_score_report = F1score(X_test, y_test)\n","model.fit(X_train, y_train, epochs = 3, batch_size=32, callbacks=[f1_score_report])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UiGruXbBLjIW","outputId":"0ecb1ec7-3782-44e8-e209-c6460be1814c","executionInfo":{"status":"ok","timestamp":1752292528135,"user_tz":-540,"elapsed":160639,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 33ms/step\n"," - f1: 58.86\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         AFW       0.38      0.07      0.12       394\n","         ANM       0.47      0.07      0.12       701\n","         CVL       0.57      0.46      0.51      5758\n","         DAT       0.69      0.60      0.64      2521\n","         EVT       0.39      0.31      0.34      1094\n","         FLD       0.00      0.00      0.00       228\n","         LOC       0.63      0.49      0.55      2126\n","         MAT       0.00      0.00      0.00        12\n","         NUM       0.77      0.72      0.74      5590\n","         ORG       0.59      0.58      0.58      4086\n","         PER       0.71      0.72      0.72      4426\n","         PLT       0.00      0.00      0.00        34\n","         TIM       0.42      0.11      0.18       314\n","         TRM       0.58      0.29      0.38      1964\n","\n","   micro avg       0.65      0.54      0.59     29248\n","   macro avg       0.44      0.31      0.35     29248\n","weighted avg       0.63      0.54      0.57     29248\n","\n","\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 23ms/step - loss: 1.6409\n","Epoch 2/3\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step\n"," - f1: 65.08\n","              precision    recall  f1-score   support\n","\n","         AFW       0.41      0.18      0.25       394\n","         ANM       0.66      0.28      0.39       701\n","         CVL       0.64      0.52      0.57      5758\n","         DAT       0.77      0.68      0.72      2521\n","         EVT       0.44      0.41      0.42      1094\n","         FLD       0.44      0.08      0.14       228\n","         LOC       0.65      0.59      0.62      2126\n","         MAT       0.00      0.00      0.00        12\n","         NUM       0.80      0.76      0.78      5590\n","         ORG       0.66      0.65      0.65      4086\n","         PER       0.75      0.77      0.76      4426\n","         PLT       0.00      0.00      0.00        34\n","         TIM       0.74      0.43      0.55       314\n","         TRM       0.58      0.40      0.47      1964\n","\n","   micro avg       0.69      0.61      0.65     29248\n","   macro avg       0.54      0.41      0.45     29248\n","weighted avg       0.69      0.61      0.64     29248\n","\n","\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - loss: 0.5475\n","Epoch 3/3\n","\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step\n"," - f1: 67.11\n","              precision    recall  f1-score   support\n","\n","         AFW       0.39      0.22      0.28       394\n","         ANM       0.67      0.35      0.46       701\n","         CVL       0.66      0.54      0.59      5758\n","         DAT       0.79      0.71      0.75      2521\n","         EVT       0.46      0.47      0.47      1094\n","         FLD       0.53      0.18      0.27       228\n","         LOC       0.70      0.58      0.63      2126\n","         MAT       0.00      0.00      0.00        12\n","         NUM       0.81      0.78      0.80      5590\n","         ORG       0.67      0.69      0.68      4086\n","         PER       0.76      0.77      0.77      4426\n","         PLT       0.00      0.00      0.00        34\n","         TIM       0.79      0.55      0.65       314\n","         TRM       0.59      0.44      0.50      1964\n","\n","   micro avg       0.71      0.64      0.67     29248\n","   macro avg       0.56      0.45      0.49     29248\n","weighted avg       0.70      0.64      0.66     29248\n","\n","\u001b[1m2532/2532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - loss: 0.4615\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7b9c6c6bee90>"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["###3. 예측\n","이제 학습하지 않은 임의의 문장에 대해서 개체명 태깅 정보를 예측해봅시다. 임의의 문장에 대한 예측\n","을 위해 전처리를 수행하는 함수인 convert_examples_to_features_for_prediction 를 구현합니다. 기본\n","적으로 해당 함수가 input_ids, attention_masks, token_type_ids 를 준비하는 방식은 학습 단계에서 전\n","처리를 위해 사용했던 함수인 convert_examples_to_features 와 같습니다. 학습 단계나 테스트 단계나\n","BERT 의 입력 형식은 동일하기 때문입니다.\n","예측을 위한 함수인 convert_examples_to_features_for_prediction 를 구현 시 고려해야할 점은 학습 단\n","계에서는 레이블이 있었으나 임의의 문장에 대한 예측 시에는 레이블이 존재하지 않는다는 점입니다. 앞\n","서 f1 score 를 계산할 때는 ‐100 의 위치를 레이블인 y_test 로 파악했습니다. 하지만 이번에는 레이블이\n","존재하지 않으므로 ‐100 의 위치를 기록해두는 label_mask 를 만들어줍니다."],"metadata":{"id":"VCUMsDmnoONc"}},{"cell_type":"code","source":["def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer, pad_token_id_for_segment=0,\n","                                                pad_token_id_for_label=-100):\n","  cls_token = tokenizer.cls_token\n","  sep_token = tokenizer.sep_token\n","  pad_token_id = tokenizer.pad_token_id\n","\n","  input_ids, attention_masks, token_type_ids, label_masks = [], [],[], []\n","\n","  for example in tqdm(examples):\n","    tokens = []\n","    label_mask = []\n","    for one_word in example:\n","      # 하나의 단어에 대해서 서브워드로 토큰화\n","      subword_tokens = tokenizer.tokenize(one_word)\n","      tokens.extend(subword_tokens)\n","      # 서브 워드 중 첫번째 서브워드를 제외하고 그 뒤의 서브워드들은  -100으로 채운다.\n","      label_mask.extend([0] + [pad_token_id_for_label] * (len(subword_tokens) -1))\n","\n","    # [CLS]와 [SEP]를 후에 추가 할 것을 고려하여 최대 길이를 초과하는 샘플의 경우 max_seq_len - 2의 길이로 변환.\n","    # ex) max_seq_len = 64라면 길이가 62보다 긴 샘플은 뒷 부분을 자르고 길이 62로 변환.\n","    special_tokens_count = 2\n","    if len(tokens) > max_seq_len - special_tokens_count:\n","      tokens = tokens[:(max_seq_len - special_tokens_count)]\n","      label_mask = label_mask[:(max_seq_len - special_tokens_count)]\n","\n","    # [SEP]을 추가하는 코드\n","    # 1. 토큰화 결과의 맨 뒷 부분에 [SEP]토큰 추가\n","    # 2. 레이블에도 맨 뒷 부분에 -100추가.\n","    tokens += [sep_token]\n","    label_mask += [pad_token_id_for_label]\n","\n","    # [CLS]를 추가 하는 코드\n","    # 1. 토큰화 결과의 앞 부분에 [CLS] 토큰 추가\n","    # 2. 레이블의 맨 앞 부분에도 -100 추가.\n","    tokens = [cls_token] + tokens\n","    label_mask = [pad_token_id_for_label] + label_mask\n","\n","    # 정수 인코딩\n","    input_id = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # 어텐션 마스크 생성\n","    attention_mask = [1] * len(input_id)\n","\n","    # 정수 인코딩에 추가할 패딩 길이 연산\n","    padding_count = max_seq_len - len(input_id)\n","\n","    # 정수 인코딩, 어텐션 마스크에 패딩 추가\n","    input_id = input_id + ([pad_token_id] * padding_count)\n","    attention_mask = attention_mask + ([0] * padding_count)\n","\n","    # 세그먼트 인코딩.\n","    token_type_id = [pad_token_id_for_segment] * max_seq_len\n","\n","    #  레이블 패딩. (단, 이 경우는 패딩 토큰의 ID가 -100)\n","    label_mask = label_mask + ([pad_token_id_for_label] * padding_count)\n","\n","    assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\". format(len(input_id), max_seq_len)\n","    assert len(attention_mask) == max_seq_len, \"Error with attention_mask length {} vs {}\". format(len(attention_mask), max_seq_len)\n","    assert len(token_type_id) == max_seq_len, \"Error with token_type_id length {} vs {}\". format(len(token_type_id), max_seq_len)\n","    assert len(label_mask) == max_seq_len, \"Error with label_mask length {} vs {}\". format(len(label_mask), max_seq_len)\n","\n","    input_ids.append(input_id)\n","    attention_masks.append(attention_mask)\n","    token_type_ids.append(token_type_id)\n","    label_masks.append(label_mask)\n","\n","  input_ids = np.array(input_ids, dtype=int)\n","  attention_masks = np.array(attention_masks, dtype=int)\n","  token_type_ids = np.array(token_type_ids, dtype=int)\n","  label_masks = np.asarray(label_masks, dtype=np.int32)\n","\n","  return (input_ids, attention_masks, token_type_ids), label_masks"],"metadata":{"id":"a11tzO_kMNA7","executionInfo":{"status":"ok","timestamp":1752292549766,"user_tz":-540,"elapsed":3,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["임의로 테스트 데이터 중 샘플 5 개를 입력으로 사용하여 정상적으로 개체명 인식 모델이 동작하는지 테\n","스트 해봅시다."],"metadata":{"id":"I_UV3IICtcvm"}},{"cell_type":"code","source":["X_pred, label_masks = convert_examples_to_features_for_prediction(\n","    test_data_sentence[:5], max_seq_len=128, tokenizer=tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHo5QsHzteMe","executionInfo":{"status":"ok","timestamp":1752292550116,"user_tz":-540,"elapsed":7,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"28f3d07d-c1e9-4402-c4e7-f2e73cd2ec9e"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:00<00:00, 1999.76it/s]\n"]}]},{"cell_type":"markdown","source":["테스트 데이터의 첫번째 샘플인 [‘라티은‐원윤정,’, ‘휘닉스파크클래식’, ‘프로골퍼’] 에 대해서 con‐\n","vert_examples_to_features_for_prediction 가 변환한 결과를 확인해봅시다."],"metadata":{"id":"LlWOC9sRtpNW"}},{"cell_type":"code","source":["print('기 존 원 문 :', test_data_sentence[0])\n","print('-' * 50)\n","print('토 큰 화 후 원 문 :', [tokenizer.decode([word]) for word in X_pred[0][0]])\n","print('레 이 블 마 스 크 :', ['[PAD]' if idx == -100 else '[FIRST]' for idx in label_masks[0]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KV5-2K9trIR","executionInfo":{"status":"ok","timestamp":1752292562359,"user_tz":-540,"elapsed":9,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"96e82089-cd60-4729-a418-441e47402242"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["기 존 원 문 : ['라티은-원윤정,', '휘닉스파크클래식', '프로골퍼']\n","--------------------------------------------------\n","토 큰 화 후 원 문 : ['[CLS]', '라', '##티', '##은', '-', '원', '##윤', '##정', ',', '휘', '##닉스', '##파크', '##클', '##래', '##식', '프로', '##골', '##퍼', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","레 이 블 마 스 크 : ['[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[FIRST]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"]}]},{"cell_type":"markdown","source":["레이블 마스크는 BERT 토크나이저가 하나의 단어에 대해서 서브워드로 분리할 경우, 첫번째 서브워드를 제외한 나머지 서브워드들에 대해서는 [PAD] 토큰.  \n","즉, ‐100 을 부여합니다. 이제 모델이 임의의 문장에\n","대해서 예측했을 때, 레이블 마스크의 값을 참고하여 첫번째 서브워드가 아닌 뒤의 서브워드들에 대한 예\n","측값은 무시합니다."],"metadata":{"id":"ezQMNVGtt2zO"}},{"cell_type":"code","source":["def ner_prediction(examples, max_seq_len, tokenizer):\n","  examples = [sent.split() for sent in examples]\n","  X_pred, label_masks = convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer = tokenizer)\n","  y_predicted = model.predict(X_pred)\n","  y_predicted = np.argmax(y_predicted, axis = 2)\n","\n","  pred_list = []\n","  result_list = []\n","\n","  for i in range(0, len(label_masks)):\n","    pred_tag = []\n","\n","    # ex) 모델의 예측값 디코딩 과정\n","    # 예측값(y_predicted)에서 레이블 마스크 (label_masks)의 값이 -100인 동일 위치의 값을 삭제\n","    # label_masks : [-100 0 -100 0 -100]\n","    # y_predicted : [0 1 0 2 0] --> [1 2] --> 최종 예측(pred_tag) : [PER-B PER-I]\n","\n","    for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n","      if label_index != -100:\n","        pred_tag.append(index_to_tag[pred_index])\n","\n","    pred_list.append(pred_tag)\n","\n","  for example, pred in zip(examples, pred_list):\n","    one_sample_result = []\n","    for one_word, label_token in zip(example, pred):\n","      one_sample_result.append((one_word, label_token))\n","    result_list.append(one_sample_result)\n","\n","  return result_list"],"metadata":{"id":"aEzrUDtet-hO","executionInfo":{"status":"ok","timestamp":1752292605684,"user_tz":-540,"elapsed":11,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터에 존재하지 않았던 임의의 두 개의 문장에 대해서 개체명을 예측해봅시다."],"metadata":{"id":"JHStf5iFwJii"}},{"cell_type":"code","source":["sent1 = '오리온스는 리그 최정상급 포인트가드 김동훈을 앞세우는 빠른 공수전환이 돋보이는 팀 이다'\n","sent2 = '하이신사에 속한 섬들도 위로 솟아 있는데 타인은 살고 있어요'\n","test_samples = [sent1, sent2]\n","result_list = ner_prediction(test_samples, max_seq_len=128, tokenizer=tokenizer)\n","result_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6oeKTZqwJ6J","executionInfo":{"status":"ok","timestamp":1752292623535,"user_tz":-540,"elapsed":6471,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"b730fcce-0b9e-4143-c525-58e2e19f326a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:00<00:00, 2208.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["[[('오리온스는', 'ORG-B'),\n","  ('리그', 'O'),\n","  ('최정상급', 'O'),\n","  ('포인트가드', 'CVL-B'),\n","  ('김동훈을', 'PER-B'),\n","  ('앞세우는', 'O'),\n","  ('빠른', 'O'),\n","  ('공수전환이', 'O'),\n","  ('돋보이는', 'O'),\n","  ('팀', 'O'),\n","  ('이다', 'O')],\n"," [('하이신사에', 'LOC-B'),\n","  ('속한', 'O'),\n","  ('섬들도', 'O'),\n","  ('위로', 'O'),\n","  ('솟아', 'O'),\n","  ('있는데', 'O'),\n","  ('타인은', 'O'),\n","  ('살고', 'O'),\n","  ('있어요', 'O')]]"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":[],"metadata":{"id":"R5_qBkvPx2rt"},"execution_count":null,"outputs":[]}]}