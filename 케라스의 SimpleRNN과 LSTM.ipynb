{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYKuQukEhjaRv38YbHD1OX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 케라스의 SimpleRNN과 LSTM 이해하기"],"metadata":{"id":"3CQGv4OBGqAy"}},{"cell_type":"markdown","source":["#### 1. 임의의 입력 생성하기"],"metadata":{"id":"1uY1M8FCGz6d"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"dy5-Oys7GjY0","executionInfo":{"status":"ok","timestamp":1749630608475,"user_tz":-540,"elapsed":8258,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import SimpleRNN, LSTM, Bidirectional"]},{"cell_type":"code","source":["# 우선 RNN과 LSTM을 테스트하기 위한 임의의 입력을 만듭니다.\n","train_X = [[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1],\n","           [2.2, 1.4, 0.5, 0.9, 1.1]]\n","print(np.shape(train_X))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FjWr4JiHClg","executionInfo":{"status":"ok","timestamp":1749630647718,"user_tz":-540,"elapsed":50,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"fe84e518-6166-4403-aeed-d82d0fb33ab0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 5)\n"]}]},{"cell_type":"markdown","source":["위 입력은 단어 벡터의 차원은 5이고, 문장의 길이가 4인 경우를 가정한 입력입니다. 다시 말해 4번의 시점(timesteps)이 존재하고, 각 시점마다 5차원의 단어 벡터가 입력으로 사용됩니다. 그런데 앞서 RNN은 2D 텐서가 아니라 3D 텐서를 입력을 받는다고 언급한 바 있습니다. 즉, 위에서 만든 2D 텐서를 3D 텐서로 변경합니다. 이는 배치 크기 1을 추가해주므로서 해결합니다."],"metadata":{"id":"Vq9PTE1xHZcV"}},{"cell_type":"code","source":["train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]]\n","train_X = np.array(train_X, dtype=np.float32)\n","print(train_X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yd9yZ1lcHeEw","executionInfo":{"status":"ok","timestamp":1749630716997,"user_tz":-540,"elapsed":48,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"20f11a67-3bd2-43f1-b816-d211ff546818"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 4, 5)\n"]}]},{"cell_type":"markdown","source":["(batch_size, timesteps, input_dim)에 해당되는 (1, 4, 5)의 크기를 가지는 3D 텐서가 생성되었습니다. batch_size는 한 번에 RNN이 학습하는 데이터의 양을 의미하지만, 여기서는 샘플이 1개 밖에 없으므로 batch_size는 1입니다."],"metadata":{"id":"lGBktAUoHS47"}},{"cell_type":"markdown","source":["### 2. SimpleRNN 이해하기"],"metadata":{"id":"35Z-jeN0Hphw"}},{"cell_type":"markdown","source":["위에서 생성한 데이터를 SimpleRNN의 입력으로 사용하여 SimpleRNN의 출력값을 이해해보겠습니다. SimpleRNN에는 여러 인자가 있으며 대표적인 인자로 return_sequences와 return_state가 있습니다. 기본값으로는 둘 다 False로 지정되어져 있으므로 별도 지정을 하지 않을 경우에는 False로 처리됩니다. 우선, 은닉 상태의 크기를 3으로 지정하고, 두 인자 값이 모두 False일 때의 출력값을 보겠습니다.\n","\n","앞으로의 실습에서 SimpleRNN을 매번 재선언하므로 은닉 상태의 값 자체는 매번 초기화되어 이전 출력과 값의 일관성은 없습니다. 그래서 출력값 자체보다는 해당 값의 크기(shape)에 주목해야합니다."],"metadata":{"id":"CIhIUUCJH1zZ"}},{"cell_type":"code","source":["rnn = SimpleRNN(3)\n","# rnn = SimpleRNN(3, return_sequences=False, return_state=False)와 동일\n","hidden_state = rnn(train_X)\n","\n","print('hidden state : {}'.format(hidden_state, hidden_state.shape) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKK5W4W5HPw-","executionInfo":{"status":"ok","timestamp":1749630941300,"user_tz":-540,"elapsed":123,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"243cd281-e9aa-4162-97c6-0e1d77d04548"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden state : [[ 0.86711144 -0.8893518  -0.84088624]]\n"]}]},{"cell_type":"markdown","source":["(1, 3) 크기의 텐서가 출력되는데, 이는 마지막 시점의 은닉 상태입니다. 은닉 상태의 크기를 3으로 지정했음을 주목합시다. 기본적으로 return_sequences가 False인 경우에는 SimpleRNN은 마지막 시점의 은닉 상태만 출력합니다. 이번에는 return_sequences를 True로 지정하여 모든 시점의 은닉 상태를 출력해봅시다."],"metadata":{"id":"WzwvOMNJIcmO"}},{"cell_type":"code","source":["rnn = SimpleRNN(3, return_sequences=True)\n","hidden_states = rnn(train_X)\n","\n","print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jZnuR6pIXau","executionInfo":{"status":"ok","timestamp":1749631059890,"user_tz":-540,"elapsed":24,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"0d3a4c21-3340-4046-f990-d3834f7aa0c8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden states : [[[ 0.96263355 -0.99368376  0.99996084]\n","  [ 0.6160398  -0.9999354   0.9951864 ]\n","  [ 0.78532606 -0.998774    0.78961414]\n","  [ 0.11712066 -0.9969151   0.27246574]]], shape: (1, 4, 3)\n"]}]},{"cell_type":"markdown","source":["(1, 4, 3) 크기의 텐서가 출력됩니다. 앞서 입력 데이터는 (1, 4, 5)의 크기를 가지는 3D 텐서였고, 그 중 4가 시점(timesteps)에 해당하는 값이므로 모든 시점에 대해서 은닉 상태의 값을 출력하여 (1, 4, 3) 크기의 텐서를 출력하는 것입니다.\n","\n","return_state가 True일 경우에는 return_sequences의 True/False 여부와 상관없이 마지막 시점의 은닉 상태를 출력합니다. 가령, return_sequences가 True이면서, return_state를 True로 할 경우 SimpleRNN은 두 개의 출력을 리턴합니다."],"metadata":{"id":"_CR3ZG0sJWAL"}},{"cell_type":"code","source":["rnn = SimpleRNN(3, return_sequences=True, return_state=True)\n","hidden_states, last_state = rnn(train_X)\n","\n","print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n","print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HfB8O87PI0ZT","executionInfo":{"status":"ok","timestamp":1749631351723,"user_tz":-540,"elapsed":52,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"7bca17d2-c6ec-4f97-fa06-73137c8ee2a3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden states : [[[ 0.8826345   0.35551527 -0.99687845]\n","  [ 0.9626614   0.9951557  -0.8957286 ]\n","  [ 0.9053221   0.58595765 -0.22743349]\n","  [ 0.50413644  0.96682465 -0.96367764]]], shape: (1, 4, 3)\n","last hidden state : [[ 0.50413644  0.96682465 -0.96367764]], shape: (1, 3)\n"]}]},{"cell_type":"markdown","source":["첫번째 출력은 return_sequences=True로 인한 출력으로 모든 시점의 은닉 상태입니다. 두번째 출력은 return_state=True로 인한 출력으로 마지막 시점의 은닉 상태입니다. 실제로 출력을 보면 모든 시점의 은닉 상태인 (1, 4, 3) 텐서의 마지막 벡터값이 return_state=True로 인해 출력된 벡터값과 일치하는 것을 볼 수 있습니다\n","\n","그렇다면 return_sequences는 False인데, retun_state가 True인 경우는 어떨까요?"],"metadata":{"id":"krYvSWhTKaFz"}},{"cell_type":"code","source":["rnn = SimpleRNN(3, return_sequences=False, return_state=True)\n","hidden_state, last_state = rnn(train_X)\n","\n","print('hidden state : {}, shape: {}'.format(hidden_state, hidden_state.shape))\n","print('last hidden state : {}, shape: {}'.format(last_state, last_state.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDGn3tcOJ2NT","executionInfo":{"status":"ok","timestamp":1749631585281,"user_tz":-540,"elapsed":19,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"e086bd1d-33bd-4889-e79f-f19a4ebaf4be"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden state : [[ 0.6969066   0.17771791 -0.95390636]], shape: (1, 3)\n","last hidden state : [[ 0.6969066   0.17771791 -0.95390636]], shape: (1, 3)\n"]}]},{"cell_type":"markdown","source":["두 개의 출력 모두 마지막 시점의 은닉 상태를 출력하게 됩니다."],"metadata":{"id":"DDYNniifLAkC"}},{"cell_type":"markdown","source":["## 3. LSTM 이해하기\n","실제로 SimpleRNN이 사용되는 경우는 거의 없습니다. 이보다는 LSTM이나 GRU을 주로 사용하는데, 이번에는 임의의 입력에 대해서 LSTM을 사용할 경우를 보겠습니다. 우선 return_sequences를 False로 두고, return_state가 True인 경우를 봅시다."],"metadata":{"id":"gQVUJo3ULats"}},{"cell_type":"code","source":["lstm = LSTM(3, return_sequences=False, return_state=True)\n","hidden_states, last_state, last_cell_state = lstm(train_X)\n","\n","print('hidden state : {}, shape : {}'.format(hidden_state, hidden_state.shape))\n","print('last hidden state : {}, shape : {}'.format(last_state, last_state.shape))\n","print('last cell state : {}, shape : {}'.format(last_cell_state, last_cell_state.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fONtr5vVKzko","executionInfo":{"status":"ok","timestamp":1749632104638,"user_tz":-540,"elapsed":61,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"e8a29353-7e17-4d7c-c5c8-e39e51a0d4ba"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden state : [[-0.5711909   0.4835549   0.18100376]], shape : (1, 3)\n","last hidden state : [[-0.4860103   0.47650895 -0.38276136]], shape : (1, 3)\n","last cell state : [[-0.8267723  1.1904193 -0.8586818]], shape : (1, 3)\n"]}]},{"cell_type":"markdown","source":["이번에는 SimpleRNN 때와는 달리, 세 개의 결과를 반환합니다. return_sequences가 False이므로 우선 첫번째 결과는 마지막 시점의 은닉 상태입니다. 그런데 LSTM이 SimpleRNN과 다른 점은 return_state를 True로 둔 경우에는 마지막 시점의 은닉 상태뿐만 아니라 셀 상태까지 반환한다는 점입니다. 이번에는 return_sequences를 True로 바꿔보겠습니다."],"metadata":{"id":"snNMQid3Mlt6"}},{"cell_type":"code","source":["lstm = LSTM(3, return_sequences=True, return_state=True)\n","hidden_states, last_hidden_state, last_cell_state = lstm(train_X)\n","\n","print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n","print('last hidden state : {}, shape: {}'.format(last_hidden_state, last_hidden_state.shape))\n","print('last cell state : {}, shape: {}'.format(last_cell_state, last_cell_state.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMkV8qMRMTbP","executionInfo":{"status":"ok","timestamp":1749632106268,"user_tz":-540,"elapsed":52,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"56ce7d25-838b-4125-81e8-9cc278515c1f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden states : [[[ 0.0016936  -0.40220103 -0.349883  ]\n","  [ 0.00345773 -0.58112663 -0.5771329 ]\n","  [ 0.01868253 -0.6383377  -0.5991551 ]\n","  [ 0.00389557 -0.55566025 -0.37123898]]], shape: (1, 4, 3)\n","last hidden state : [[ 0.00389557 -0.55566025 -0.37123898]], shape: (1, 3)\n","last cell state : [[ 0.03455745 -1.6681244  -1.4870386 ]], shape: (1, 3)\n"]}]},{"cell_type":"markdown","source":["return_state가 True이므로 두번째 출력값이 마지막 은닉 상태, 세번째 출력값이 마지막 셀 상태인 것은 변함없지만 return_sequences가 True이므로 첫번째 출력값은 모든 시점의 은닉 상태가 출력됩니다."],"metadata":{"id":"wQpe_vNfM2wh"}},{"cell_type":"markdown","source":["### 4. Bidirectional(LSTM) 이해하기\n","BidirectionalLSTM은 두 개의 별도의 LSTM이 있으며 하나는 순방향으로 다른 하나는 역방향으로 시퀀스를 처리합니다.\n","\n","난이도를 조금 올려서 양방향 LSTM의 출력값을 확인해보겠습니다. return_sequences가 True인 경우와 False인 경우에 대해서 은닉 상태의 값이 어떻게 바뀌는지 직접 비교하기 위해서 이번에는 출력되는 은닉 상태의 값을 고정시켜주겠습니다."],"metadata":{"id":"TOO_t5cRNBrv"}},{"cell_type":"code","source":["k_init = tf.keras.initializers.Constant(value=0.1)\n","b_init = tf.keras.initializers.Constant(value=0)\n","r_init = tf.keras.initializers.Constant(value=0.1)"],"metadata":{"id":"QTax63VrMn4J","executionInfo":{"status":"ok","timestamp":1749632323779,"user_tz":-540,"elapsed":29,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 우선 return_sequences가 False이고, return_state가 True인 경우입니다.\n","bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\\n","                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n","\n","hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n","\n","print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n","print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n","print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3Ho4eedNo9Y","executionInfo":{"status":"ok","timestamp":1749632884936,"user_tz":-540,"elapsed":106,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"6c48f6ff-8ee1-411a-e8f6-24ef2cbb4767"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden states : [[0.6303138 0.6303138 0.6303138 0.7038734 0.7038734 0.7038734]], shape: (1, 6)\n","forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n","backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"]}]},{"cell_type":"markdown","source":["이번에는 무려 5개(hidden_states, forward_h, forward_c, backward_h, backward_c)의 값을 반환합니다.\n","return_state가 True인 경우에는 정방향 LSTM의 은닉 상태와 셀 상태, 역방향 LSTM의 은닉 상태와 셀 상태 4가지를 반환하기 때문입니다. 다만, 셀 상태는 각각 forward_c와 backward_c에 저장만 하고 출력하지 않았습니다. 첫번째 출력값의 크기가 (1, 6)인 것에 주목합시다. 이는 return_sequences가 False인 경우 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태가 연결된 채 반환되기 때문입니다.\n","\n","마찬가지로 return_state가 True인 경우에 반환한 은닉 상태의 값인 forward_h와 backward_h는 각각 정방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫번째 시점의 은닉 상태값입니다. 그리고 이 두 값을 연결한 값이 hidden_states에 출력되는 값입니다.\n","\n","정방향 LSTM의 마지막 시점의 은닉 상태값과 역방향 LSTM의 첫번째 은닉 상태값을 기억해둡시다.\n","\n","\n","* 정방향 LSTM의 마지막의 은닉 상태값:forward state : [0.6303138 0.6303138 0.6303138]\n","\n","* 역방향 LSTM의 첫번째의 은닉 상태값:backward state : [0.7038734 0.7038734 0.7038734]\n","\n","현재 은닉 상태의 값을 고정시켜두었기 때문에return_sequences를 True로 할 경우, 출력이 어떻게 바뀌는지 비교가 가능합니다."],"metadata":{"id":"i28bXKVWPyrC"}},{"cell_type":"code","source":["bilstm = Bidirectional(LSTM(3, return_sequences=True, return_state=True, \\\n","                            kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init))\n","hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X)\n"],"metadata":{"id":"E6UGGrtsOwUt","executionInfo":{"status":"ok","timestamp":1749634705108,"user_tz":-540,"elapsed":52,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["print('hidden states : {}, shape: {}'.format(hidden_states, hidden_states.shape))\n","print('forward state : {}, shape: {}'.format(forward_h, forward_h.shape))\n","print('backward state : {}, shape: {}'.format(backward_h, backward_h.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWh9Ze4MWuVb","executionInfo":{"status":"ok","timestamp":1749634710054,"user_tz":-540,"elapsed":53,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"6f194246-aad6-49d0-fcd0-b9ae6fb7d2c6"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden states : [[[0.35906473 0.35906473 0.35906473 0.7038734  0.7038734  0.7038734 ]\n","  [0.55111325 0.55111325 0.55111325 0.58863586 0.58863586 0.58863586]\n","  [0.59115744 0.59115744 0.59115744 0.3951699  0.3951699  0.3951699 ]\n","  [0.6303138  0.6303138  0.6303138  0.21942244 0.21942244 0.21942244]]], shape: (1, 4, 6)\n","forward state : [[0.6303138 0.6303138 0.6303138]], shape: (1, 3)\n","backward state : [[0.7038734 0.7038734 0.7038734]], shape: (1, 3)\n"]}]},{"cell_type":"markdown","source":["hidden states의 출력값에서는 이제 모든 시점의 은닉 상태가 출력됩니다. 역방향 LSTM의 첫번째 시점의 은닉 상태는 더 이상 정방향 LSTM의 마지막 시점의 은닉 상태와 연결되는 것이 아니라 정방향 LSTM의 첫번째 시점의 은닉 상태와 연결됩니다."],"metadata":{"id":"mwBgeOxiW35m"}}]}