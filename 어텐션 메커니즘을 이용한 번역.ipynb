{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyOj9779raCpmoRuuH25tnf1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 바다나우 어텐션(Bahdanau Attention) 번역"],"metadata":{"id":"Ube6malWXBqa"}},{"cell_type":"markdown","source":["###1. 데이터 로드 및 전처리\n","\n","약 19만개의 병렬 문장 샘플을 포함하고 있습니다. 데이터를 읽고 전처리를 진행해보겠습니다. 앞으로의 코드에서 src는 source의 줄임말로 입력 문장을 나타내며, tar는 target의 줄임말로 번역하고자 하는 문장을 나타냅니다."],"metadata":{"id":"VzVbKk0zXPZS"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"NFLHXlr6W9sG","executionInfo":{"status":"ok","timestamp":1750936779559,"user_tz":-540,"elapsed":4453,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"outputs":[],"source":["import re\n","import os\n","import unicodedata\n","import urllib3\n","import zipfile\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import torch\n","from collections import Counter\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","source":["약 19만개의 데이터 중 33,000개의 샘플만을 사용할 예정입니다."],"metadata":{"id":"z0XcPv5gX-Dp"}},{"cell_type":"code","source":["num_samples = 33000"],"metadata":{"id":"RB4D_1CBX1M0","executionInfo":{"status":"ok","timestamp":1750936779577,"user_tz":-540,"elapsed":7,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!wget -c http://www.manythings.org/anki/fra-eng.zip && unzip -o fra-eng.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWKIuE45YAL0","executionInfo":{"status":"ok","timestamp":1750936781660,"user_tz":-540,"elapsed":2075,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8768b8c1-b69c-4274-e9fb-84249fc716af"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-06-26 11:19:38--  http://www.manythings.org/anki/fra-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n","Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8143096 (7.8M) [application/zip]\n","Saving to: ‘fra-eng.zip’\n","\n","fra-eng.zip         100%[===================>]   7.77M  4.02MB/s    in 1.9s    \n","\n","2025-06-26 11:19:40 (4.02 MB/s) - ‘fra-eng.zip’ saved [8143096/8143096]\n","\n","Archive:  fra-eng.zip\n","  inflating: _about.txt              \n","  inflating: fra.txt                 \n"]}]},{"cell_type":"markdown","source":["전처리 함수들을 구현합니다. 구두점 등을 제거하거나 단어와 구분해주기 위한 전처리입니다."],"metadata":{"id":"87fx2lfHYFiK"}},{"cell_type":"code","source":["def unicode_to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n"],"metadata":{"id":"9g9I_fJd8dI9","executionInfo":{"status":"ok","timestamp":1750936781669,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def preprocess_sentence(sent):\n","  # 악센트 삭제 함수 호출\n","  sent = unicode_to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백을 만듭니다.\n","  # Ex) \"he is a boy.\" => \"he is a boy .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent\n"],"metadata":{"id":"VBFsD5W5YaeC","executionInfo":{"status":"ok","timestamp":1750936781682,"user_tz":-540,"elapsed":9,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], []\n","\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines):\n","      # source 데이터와 target 데이터 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","\n","  return encoder_input, decoder_input, decoder_target\n"],"metadata":{"id":"eJD0h54rYoFO","executionInfo":{"status":"ok","timestamp":1750936781690,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["구현한 전처리 함수들을 임의의 문장을 입력으로 테스트해봅시다."],"metadata":{"id":"uTxZhCrgZ8Hg"}},{"cell_type":"code","source":["# 전처리 테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print('전처리 전 영어 문장 :', en_sent)\n","print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n","print('전처리 전 프랑스어 문장 :', fr_sent)\n","print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma10xQRRZ6IR","executionInfo":{"status":"ok","timestamp":1750936781839,"user_tz":-540,"elapsed":142,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"2799ec10-f018-4e6c-aba8-d525f6b1f5f0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["전처리 전 영어 문장 : Have you had dinner?\n","전처리 후 영어 문장 : have you had dinner ?\n","전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n","전처리 후 프랑스어 문장 : avez vous deja dine ?\n"]}]},{"cell_type":"code","source":["sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n","print('인코더의 입력 :',sents_en_in[:5])\n","print('디코더의 입력 :',sents_fra_in[:5])\n","print('디코더의 레이블 :',sents_fra_out[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GvVHpk8Z-EI","executionInfo":{"status":"ok","timestamp":1750936782540,"user_tz":-540,"elapsed":694,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"1e90b2f5-e85c-4990-a71c-fb7f30516007"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n","디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n","디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"]}]},{"cell_type":"markdown","source":["단어로부터 정수를 얻는 딕셔너리. 즉, 단어 집합(Vocabulary)을 만들어봅시다. 이를 위한 함수로 build_vocab()을 구현합니다. build_vocab은 입력된 데이터로부터 단어의 등장 빈도순으로 정렬 후에 등장 빈도가 높은 순서일 수록 낮은 정수를 부여합니다. 이때, 패딩 토큰을 위한 <PAD> 토큰은 0번, OOV에 대응하기 위한 <UNK> 토큰은 1번에 할당합니다. 이렇게 되면 빈도수가 가장 높은 단어는 정수가 2번, 빈도수가 두번 째로 많은 단어는 정수 3번이 할당됩니다."],"metadata":{"id":"bgPu1sDgarLG"}},{"cell_type":"code","source":["def build_vocab(sents):\n","  word_list = []\n","\n","  for sent in sents:\n","      for word in sent:\n","        word_list.append(word)\n","\n","  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n","  word_counts = Counter(word_list)\n","  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","\n","  word_to_index = {}\n","  word_to_index['<PAD>'] = 0\n","  word_to_index['<UNK>'] = 1\n","\n","  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n","  for index, word in enumerate(vocab) :\n","    word_to_index[word] = index + 2\n","\n","  return word_to_index\n"],"metadata":{"id":"mkYUDjd4ahfy","executionInfo":{"status":"ok","timestamp":1750936782552,"user_tz":-540,"elapsed":5,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["영어를 위한 단어 집합 src_vocab과 프랑스어를 이용한 단어 집합 tar_vocab를 만들어봅시다. 구현 방식에 따라서는 하나의 단어 집합으로 만들어도 상관없으며 이는 선택의 차이입니다."],"metadata":{"id":"RO1MdZNMboye"}},{"cell_type":"code","source":["src_vocab = build_vocab(sents_en_in)\n","tar_vocab = build_vocab(sents_fra_in + sents_fra_out)\n","\n","src_vocab_size = len(src_vocab)\n","tar_vocab_size = len(tar_vocab)\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uD1286HCbkmi","executionInfo":{"status":"ok","timestamp":1750936782704,"user_tz":-540,"elapsed":111,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"8cf3a8f1-f2ad-4d3b-b7bf-30f24826281c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 4498, 프랑스어 단어 집합의 크기 : 7895\n"]}]},{"cell_type":"markdown","source":["정수로부터 단어를 얻는 딕셔너리를 각각 만들어줍니다. 이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용됩니다."],"metadata":{"id":"iUNqaGwHchkj"}},{"cell_type":"code","source":["index_to_src = {v: k for k, v in src_vocab.items()}\n","index_to_tar = {v: k for k, v in tar_vocab.items()}\n","\n","def texts_to_sequences(sents, word_to_index):\n","  encoded_X_data = []\n","  for sent in tqdm(sents):\n","    index_sequences = []\n","    for word in sent:\n","      try:\n","        index_sequences.append(word_to_index[word])\n","      except KeyError:\n","        index_sequences.append(word_to_index['<UNK>'])\n","    encoded_X_data.append(index_sequences)\n","  return encoded_X_data"],"metadata":{"id":"WY644z_Jb_pT","executionInfo":{"status":"ok","timestamp":1750936782734,"user_tz":-540,"elapsed":25,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n","decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n","decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7sRwaAhBeIoP","executionInfo":{"status":"ok","timestamp":1750936783338,"user_tz":-540,"elapsed":597,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"4e15c54f-8d05-4c2c-e799-e345bb283021"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 33000/33000 [00:00<00:00, 1002927.60it/s]\n","100%|██████████| 33000/33000 [00:00<00:00, 885412.01it/s]\n","100%|██████████| 33000/33000 [00:00<00:00, 169676.86it/s]\n"]}]},{"cell_type":"code","source":["# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n","# 인코더 입력이므로 <sos>나 <eos>가 없음\n","for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n","    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JU-zHRQeKxz","executionInfo":{"status":"ok","timestamp":1750936783364,"user_tz":-540,"elapsed":15,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"e2e85e2d-bd20-4dc8-832f-31035d1f658f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n","Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [740, 2]\n"]}]},{"cell_type":"code","source":["def pad_sequences(sentences, max_len=None):\n","  # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n","  if max_len is None:\n","    max_len = max([len(sentence) for sentence in sentences])\n","\n","  features = np.zeros((len(sentences), max_len), dtype=int)\n","  for index, sentence in enumerate(sentences):\n","    if len(sentence) != 0:\n","      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n","\n","  return features"],"metadata":{"id":"DH4-nXmZe0jd","executionInfo":{"status":"ok","timestamp":1750936783372,"user_tz":-540,"elapsed":5,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["encoder_input = pad_sequences(encoder_input)\n","decoder_input = pad_sequences(decoder_input)\n","decoder_target = pad_sequences(decoder_target)"],"metadata":{"id":"M42FdmUlfrMe","executionInfo":{"status":"ok","timestamp":1750936783430,"user_tz":-540,"elapsed":52,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iY3rAMbfuHp","executionInfo":{"status":"ok","timestamp":1750936783454,"user_tz":-540,"elapsed":18,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"1cea1598-f430-4dd3-ed8d-788831ca32ea"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (33000, 7)\n","디코더의 입력의 크기(shape) : (33000, 16)\n","디코더의 레이블의 크기(shape) : (33000, 16)\n"]}]},{"cell_type":"markdown","source":["테스트 데이터를 분리하기 전 데이터를 섞어줍니다. 이를 위해서 순서가 섞인 정수 시퀀스 리스트를 만듭니다."],"metadata":{"id":"zwClGzUcgV5L"}},{"cell_type":"code","source":["indices = np.arange(encoder_input.shape[0])\n","np.random.shuffle(indices)\n","print('랜덤 시퀀스: ', indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PPS-UYefwEW","executionInfo":{"status":"ok","timestamp":1750936783476,"user_tz":-540,"elapsed":16,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"4d6f3192-1712-4edb-d7fe-c7c971297bb6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["랜덤 시퀀스:  [24514 15865 25010 ... 27406   103 14338]\n"]}]},{"cell_type":"markdown","source":["이를 데이터셋의 순서로 지정해주면 샘플들이 기존 순서와 다른 순서로 섞이게 됩니다."],"metadata":{"id":"_Kc4Zilvhfj1"}},{"cell_type":"code","source":["encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]\n"],"metadata":{"id":"flmh4U2thYIP","executionInfo":{"status":"ok","timestamp":1750936783484,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["임의로 30,997번째 샘플을 출력해봅시다. 이때 decoder_input과 decoder_target은 데이터의 구조상으로 앞에 붙은 <sos> 토큰과 뒤에 붙은 <eos>을 제외하면 동일한 시퀀스를 가져야 합니다."],"metadata":{"id":"Ojien4ZfhlsK"}},{"cell_type":"code","source":["print([index_to_src[word] for word in encoder_input[30997]])\n","print([index_to_tar[word] for word in decoder_input[30997]])\n","print([index_to_tar[word] for word in decoder_target[30997]])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVNyAR5dhhvj","executionInfo":{"status":"ok","timestamp":1750936783503,"user_tz":-540,"elapsed":15,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"b8d1ed4e-6e2a-4648-c4f4-30895d586250"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['tom', 'talks', 'tough', '.', '<PAD>', '<PAD>', '<PAD>']\n","['<sos>', 'tom', 'parle', 'comme', 'un', 'dur', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n","['tom', 'parle', 'comme', 'un', 'dur', '.', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"]}]},{"cell_type":"code","source":["# 33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용합니다\n","n_of_val = int(33000*0.1)\n","print('검증 데이터의 개수 :',n_of_val)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKmdOT4OhnXT","executionInfo":{"status":"ok","timestamp":1750936783521,"user_tz":-540,"elapsed":12,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"3c11efb6-aab2-4ef3-f9a7-95857048edce"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["검증 데이터의 개수 : 3300\n"]}]},{"cell_type":"code","source":["encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]\n"],"metadata":{"id":"-wzMOJ1Nhv-q","executionInfo":{"status":"ok","timestamp":1750936783609,"user_tz":-540,"elapsed":82,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mkMmN5zh5YD","executionInfo":{"status":"ok","timestamp":1750936783644,"user_tz":-540,"elapsed":29,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"3e1367f1-c71c-480e-eac3-cd3073801e33"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (29700, 7)\n","훈련 target 데이터의 크기 : (29700, 16)\n","훈련 target 레이블의 크기 : (29700, 16)\n","테스트 source 데이터의 크기 : (3300, 7)\n","테스트 target 데이터의 크기 : (3300, 16)\n","테스트 target 레이블의 크기 : (3300, 16)\n"]}]},{"cell_type":"markdown","source":["###2. 기계 번역기 만들기\n","임베딩 벡터의 차원은 256, 은닉 상태의 차원 또한 256으로 지정합니다."],"metadata":{"id":"faMgZy1oiJZG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","embedding_dim = 256\n","hidden_units = 256"],"metadata":{"id":"eve3MGb3h83j","executionInfo":{"status":"ok","timestamp":1750936783656,"user_tz":-540,"elapsed":6,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["임베딩 층(Embedding layer)과 LSMT 층으로 구성되며 입력 문장은 임베딩 층을 통해 각 단어가 임베딩되고, 인코더의 LSTM을 통과합니다."],"metadata":{"id":"Ud4DDMLElLAg"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n","\n","    def forward(self, x):\n","        # x.shape == (batch_size, seq_len, embedding_dim)\n","        x = self.embedding(x)\n","        # hidden.shape == (1, batch_size, hidden_units), cell.shape == (1, batch_size, hidden_units)\n","        outputs, (hidden, cell) = self.lstm(x)\n","        return outputs, hidden, cell\n"],"metadata":{"id":"TEoHoE5ziY0q","executionInfo":{"status":"ok","timestamp":1750936783665,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["어텐션 메커니즘 자체가 디코더 단에서 시작되기 때문인데, 컨텍스트 벡터를 얻기 위한 연산 과정 자체는 바이트페어인코딩에서 설명한 순서대로 진행됩니다. 우선 디코더의 은닉 상태와 인코더의 모든 시점의 은닉 상태간의 내적(dot product)를 통해서 어텐션 스코어(attention_scores)를 구합니다.\n","\n","그 후 어텐션 스코어를 소프트맥스(softmax) 함수를 통과시켜 어텐션 가중치(attention_weights)를 얻습니다. 어텐션 가중치는 어텐션 메커니즘에서 Value에 해당하는 인코더의 모든 시점의 은닉 상태와 다시 각각 곱해지고 이를 모두 더하여 컨텍스트 벡터(context_vector)를 얻습니다.\n","\n","이 컨텍스트 벡터를 사용하는 방법은 어텐션 메커니즘을 구현하기 나름인데, 아래의 코드에서는 15-02에서의 설명과 같이 임베딩 벡터(아래에서는 변수 x)와 연결되어 입력으로 사용합니다."],"metadata":{"id":"N7FAI_yUmKi1"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n","        super(Decoder, self).__init__()\n","        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim + hidden_units, hidden_units, batch_first=True)\n","        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x, encoder_outputs, hidden, cell):\n","        x = self.embedding(x)\n","\n","        # Dot product attention\n","        # attention_scores.shape: (batch_size, source_seq_len, 1)\n","        attention_scores = torch.bmm(encoder_outputs, hidden.transpose(0, 1).transpose(1, 2))\n","        # transpose() 함수는 차원의 순서를 바꿈\n","        # torch.bmm()은 batch matrix multiplication을 의미, 이름 그대로 \"batch 단위로 행렬 곱을 수행\"하는 함수\n","        # attention_weights.shape: (batch_size, source_seq_len, 1)\n","\n","        attention_weights = self.softmax(attention_scores)\n","\n","        # context_vector.shape: (batch_size, 1, hidden_units)\n","        context_vector = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)\n","\n","        # Repeat context_vector to match seq_len\n","        # context_vector_repeated.shape: (batch_size, target_seq_len, hidden_units)\n","        seq_len = x.shape[1]\n","        context_vector_repeated = context_vector.repeat(1, seq_len, 1)\n","        # repeat() 함수는 텐서의 데이터를 지정한 차원 방향으로 반복해서 복제하는 함수예요. 쉽게 말해, 텐서를 복사해서 크기를 키우는 데 쓰입니다\n","\n","        # Concatenate context vector and embedded input\n","        # x.shape: (batch_size, target_seq_len, embedding_dim + hidden_units)\n","        x = torch.cat((x, context_vector_repeated), dim=2)\n","\n","        # output.shape: (batch_size, target_seq_len, hidden_units)\n","        # hidden.shape: (1, batch_size, hidden_units)\n","        # cell.shape: (1, batch_size, hidden_units)\n","        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n","\n","        # output.shape: (batch_size, target_seq_len, tar_vocab_size)\n","        output = self.fc(output)\n","\n","        return output, hidden, cell\n"],"metadata":{"id":"EoWAGs__6m22","executionInfo":{"status":"ok","timestamp":1750936783676,"user_tz":-540,"elapsed":6,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["인코더와 디코더를 연결하여 Seq2Seq 모델을 완성합니다. 결국 어텐션 메커니즘 자체가 정의되는 것은 디코더 클래스 내부의 코드입니다."],"metadata":{"id":"_pCLQkEvrqyz"}},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg):\n","        encoder_outputs, hidden, cell = self.encoder(src)\n","        output, _, _ = self.decoder(trg, encoder_outputs, hidden, cell)\n","        return output\n","\n","encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n","decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n","model = Seq2Seq(encoder, decoder)\n","\n","loss_function = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(model.parameters())\n"],"metadata":{"id":"ingQ_aX5pLrP","executionInfo":{"status":"ok","timestamp":1750936787957,"user_tz":-540,"elapsed":4276,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["디코더는 인코더의 마지막 은닉 상태로부터 초기 은닉 상태를 얻습니다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다. seq2seq의 디코더는 기본적으로 각 시점마다 다중 클래스 분류 문제를 풀고있습니다. 매 시점마다 프랑스어 단어 집합의 크기(tar_vocab_size)의 선택지에서 단어를 1개 선택하여 이를 이번 시점에서 예측한 단어로 택합니다. 다중 클래스 분류 문제이므로 손실 함수를 크로스 엔트로피 함수를 사용합니다."],"metadata":{"id":"ZRHS1RrftTnd"}},{"cell_type":"code","source":["# 평가 함수를 작성합니다.\n","def evaluation(model, dataloader, loss_function, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_count = 0\n","\n","    with torch.no_grad():\n","        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n","            encoder_inputs = encoder_inputs.to(device)\n","            decoder_inputs = decoder_inputs.to(device)\n","            decoder_targets = decoder_targets.to(device)\n","\n","            # 순방향 전파\n","            # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n","            outputs = model(encoder_inputs, decoder_inputs)\n","\n","            # 손실 계산\n","            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n","            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n","            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n","            total_loss += loss.item()\n","\n","            # 정확도 계산 (패딩 토큰 제외)\n","            mask = decoder_targets != 0\n","            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n","            total_count += mask.sum().item()\n","\n","    return total_loss / len(dataloader), total_correct / total_count\n"],"metadata":{"id":"DWeJvCK7sm4q","executionInfo":{"status":"ok","timestamp":1750936787970,"user_tz":-540,"elapsed":8,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["각각의 데이터를 파이토치 텐서로 변환하고 배치 크기 128로 데이터로더로 변환합니다. 그후 학습 에포크를 30으로 설정합니다."],"metadata":{"id":"Uy8pV0fnvp4z"}},{"cell_type":"code","source":["encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n","decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n","decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n","\n","encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n","decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n","decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n","\n","# 데이터셋 및 데이터로더 생성\n","batch_size = 128\n","\n","train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","\n","# 학습 설정\n","num_epochs = 30\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1EIwMRavpi9","executionInfo":{"status":"ok","timestamp":1750936788194,"user_tz":-540,"elapsed":220,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"a04b0228-8a76-42e1-bd92-686b390780f2"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(4498, 256, padding_idx=0)\n","    (lstm): LSTM(256, 256, batch_first=True)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(7895, 256, padding_idx=0)\n","    (lstm): LSTM(512, 256, batch_first=True)\n","    (fc): Linear(in_features=256, out_features=7895, bias=True)\n","    (softmax): Softmax(dim=1)\n","  )\n",")"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["모델을 훈련합니다. 128개의 배치 크기(128개씩 데이터를 병렬로 학습)로 총 50 에포크 학습합니다. 검증 데이터로 훈련이 제대로 되고있는지 모니터링하겠습니다."],"metadata":{"id":"ekgFbTO0v66C"}},{"cell_type":"code","source":["# Training loop\n","best_val_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    # 훈련 모드\n","    model.train()\n","\n","    for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n","        encoder_inputs = encoder_inputs.to(device)\n","        decoder_inputs = decoder_inputs.to(device)\n","        decoder_targets = decoder_targets.to(device)\n","\n","        # 기울기 초기화\n","        optimizer.zero_grad()\n","\n","        # 순방향 전파\n","        # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n","        outputs = model(encoder_inputs, decoder_inputs)\n","\n","        # 손실 계산 및 역방향 전파\n","        # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n","        # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n","        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n","        loss.backward()\n","\n","        # 가중치 업데이트\n","        optimizer.step()\n","\n","    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n","    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n","\n","    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n","\n","    # 검증 손실이 최소일 때 체크포인트 저장\n","    if valid_loss < best_val_loss:\n","        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n","        best_val_loss = valid_loss\n","        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7wggoa4tcjY","executionInfo":{"status":"ok","timestamp":1750936883894,"user_tz":-540,"elapsed":95693,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"87507770-3d0a-4cf0-cbff-d317b0ac64f4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/30 | Train Loss: 2.8532 | Train Acc: 0.5449 | Valid Loss: 2.9914 | Valid Acc: 0.5344\n","Validation loss improved from inf to 2.9914. 체크포인트를 저장합니다.\n","Epoch: 2/30 | Train Loss: 2.1952 | Train Acc: 0.6179 | Valid Loss: 2.4566 | Valid Acc: 0.5977\n","Validation loss improved from 2.9914 to 2.4566. 체크포인트를 저장합니다.\n","Epoch: 3/30 | Train Loss: 1.7621 | Train Acc: 0.6594 | Valid Loss: 2.1504 | Valid Acc: 0.6251\n","Validation loss improved from 2.4566 to 2.1504. 체크포인트를 저장합니다.\n","Epoch: 4/30 | Train Loss: 1.4400 | Train Acc: 0.7027 | Valid Loss: 1.9472 | Valid Acc: 0.6527\n","Validation loss improved from 2.1504 to 1.9472. 체크포인트를 저장합니다.\n","Epoch: 5/30 | Train Loss: 1.1832 | Train Acc: 0.7443 | Valid Loss: 1.8056 | Valid Acc: 0.6703\n","Validation loss improved from 1.9472 to 1.8056. 체크포인트를 저장합니다.\n","Epoch: 6/30 | Train Loss: 0.9633 | Train Acc: 0.7822 | Valid Loss: 1.6922 | Valid Acc: 0.6874\n","Validation loss improved from 1.8056 to 1.6922. 체크포인트를 저장합니다.\n","Epoch: 7/30 | Train Loss: 0.7848 | Train Acc: 0.8177 | Valid Loss: 1.6100 | Valid Acc: 0.6990\n","Validation loss improved from 1.6922 to 1.6100. 체크포인트를 저장합니다.\n","Epoch: 8/30 | Train Loss: 0.6395 | Train Acc: 0.8489 | Valid Loss: 1.5442 | Valid Acc: 0.7104\n","Validation loss improved from 1.6100 to 1.5442. 체크포인트를 저장합니다.\n","Epoch: 9/30 | Train Loss: 0.5284 | Train Acc: 0.8730 | Valid Loss: 1.5041 | Valid Acc: 0.7167\n","Validation loss improved from 1.5442 to 1.5041. 체크포인트를 저장합니다.\n","Epoch: 10/30 | Train Loss: 0.4406 | Train Acc: 0.8867 | Valid Loss: 1.4969 | Valid Acc: 0.7188\n","Validation loss improved from 1.5041 to 1.4969. 체크포인트를 저장합니다.\n","Epoch: 11/30 | Train Loss: 0.3764 | Train Acc: 0.9007 | Valid Loss: 1.4812 | Valid Acc: 0.7240\n","Validation loss improved from 1.4969 to 1.4812. 체크포인트를 저장합니다.\n","Epoch: 12/30 | Train Loss: 0.3298 | Train Acc: 0.9074 | Valid Loss: 1.4769 | Valid Acc: 0.7249\n","Validation loss improved from 1.4812 to 1.4769. 체크포인트를 저장합니다.\n","Epoch: 13/30 | Train Loss: 0.2913 | Train Acc: 0.9147 | Valid Loss: 1.4784 | Valid Acc: 0.7262\n","Epoch: 14/30 | Train Loss: 0.2602 | Train Acc: 0.9200 | Valid Loss: 1.4844 | Valid Acc: 0.7266\n","Epoch: 15/30 | Train Loss: 0.2401 | Train Acc: 0.9226 | Valid Loss: 1.4984 | Valid Acc: 0.7249\n","Epoch: 16/30 | Train Loss: 0.2210 | Train Acc: 0.9261 | Valid Loss: 1.5081 | Valid Acc: 0.7253\n","Epoch: 17/30 | Train Loss: 0.2089 | Train Acc: 0.9267 | Valid Loss: 1.5197 | Valid Acc: 0.7251\n","Epoch: 18/30 | Train Loss: 0.1984 | Train Acc: 0.9284 | Valid Loss: 1.5257 | Valid Acc: 0.7281\n","Epoch: 19/30 | Train Loss: 0.1913 | Train Acc: 0.9295 | Valid Loss: 1.5402 | Valid Acc: 0.7267\n","Epoch: 20/30 | Train Loss: 0.1832 | Train Acc: 0.9297 | Valid Loss: 1.5453 | Valid Acc: 0.7259\n","Epoch: 21/30 | Train Loss: 0.1801 | Train Acc: 0.9308 | Valid Loss: 1.5598 | Valid Acc: 0.7257\n","Epoch: 22/30 | Train Loss: 0.1754 | Train Acc: 0.9313 | Valid Loss: 1.5764 | Valid Acc: 0.7252\n","Epoch: 23/30 | Train Loss: 0.1717 | Train Acc: 0.9313 | Valid Loss: 1.5855 | Valid Acc: 0.7262\n","Epoch: 24/30 | Train Loss: 0.1687 | Train Acc: 0.9315 | Valid Loss: 1.5957 | Valid Acc: 0.7260\n","Epoch: 25/30 | Train Loss: 0.1659 | Train Acc: 0.9319 | Valid Loss: 1.6111 | Valid Acc: 0.7239\n","Epoch: 26/30 | Train Loss: 0.1620 | Train Acc: 0.9319 | Valid Loss: 1.6078 | Valid Acc: 0.7249\n","Epoch: 27/30 | Train Loss: 0.1587 | Train Acc: 0.9323 | Valid Loss: 1.6180 | Valid Acc: 0.7271\n","Epoch: 28/30 | Train Loss: 0.1568 | Train Acc: 0.9321 | Valid Loss: 1.6266 | Valid Acc: 0.7271\n","Epoch: 29/30 | Train Loss: 0.1559 | Train Acc: 0.9325 | Valid Loss: 1.6387 | Valid Acc: 0.7266\n","Epoch: 30/30 | Train Loss: 0.1521 | Train Acc: 0.9329 | Valid Loss: 1.6344 | Valid Acc: 0.7246\n"]}]},{"cell_type":"markdown","source":["이는 15-02에서 확인한 기록과 비교하면 검증 데이터의 손실이 좀 더 작으며 검증 데이터의 정확도는 더 큰 것을 알 수 있습니다. 검증 데이터 손실이 가장 최소일 때의 모델을 로드하고 다시 재평가해봅시다."],"metadata":{"id":"YcukHU-3x7Oo"}},{"cell_type":"code","source":["# 모델 로드\n","model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n","\n","# 모델을 device에 올립니다.\n","model.to(device)\n","\n","# 검증 데이터에 대한 정확도와 손실 계산\n","val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n","\n","print(f'Best model validation loss: {val_loss:.4f}')\n","print(f'Best model validation accuracy: {val_accuracy:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDilCTmlxccd","executionInfo":{"status":"ok","timestamp":1750936884055,"user_tz":-540,"elapsed":124,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"22a7f7c1-b330-493b-f568-2e9af9acda0e"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Best model validation loss: 1.4769\n","Best model validation accuracy: 0.7249\n"]}]},{"cell_type":"markdown","source":["##3. seq2seq 기계 번역기 동작하기\n","seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다릅니다. 그래서 테스트 과정을 위해 모델을 다시 설계해주어야 합니다. 특히 디코더를 수정해야 합니다. 이번에는 번역 단계를 위해 모델을 수정하고 동작시켜보겠습니다.\n","\n","전체적인 번역 단계를 정리하면 아래와 같습니다.\n","\n","- 1 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻습니다.\n","- 2 인코더의 은닉 상태와 셀 상태, 그리고 토큰 sos를 디코더로 보냅니다.\n","- 3 디코더가 토큰 eos가 나올 때까지 다음 단어를 예측하는 행동을 반복합니다.\n","\n","결과 확인을 위한 함수를 만듭니다. seq_to_src 함수는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환합니다. seq_to_tar은 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환합니다."],"metadata":{"id":"Kp6Q2TFgyGMr"}},{"cell_type":"code","source":["index_to_src = {v: k for k, v in src_vocab.items()}\n","index_to_tar = {v: k for k, v in tar_vocab.items()}\n","\n","# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0):\n","      sentence = sentence + index_to_src[encoded_word] + ' '\n","  return sentence\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","  sentence = ''\n","  for encoded_word in input_seq:\n","    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']):\n","      sentence = sentence + index_to_tar[encoded_word] + ' '\n","  return sentence\n"],"metadata":{"id":"uzuoeSAAx_Sw","executionInfo":{"status":"ok","timestamp":1750936884099,"user_tz":-540,"elapsed":37,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["25번 샘플의 정수 인코딩이 진행된 인코더 입력, 디코더 입력, 그리고 디코더의 레이블을 출력해봅시다."],"metadata":{"id":"jLOsIXhsz_mA"}},{"cell_type":"code","source":["print(encoder_input_test[25])\n","print(decoder_input_test[25])\n","print(decoder_target_test[25])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3701rePznhL","executionInfo":{"status":"ok","timestamp":1750936884141,"user_tz":-540,"elapsed":35,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"c47aa594-9c80-4399-de48-3501f6dbdd8d"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["[   3  710   10 3841    2    0    0]\n","[   3    5   74 1261    9   25 6577    2    0    0    0    0    0    0\n","    0    0]\n","[   5   74 1261    9   25 6577    2    4    0    0    0    0    0    0\n","    0    0]\n"]}]},{"cell_type":"markdown","source":["decode_sequence() 함수를 봅시다. 테스트 단계에서는 디코더를 매 시점 별로 컨트롤 하게 됩니다. 각 시점을 for문을 통해서 컨트롤하게 되며, 현재 시점의 예측은 다음 시점의 입력으로 사용됩니다. 여기서 사용될 변수는 decoder_input입니다."],"metadata":{"id":"lrHQzpdK0Jjd"}},{"cell_type":"code","source":["def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n","  encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n","\n","  # 인코더의 초기 상태 설정\n","  encoder_outputs, hidden, cell = model.encoder(encoder_inputs)\n","\n","  # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n","  decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device)\n","\n","  decoded_tokens = []\n","\n","  # for문을 도는 것 == 디코더의 각 시점\n","  for _ in range(max_output_len):\n","    output, hidden, cell = model.decoder(decoder_input, encoder_outputs, hidden, cell)\n","\n","    # 소프트맥스 회귀를 수행. 예측 단어의 인덱스\n","    output_token = output.argmax(dim=-1).item()\n","\n","    # 종료 토큰 <eos>\n","    if output_token == 4:\n","      break\n","\n","    # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴합니다.\n","    decoded_tokens.append(output_token)\n","\n","    # 현 시점의 예측. 다음 시점의 입력으로 사용된다.\n","    decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n","\n","  return ' '.join(int_to_tar_token[token] for token in decoded_tokens)\n","\n"],"metadata":{"id":"kTXkiMQX0FOq","executionInfo":{"status":"ok","timestamp":1750936884151,"user_tz":-540,"elapsed":4,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."],"metadata":{"id":"5xyihkvM2Nvb"}},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_train[seq_index]\n","  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n","  print(\"번역문장 :\",translated_text)\n","  print(\"-\"*50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PcSrpimC2OgO","executionInfo":{"status":"ok","timestamp":1750936884185,"user_tz":-540,"elapsed":30,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"3b55bb28-d2dc-4d91-8e98-4f0142dac804"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : tom has seniority . \n","정답문장 : tom a de l experience . \n","번역문장 : tom a de l experience .\n","--------------------------------------------------\n","입력문장 : you re all fired . \n","정답문장 : vous etes tous licencies . \n","번역문장 : vous etes tous licencies .\n","--------------------------------------------------\n","입력문장 : his word is law . \n","정답문장 : sa parole est loi . \n","번역문장 : sa parole est la loi .\n","--------------------------------------------------\n","입력문장 : you re dangerous . \n","정답문장 : vous etes dangereux . \n","번역문장 : vous etes dangereuses .\n","--------------------------------------------------\n","입력문장 : we aren t related . \n","정답문장 : nous ne sommes pas apparentes . \n","번역문장 : nous ne sommes pas apparentes .\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["테스트 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력해봅시다."],"metadata":{"id":"y7x_iH6x2S65"}},{"cell_type":"code","source":["for seq_index in [3, 50, 100, 300, 1001]:\n","  input_seq = encoder_input_test[seq_index]\n","  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n","\n","  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n","  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n","  print(\"번역문장 :\",translated_text)\n","  print(\"-\"*50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zuRfiAB2QHw","executionInfo":{"status":"ok","timestamp":1750936884236,"user_tz":-540,"elapsed":46,"user":{"displayName":"유진철","userId":"18428759730043573350"}},"outputId":"dd6c1d64-dec4-4507-c63a-ef96a84ce803"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : be creative . \n","정답문장 : soyez creative ! \n","번역문장 : soyez creatives !\n","--------------------------------------------------\n","입력문장 : tom has a big tv . \n","정답문장 : tom a une grande television . \n","번역문장 : tom a une grosse grosse .\n","--------------------------------------------------\n","입력문장 : no one will know . \n","정답문장 : personne ne saura . \n","번역문장 : personne ne le saura .\n","--------------------------------------------------\n","입력문장 : we re kidding . \n","정답문장 : nous plaisantons . \n","번역문장 : nous plaisantons .\n","--------------------------------------------------\n","입력문장 : take mine . \n","정답문장 : prenez la mienne . \n","번역문장 : prends les miennes .\n","--------------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TFWC2XE-2Ubm","executionInfo":{"status":"ok","timestamp":1750936884246,"user_tz":-540,"elapsed":5,"user":{"displayName":"유진철","userId":"18428759730043573350"}}},"execution_count":35,"outputs":[]}]}